\section{Kernels}

\begin{*definition}[Gaussian kernel]
    The \highlight{gaussian kernel} is a prime example of a kernel:
    \[k(x,y)\coloneqq \exp\left(-\alpha\Vert x-y\Vert_2^2\right)=\phi(\Vert x-y\Vert_2)\]
    for all $x,y\in\R^d$ where $\alpha$ is a scaling parameter.    
\end{*definition}

\begin{definition}\label{def:1.1}
    Let $\Omega$ be an arbitrary nonempty set. 
    A function $k:\Omega\times \Omega\to\R$ is called \dhighlight{kernel} on $\Omega$.
    We call $k$ a \dhighlight{symmetric kernel} if 
    \[k(x,y)=k(y,x)\]
    for all $x,y\in\Omega$.
\end{definition}

\begin{definition}\label{def:1.2}
    A function $\Phi:\R^d\to\R$ is said to be \dhighlight{radial} if there exists a function $\phi:[0,\infty]\to\R$
    such that 
    \[\Phi(x)=\phi(\Vert x\Vert_2)\]
    for all $x\in\R^d$. Such a function is traditionally called a \dhighlight{radial basis function (rbf)}.
\end{definition}

\subsection{Examples}

\begin{example}[(Inverse) multiquadratics]
    \highlight{Multiquadratics} are of the form 
    \[\phi(r)=(1+\alpha r^2)^\beta\]
    for positive $\beta$, while \highlight{inverse multiquadratics}
    have a $\beta<0$.
\end{example}
\begin{example}[Polyharmonic kernels]
    \highlight{Polyharmonic kernels}
    \marginnote{While the previous examples were monotone kernels
        (as a function of $r$), these are not!}
    are of the form
    \[\phi(r)=r^\beta \log (|r|)\]
    where $\beta\in2\Z$.
    
    The special case $\beta=2$ is the so-called \highlight{thin-plate spline}.
    It relates to the partial differential equation that describes the bending of thin plates.
\end{example}
\begin{example}[Wendland's kernels]
   \highlight{Wendland's kernels} are of the form 
   \[\phi_{a,1}\coloneqq (1-r)_+^{(a+1)}(1+(a+1)r)\]
   with the \highlight{cut-off function}
   \[(x)_+\coloneqq \begin{cases}
    x & x\geq 0\\
    0 & x <0
   \end{cases}\] 
\end{example}

\begin{remark}
    There are also non radial kernels:
    
    \highlight{Translation-invariant} or \highlight{stationary} 
    kernels are functions of differences:
    \[k(x,y)=\Phi(x-y).\]
    For periodic setups, we have the \highlight{Dirichlet kernel} as an example:
    \[D(\phi)\coloneqq \frac{1}{2}+\sum_{j=1}^N\cos(j\varphi)=\frac{\sin\left(\left(n+\frac{1}{2}\right)\phi\right)}{2\sin\left(\frac{\phi}{2}\right)}.\]
    This is applied to differences $\phi=\alpha-\beta$ of angles or $2\pi$-periodic arguments and is an important tool for Fourier series theory.

    There are so called \highlight{zonal kernels}, for working on a sphere, where the kernel 
    can be represented as a function of an angle. An example are functions of inner products, such as 
    \[k(x,y)=\exp(x^\intercal y).\]
    Remember, $x^\intercal y$ is the (scaled) cosine of the angle between the two vectors.
\end{remark}

\begin{remark}
    We wil see that a kernel $k$ on $\Omega$ defines a function $k(x,\cdot)$ for all fixed $x\in\Omega$. The space
    \[\mathcal{K}_0\coloneqq \text{span}\{k(x,\cdot)\mid x\in\Omega\}\]
    can for example be used as a so called trial space in meshless methods for solving partial differential equations.
\end{remark}
\begin{remark}
    Kernels can always be restricted to subsets without losing essential properties.
    This easily allows kernels on embedded manifolds, e.g. the sphere.
\end{remark}
\begin{remark}
    Most of this works for complex kernels too.
\end{remark}

\subsection{Kernels in machine learning}

In machine learning the data $x\in\Omega$ can be quite diverse and without (much)
structure on first glance. For example consider images, text documents, customers, graphs, \dots

Here, one views the kernel as a \highlight{similarity measure}\marginnote{In $\R^d$, we can work with the standard scalar product}, i.e.
\[k:\Omega\times \Omega\to\R\]
return a number $k(x,y)$ describing the similarity of two patterns $x$ and $y$.

To work with general data, 
we first need to represent it in a Hilbert space
\marginnote{Reminder: A Hilbert space is a complete vector space 
    with a scalar product}
$\cF$, the so-called \highlight{feature space}. 
One considers the (application dependent) \highlight{feature map}
\[\Phi:\Omega\to\cF.\]
The map describes each $x\in\Omega$ by a collection of \highlight{features} which are 
characteristic for a $x$ and capture the essentials of elements of $\Omega$. 
Since we are now in $\cF$ we can work with linear techniques. In particular we can use 
the scalar product in $\cF$ of two elements of $\Omega$ represented by their features:
\[\langle\Phi(x),\Phi(y) \rangle_{\cF}\eqqcolon k(x,y)\]
and define a kernel that way.

\begin{remark}
    Given a kernel, neither the feature map nor the feature space are unique, as the following example shows:
\end{remark}
\begin{example}\marginnote{Such a construction can be made for any arbitrary kernel, therefore every kernel has many different feature spaces}
    Let $\Omega=\R,k(x,y)=x\cdot y$. A feature map, with feature space $\cF=\R$ is given by the identity map.

    But,the map $\Phi:\Omega\to\R^2$ defined by 
    \[\Phi(x)\coloneqq (x/\sqrt{2},x/\sqrt{2})\]
    is also a feature map given the same $k$! 
\end{example}
The following two examples show how one can handle non-euclidean origin spaces:
\begin{example}[Kernels on a set of documents]
    Consider a collection of documents. 
    We represent each document as a \highlight{bag of words}
    \marginnote{that is a set of frequencies of (chosen) words}
    and describe a bag as a vector in a space in which each dimension is associated
    with a term from the set of words, i.e. the dictionary. The feature map is 
    \[\Phi(t)\coloneqq(\text{wf}(w_1,t),\text{wf}(w_2,t),\dots,\text{wf}(w_d,t))\in\R^d\]
    where $\text{wf}(w_i,t)$ is the frequency of word $w_i$ in document $t$.
    
    A simple kernel is the vector space kernel
    \[k(t_1,t_2)=\langle \Phi(t_1),\Phi(t_2)\rangle=\sum_{j=1}^d \text{wf}(w_j,t_1)\text{wf}(w_j,t_2).\]

    Natural extensions to this kernel take e.g. word order, relevance or semantics into account,
    which can be achieved by using matrices in the scalar product:
    \[k(t_1,t_2)=\langle S\Phi(t_1),S\Phi(t_2)\rangle=\Phi^\intercal(t_1)S^\intercal S\Phi(t_2.)\]
\end{example}
\begin{example}[Graph kernels]
    Another non-euclidean data object are graphs, 
    where the class of \highlight{random walk kernels}
    can be defined. These are based on the idea that given a pair of graphs,
    one performs random walks on both and counts the number of matching walks.
    With $\tilde{A}_\times$ the adjacency matrix of the \highlight{direct product graph} of the two 
    involved graphs, one defines:
    \[k(G,H)\coloneqq\sum_{j=1}^{N_G}\sum_{k=1}^{N_H}\sum_{l=1}^\infty\lambda_l[\tilde{A}^l_\times]_{j,k}.\]
    More generally, one can define a \highlight{random walk graph kernel} $k$ as 
    \[k(G,H)\coloneqq \sum_{k=0}^\infty \lambda_k q_{\times}^T W^k_{\times}p_\times,\]
    where $W_\times$ is the \highlight{weight matrix} of the direct product graph, 
    $q_\times^T$ is the \highlight{stopping probability} on the direct product graph, 
    and $p_\times$ is the initial product distribution on the direct product graph.
\end{example}

\subsection{Mercer kernels}

More generally, one can consider kernels of the \highlight{Hilbert-Schmidt} or \highlight{Mercer} form 
\[k(x,y)=\sum_{i\in I}\lambda_i\varphi_i(x)\varphi_i(y),\]
with certain functions $\varphi_i:\Omega\to\R$, certain positive \highlight{weights} $\lambda_i$ and an 
index set $I$ such that the following \highlight{summability condition} holds for all $x\in\Omega$:
\begin{equation}
    k(x,y)=\sum_{i\in I}\lambda_i\varphi_i(x)^2<\infty
\end{equation}

\begin{remark}
    Such kernels arise in machine learning if the functions $\varphi_i$ each describe a feature of $x$ and the 
    feature space is the weighted $l_2$-space of sequences with indices in $I$:
    \[l_{2,I,\lambda}\coloneqq\left\{\{\xi_i\}_{i\in I}:\sum_{i\in I}\lambda_i\xi_i^2 <\infty\right\}.\]
\end{remark}

This expansion also occurs when kernels generating positive operators are expanded into eigenfunctions on $\Omega$. Such kernels can 
be views as arising from generalized convolutions.

Generally kernels have three major application fields:
\begin{itemize}
    \item Convolutions 
    \item Trial spaces
    \item Covariances
\end{itemize}

We are mainly concerned with the last two.

\beginlecture{02}{11.04.23}

\subsection{Properties of kernels}
Consider an arbitrary set $X=\{x_1,\dots,x_N\}$\marginnote{$N$ is the number of data points (always!)} 
of $N$ \highlight{distinct} elements of $\Omega$ 
and a symmetric Kernel $K$ on $\Omega\times \Omega$.
\[f(x)=\sum_{j=1}^N a_j k(x_j,x),x\in\Omega\]
\begin{remark}
    The set of $k(x_j,\cdot)$ might not be linear independent!
\end{remark}

For $X$ we construct the symmetric $N\times N$ Kernel matrix
\[K=K_{X,X}=(k(x_j,x_k))_{{1\leq j,k\leq N}}\]
and obtain the interpolation problem 
\[\hat{f}_k=f(x_k)=\sum_{j=1}^N a_j k(x_j,x_k)\]
in matrix form 
\[K_{X,X}a=\hat{F}\]

\begin{remark}
    With kernels, we will see that this is indeed solvable, because our matrix is symmetric and positive definite.
\end{remark}

\begin{definition}\label{def:1.3}
    A Kernel on $\Omega\times \Omega$ is 
    \dhighlight{symmetric and positive semidefinite}
    \marginnote{semidefinite and definite have conflicting definitions 
    in the literature!},
    if all Kernel matrices for all finite sets of distinct elements of 
    $\Omega$ are symmetric and positive definite
\end{definition}

\begin{theorem}\label{thm:1.4}
    \begin{enumerate}
        \item Kernels arising from \highlight{feature maps} via \[k(x,y)=\langle\Phi(x),\Phi(y)\rangle\] are positive semidefinite.
        \item \highlight{Hilbert-Schmidt} or \highlight{Mercer Kernels} \[k(x,y)=\sum_{i\in I}\varphi_i(x)\varphi_i(y)\] are positive semidefinite.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item $K$ is a \highlight{Gram(-ian)} matrix\marginnote{A Gram matrix, is a matrix whose entries are given by inner products $K_{i,j}=\langle v_i,v_j\rangle$}
        \item \begin{align*}
            a^\intercal K a&=\sum_{j,k=1}^N a_ja_k k(x_j,x_k)=\sum_{j,k=1}^Na_ja_k\sum_{i\in I}\varphi_i(x)\varphi_i(y)\\
            &=\sum_{i\in I}\lambda_i\sum_{j=1}^N a_j\varphi_i(x_j)\sum_{k=1}^N a_k\varphi_i(x_k)=\sum_{i\in I}\lambda_i\left(\sum_{j=1}^N a_j\phi_i(x_j)\right)^2\geq 0
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}\label{thm:1.5}
    Let $K$ be a symmetric positive semidefinite (spsd) Kernel on $\Omega$. Then  
    \begin{enumerate}
        \item $k(x,x)\geq 0$ for all $x\in \Omega$
        \item\label{thm:1.5_item2} $\vert k(x,y)\vert^2\leq k(x,x)k(y,y)$ for all $x,y\in\Omega$
        \item $2\vert k(x,y)\vert^2\leq k(x,x)^2+k(y,y)^2$ for all $x,y\in\Omega$
        \item Any finite linear combination spsd Kernels with nonnegative coefficients gives a spsd Kernel. If any of these kernels is positive definite, and its coefficient is positive, then the combination of kernels is positive definite.
        \item The product of two spsd kernels is spsd.
        \item The product of two spd kernels is spd.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \underline{1.:} Use the set $\{x\}$ in Definition \ref{def:1.3}.\\
    \underline{2.:} Consider $K$ of $\{x,y\}$. 
    The determinant of such a positve semidefinite matrix is nonnegative,
    therefore \[k(x,x)k(y,y)-k(x,y)^2\geq 0\]
    \underline{3.:} $2ab\leq a^2+b^2$ for all $a,b\in\R_0^+$. Therefore this follows from \ref{thm:1.5_item2}.\\
    \underline{4.:} Expand $a^\intercal K a$ to see this.\\
    \underline{5.:} Follows from Lemma \ref{lemma:1.6}.\\
    \underline{6.:} Follows from Lemma \ref{lemma:1.6} and a bit more linear algebra.
\end{proof}

\begin{lemma}[Schur's Lemma]\label{lemma:1.6}
    For two matrices $A,B$, the matrix $C$ with elements 
    \[C_{jk}=A_{jk}B_{jk}\]
    is called the \dhighlight{Schur product} or \dhighlight{Hardarmard product}. The Schur product of two 
    psd matrices is psd.
\end{lemma}

\begin{proof}
    Decompose $A=S^\intercal DS$ with $S$ an 
    orthogonal matrix and $D=\text{diag}(\lambda_1,\dots,\lambda_n)$ 
    a diagonal matrix with $\lambda_i\geq 0$ the eigenvalues of $A$.

    For all $q\in\R^N$ we look at 
    \begin{align*}
        q^\intercal Cq&=\sum_{j,k}q_jq_ka_{jk}b_{jk}=\sum_{j,k=1}^N q_j q_k\sum_{m=1}^N\lambda_m S_{jm}S_{km}\\
        &=\sum_{m=1}^N \lambda_m \sum_{j,k=1}^N \underbrace{q_j S_{jm}}_{P_{k,m}} \underbrace{q_k S_{km}}_{P_{km}} b_{jk}=\sum_{m=1}^N\sum_{j,k=1}^N\underbrace{P_{jm}P_{km}b_{jk}}_{\geq 0\text{ since } B \text{ is psd}}\geq 0
    \end{align*}
\end{proof}

\begin{remark}
    Note that we only considered symmetric matrices, the above also holds if one of the matrices is not symmetric, but positive definite instead. %TODO: Klären, brauchen wir jetzt symmetric oder nicht?
\end{remark}

\begin{remark}%TODO Link to the definition of RKHS
    Our overall aim is to go from kernels to a \highlight{reproducing kernel Hilbert space (RKHS)}. Therefore 
    we define candidate spaces and a bilinear form in a way we would expec them.
\end{remark}

\begin{*definition}
    For spsd $K$ we define 
    \[H\coloneqq\text{span}\{k(x,\cdot)\mid x\in\Omega\}.\]
    In the same way
    \[L\coloneqq \text{span}\{\delta_x\mid x\in\Omega,\delta_x:H\to\R\}\]
    the linear space of all finite linear combinations of pointevaluation functionals\marginnote{It is important that elements of $L$ act on elements of $H$! These two spaces are paired in some sense.} actions on functions of $H$, where \[\delta_x(f)=f(x).\]
\end{*definition}

We can, by definition, write all Elements from $L$ and $H$ as 
\[\lambda_{a,X}\coloneqq \sum_{j=1}^Na_j\delta_{x_j}\]
\[f_{a,X}\coloneqq \sum_{j=1}^Na_j k(x_j,x)=\lambda_{a,X}^{(y)}k(x,y)\]
with $a\in\R^n,X=\{x_1,\dots,x_N\}\subset\Omega$ any arbitrary finite subset of $\Omega$.

\begin{remark}
    From $f_{a,X}=0$ or $\lambda_{a,X}=0$ it does not follow that $a=0$!\marginnote{There might be different representations of elements in $L,H$. While the representation is not unique, the element is}
\end{remark}

We now define a bilinear form on $L$
\[\langle \lambda_{a,X},\lambda_{b,Y}\rangle_L\coloneqq \sum_{j=1}^M\sum_{k=1}^N a_j b_k k(x_j,x_k)=\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)=\lambda_{a,X}(f_b,Y)\]

\begin{aremark}
    One has to be a bit careful here: $\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)$ does not mean point wise multiplication, but concatenation:
    \[\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)=\lambda_{a,X}^{(x)}(\lambda_{b,Y}^{(y)} k(x,y))\]
\end{aremark}

This is well-defined, 
since it is based on the actions of the functional 
and not the specific representation.

We can observe that  the bilinear form is psd, since the kernel matrices have this property.
\begin{align*}
    \left\vert \lambda_{a,X}(f_{b,Y}) \right\vert=\vert \langle \lambda_{a,X},\lambda_{b,Y}\rangle_L\vert\leq \Vert \lambda_{a,X}\Vert_L \Vert \lambda_{b,Y}\Vert_L \text{ }(\star)
\end{align*}

\begin{theorem}\label{thm:1.7}
    If $K$ is spsd Kernel on $\Omega$, 
    the bilinear form $\langle\cdot,\cdot\rangle_L$ 
    is positive definite in the space $L$ of functionals 
    defined on $H$. This $L$ is a pre-Hilbert-space.

\end{theorem}

\begin{proof}
$0=\langle \lambda_{a,X},\lambda_{a,X}\rangle_L$ for $a\in\R^n,X=\{x_1,\dots,x_N\}\subset \Omega$.

Then by $(\star)$ we have $\lambda_{a,X}=0$ as a functional on $H$\marginnote{Here we use that the functionals in $L$ are restricted to functions in $H$}. 
\end{proof}

\begin{theorem}\label{thm:1.8}
    The mapping $R:\lambda_{a,X}\mapsto f_{a,X}=\lambda_{a,X}(k(\cdot,y))$ is linear and bijective from $L$ onto $H$. Thus 
    \[\langle f_{a,X},f_{b,Y}\rangle_H\coloneqq \langle \lambda_{a,X},\lambda_{b,Y}\rangle_L=\langle R(\lambda_{a,X},R(\lambda_{b,Y})\rangle_H\]
    is an inner product on $H$. $R$ acts as the Riesz map. 
\end{theorem}

\begin{proof}
    Linearity is obvious. If $f_{b,Y}=R(\lambda_{b,Y})\in H$ vanishes, the definition of 
    $\langle\cdot,\cdot\rangle_L$ implies that $\lambda_{b,Y}$ is orthogonal to all of $L$. Due to Theorem \ref{thm:1.7} it is zero.
    The Riesz property comes from the definition of $\langle\cdot,\cdot\rangle_L$:
    \[\lambda_{a,X}(f_{b,Y})=\langle \lambda_{a,X},\lambda_{b,Y}\rangle_L=\langle f_{a,X},f_{b,Y}\rangle_H=\langle R(\lambda_{a,X}),f_{b,Y}\rangle\]
\end{proof}

Specializing to $\lambda_{1,x}$, i.e. to a point $x\in \Omega$, we get:
\begin{align*}
    \langle \lambda_{1,x},\lambda_{b,Y}\rangle_L&=\lambda_{1,x}(f_{b,Y})=\delta_x(f_{b,Y})=f_{b,Y}(x)\\
    =\langle R(\lambda_{1,x}),R(\lambda_{b,Y})\rangle_H&=\langle R(\lambda_{1,x}),f_{b,Y}\rangle_H=\langle k(x,\cdot),f_{b,Y}\rangle_H
\end{align*}

In other words, for all $f\in H$, $x\in \Omega$, we have 
\[f(x)=\underline{\delta_x(f)}=\langle f, R(\delta_x)\rangle_H=\underline{\langle f,k(x,\cdot)\rangle_H}\]
which is the so-called \dhighlight{reproduction equation} for values of functions from the inner product.