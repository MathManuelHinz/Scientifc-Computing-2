\section{Kernels}

\begin{*definition}[Gaussian kernel]
    The \highlight{gaussian kernel} is a prime example of a kernel:
    \[k(x,y)\coloneqq \exp\left(-\alpha\Vert x-y\Vert_2^2\right)=\phi(\Vert x-y\Vert_2)\]
    for all $x,y\in\R^d$ where $\alpha$ is a scaling parameter.    
\end{*definition}

\begin{definition}\label{def:1.1}
    Let $\Omega$ be an arbitrary nonempty set. 
    A function $k:\Omega\times \Omega\to\R$ is called \dhighlight{kernel} on $\Omega$.
    We call $k$ a \dhighlight{symmetric kernel} if 
    \[k(x,y)=k(y,x)\]
    for all $x,y\in\Omega$.
\end{definition}

\begin{definition}\label{def:1.2}
    A function $\Phi:\R^d\to\R$ is said to be \dhighlight{radial} if there exists a function $\phi:[0,\infty]\to\R$
    such that 
    \[\Phi(x)=\phi(\Vert x\Vert_2)\]
    for all $x\in\R^d$. Such a function is traditionally called a \dhighlight{radial basis function (rbf)}.
\end{definition}

\subsection{Examples}

\begin{example}[(Inverse) multiquadratics]
    \highlight{Multiquadratics} are of the form 
    \[\phi(r)=(1+\alpha r^2)^\beta\]
    for positive $\beta$, while \highlight{inverse multiquadratics}
    have a $\beta<0$.
\end{example}
\begin{example}[Polyharmonic kernels]
    \highlight{Polyharmonic kernels}
    \marginnote{While the previous examples were monotone kernels
        (as a function of $r$), these are not!}
    are of the form
    \[\phi(r)=r^\beta \log (|r|)\]
    where $\beta\in2\Z$.
    
    The special case $\beta=2$ is the so-called \highlight{thin-plate spline}.
    It relates to the partial differential equation that describes the bending of thin plates.
\end{example}
\begin{example}[Wendland's kernels]
   \highlight{Wendland's kernels} are of the form 
   \[\phi_{a,1}\coloneqq (1-r)_+^{(a+1)}(1+(a+1)r)\]
   with the \highlight{cut-off function}
   \[(x)_+\coloneqq \begin{cases}
    x & x\geq 0\\
    0 & x <0
   \end{cases}\] 
\end{example}

\begin{remark}
    There are also non radial kernels:
    
    \highlight{Translation-invariant} or \highlight{stationary} 
    kernels are functions of differences:
    \[k(x,y)=\Phi(x-y).\]
    For periodic setups, we have the \highlight{Dirichlet kernel} as an example:
    \[D(\phi)\coloneqq \frac{1}{2}+\sum_{j=1}^N\cos(j\varphi)=\frac{\sin\left(\left(n+\frac{1}{2}\right)\phi\right)}{2\sin\left(\frac{\phi}{2}\right)}.\]
    This is applied to differences $\phi=\alpha-\beta$ of angles or $2\pi$-periodic arguments and is an important tool for Fourier series theory.

    There are so called \highlight{zonal kernels}, for working on a sphere, where the kernel 
    can be represented as a function of an angle. An example are functions of inner products, such as 
    \[k(x,y)=\exp(x^\perp y).\]
    Remember, $x^\perp y$ is the (scaled) cosine of the angle between the two vectors.
\end{remark}

\begin{remark}
    We wil see that a kernel $k$ on $\Omega$ defines a function $k(x,\cdot)$ for all fixed $x\in\Omega$. The space
    \[\mathcal{K}_0\coloneqq \text{span}\{k(x,\cdot)\mid x\in\Omega\}\]
    can for example be used as a so called trial space in meshless methods for solving partial differential equations.
\end{remark}
\begin{remark}
    Kernels can always be restricted to subsets without losing essential properties.
    This easily allows kernels on embedded manifolds, e.g. the sphere.
\end{remark}
\begin{remark}
    Most of this works for complex kernels too.
\end{remark}

\subsection{Kernels in machine learning}

In machine learning the data $x\in\Omega$ can be quite diverse and without (much)
structure on first glance. For example consider images, text documents, customers, graphs, \dots

Here, one views the kernel as a \highlight{similarity measure}\marginnote{In $\R^d$, we can work with the standard scalar product}, i.e.
\[k:\Omega\times \Omega\to\R\]
return a number $k(x,y)$ describing the similarity of two patterns $x$ and $y$.

To work with general data, 
we first need to represent it in a Hilbert space
\marginnote{Reminder: A Hilbert space is a complete vector space 
    with a scalar product}
$\cF$, the so-called \highlight{feature space}. 
One considers the (application dependent) \highlight{feature map}
\[\Phi:\Omega\to\cF.\]
The map describes each $x\in\Omega$ by a collection of \highlight{features} which are 
characteristic for a $x$ and capture the essentials of elements of $\Omega$. 
Since we are now in $\cF$ we can work with linear techniques. In particular we can use 
the scalar product in $\cF$ of two elements of $\Omega$ represented by their features:
\[\langle\Phi(x),\Phi(y) \rangle_{\cF}\eqqcolon k(x,y)\]
and define a kernel that way.

\begin{remark}
    Given a kernel, neither the feature map nor the feature space are unique, as the following example shows:
\end{remark}
\begin{example}\marginnote{Such a construction can be made for any arbitrary kernel, therefore every kernel has many different feature spaces}
    Let $\Omega=\R,k(x,y)=x\cdot y$. A feature map, with feature space $\cF=\R$ is given by the identity map.

    But,the map $\Phi:\Omega\to\R^2$ defined by 
    \[\Phi(x)\coloneqq (x/\sqrt{2},x/\sqrt{2})\]
    is also a feature map given the same $k$! 
\end{example}
The following two examples show how one can handle non-euclidean origin spaces:
\begin{example}[Kernels on a set of documents]
    Consider a collection of documents. 
    We represent each document as a \highlight{bag of words}
    \marginnote{that is a set of frequencies of (chosen) words}
    and describe a bag as a vector in a space in which each dimension is associated
    with a term from the set of words, i.e. the dictionary. The feature map is 
    \[\Phi(t)\coloneqq(\text{wf}(w_1,t),\text{wf}(w_2,t),\dots,\text{wf}(w_d,t))\in\R^d\]
    where $\text{wf}(w_i,t)$ is the frequency of word $w_i$ in document $t$.
    
    A simple kernel is the vector space kernel
    \[k(t_1,t_2)=\langle \Phi(t_1),\Phi(t_2)\rangle=\sum_{j=1}^d \text{wf}(w_j,t_1)\text{wf}(w_j,t_2).\]

    Natural extensions to this kernel take e.g. word order, relevance or semantics into account,
    which can be achieved by using matrices in the scalar product:
    \[k(t_1,t_2)=\langle S\Phi(t_1),S\Phi(t_2)\rangle=\Phi^\perp(t_1)S^\perp S\Phi(t_2.)\]
\end{example}
\begin{example}[Graph kernels]
    Another non-euclidean data object are graphs, 
    where the class of \highlight{random walk kernels}
    can be defined. These are based on the idea that given a pair of graphs,
    one performs random walks on both and counts the number of matching walks.
    With $\tilde{A}_\times$ the adjacency matrix of the \highlight{direct product graph} of the two 
    involved graphs, one defines:
    \[k(G,H)\coloneqq\sum_{j=1}^{N_G}\sum_{k=1}^{N_H}\sum_{l=1}^\infty\lambda_l[\tilde{A}^l_\times]_{j,k}.\]
    More generally, one can define a \highlight{random walk graph kernel} $k$ as 
    \[k(G,H)\coloneqq \sum_{k=0}^\infty \lambda_k q_{\times}^T W^k_{\times}p_\times,\]
    where $W_\times$ is the \highlight{weight matrix} of the direct product graph, 
    $q_\times^T$ is the \highlight{stopping probability} on the direct product graph, 
    and $p_\times$ is the initial product distribution on the direct product graph.
\end{example}

\subsection{Mercer kernels}

More generally, one can consider kernels of the \highlight{Hilbert-Schmidt} or \highlight{Mercer} form 
\[k(x,y)=\sum_{i\in I}\lambda_i\varphi_i(x)\varphi_i(y),\]
with certain functions $\varphi_i:\Omega\to\R$, certain positive \highlight{weights} $\lambda_i$ and an 
index set $I$ such that the following \highlight{summability condition} holds for all $x\in\Omega$:
\begin{equation}
    k(x,y)=\sum_{i\in I}\lambda_i\varphi_i(x)^2<\infty
\end{equation}

\begin{remark}
    Such kernels arise in machine learning if the functions $\varphi_i$ each describe a feature of $x$ and the 
    feature space is the weighted $l_2$-space of sequences with indices in $I$:
    \[l_{2,I,\lambda}\coloneqq\left\{\{\xi_i\}_{i\in I}:\sum_{i\in I}\lambda_i\xi_i^2 <\infty\right\}.\]
\end{remark}

This expansion also occurs when kernels generating positive operators are expanded into eigenfunctions on $\Omega$. Such kernels can 
be views as arising from generalized convolutions.

Generally kernels have three major application fields:
\begin{itemize}
    \item Convolutions 
    \item Trial spaces
    \item Covariances
\end{itemize}

We are mainly concerned with the last two.

\beginlecture{02}{11.04.24}

\subsection{Properties of kernels}
Consider an arbitrary set $X=\{x_1,\dots,x_N\}$\marginnote{$N$ is the number of data points (always!)} 
of $N$ \highlight{distinct} elements of $\Omega$ 
and a symmetric Kernel $K$ on $\Omega\times \Omega$.
\[f(x)=\sum_{j=1}^N a_j k(x_j,x),x\in\Omega\]
\begin{remark}
    The set of $k(x_j,\cdot)$ might not be linear independent!
\end{remark}

For $X$ we construct the symmetric $N\times N$ Kernel matrix
\[K=K_{X,X}=(k(x_j,x_k))_{{1\leq j,k\leq N}}\]
and obtain the interpolation problem 
\[\hat{f}_k=f(x_k)=\sum_{j=1}^N a_j k(x_j,x_k)\]
in matrix form 
\[K_{X,X}a=\hat{F}\]

\begin{remark}
    With kernels, we will see that this is indeed solvable, because our matrix is symmetric and positive definite.
\end{remark}

\begin{definition}\label{def:1.3}
    A Kernel on $\Omega\times \Omega$ is 
    \dhighlight{symmetric and positive semidefinite}
    \marginnote{semidefinite and definite have conflicting definitions 
    in the literature!},
    if all Kernel matrices for all finite sets of distinct elements of 
    $\Omega$ are symmetric and positive definite
\end{definition}

\begin{theorem}\label{thm:1.4}
    \begin{enumerate}
        \item Kernels arising from \highlight{feature maps} via \[k(x,y)=\langle\Phi(x),\Phi(y)\rangle\] are positive semidefinite.
        \item \highlight{Hilbert-Schmidt} or \highlight{Mercer Kernels} \[k(x,y)=\sum_{i\in I}\varphi_i(x)\varphi_i(y)\] are positive semidefinite.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item $K$ is a \highlight{Gram(-ian)} matrix\marginnote{A Gram matrix, is a matrix whose entries are given by inner products $K_{i,j}=\langle v_i,v_j\rangle$}
        \item \begin{align*}
            a^\perp K a&=\sum_{j,k=1}^N a_ja_k k(x_j,x_k)=\sum_{j,k=1}^Na_ja_k\sum_{i\in I}\varphi_i(x)\varphi_i(y)\\
            &=\sum_{i\in I}\lambda_i\sum_{j=1}^N a_j\varphi_i(x_j)\sum_{k=1}^N a_k\varphi_i(x_k)=\sum_{i\in I}\lambda_i\left(\sum_{j=1}^N a_j\phi_i(x_j)\right)^2\geq 0
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}\label{thm:1.5}
    Let $K$ be a symmetric positive semidefinite (spsd) Kernel on $\Omega$. Then  
    \begin{enumerate}
        \item $k(x,x)\geq 0$ for all $x\in \Omega$
        \item\label{thm:1.5_item2} $\vert k(x,y)\vert^2\leq k(x,x)k(y,y)$ for all $x,y\in\Omega$
        \item $2\vert k(x,y)\vert^2\leq k(x,x)^2+k(y,y)^2$ for all $x,y\in\Omega$
        \item Any finite linear combination spsd Kernels with nonnegative coefficients gives a spsd Kernel. If any of these kernels is positive definite, and its coefficient is positive, then the combination of kernels is positive definite.
        \item The product of two spsd kernels is spsd.
        \item The product of two spd kernels is spd.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \underline{1.:} Use the set $\{x\}$ in Definition \ref{def:1.3}.\\
    \underline{2.:} Consider $K$ of $\{x,y\}$. 
    The determinant of such a positve semidefinite matrix is nonnegative,
    therefore \[k(x,x)k(y,y)-k(x,y)^2\geq 0\]
    \underline{3.:} $2ab\leq a^2+b^2$ for all $a,b\in\R_0^+$. Therefore this follows from \ref{thm:1.5_item2}.\\
    \underline{4.:} Expand $a^\perp K a$ to see this.\\
    \underline{5.:} Follows from Lemma \ref{lemma:1.6}.\\
    \underline{6.:} Follows from Lemma \ref{lemma:1.6} and a bit more linear algebra.
\end{proof}

\begin{lemma}[Schur's Lemma]\label{lemma:1.6}
    For two matrices $A,B$, the matrix $C$ with elements 
    \[C_{jk}=A_{jk}B_{jk}\]
    is called the \dhighlight{Schur product} or \dhighlight{Hardarmard product}. The Schur product of two 
    psd matrices is psd.
\end{lemma}

\begin{proof}
    Decompose $A=S^\perp DS$ with $S$ an 
    orthogonal matrix and $D=\text{diag}(\lambda_1,\dots,\lambda_n)$ 
    a diagonal matrix with $\lambda_i\geq 0$ the eigenvalues of $A$.

    For all $q\in\R^N$ we look at 
    \begin{align*}
        q^\perp Cq&=\sum_{j,k}q_jq_ka_{jk}b_{jk}=\sum_{j,k=1}^N q_j q_k\sum_{m=1}^N\lambda_m S_{jm}S_{km}\\
        &=\sum_{m=1}^N \lambda_m \sum_{j,k=1}^N \underbrace{q_j S_{jm}}_{P_{k,m}} \underbrace{q_k S_{km}}_{P_{km}} b_{jk}=\sum_{m=1}^N\sum_{j,k=1}^N\underbrace{P_{jm}P_{km}b_{jk}}_{\geq 0\text{ since } B \text{ is psd}}\geq 0
    \end{align*}
\end{proof}

\begin{remark}
    Note that we only considered symmetric matrices, the above also holds if one of the matrices is not symmetric, but positive definite instead. %TODO: Klären, brauchen wir jetzt symmetric oder nicht?
\end{remark}

\begin{remark}
    Our overall aim is to go from kernels to a \highlight{Reproducing Kernel Hilbert space (RKHS)}. Therefore 
    we define candidate spaces and a bilinear form in a way we would expec them.
\end{remark}

\begin{*definition}
    For spsd $K$ we define 
    \[H\coloneqq\text{span}\{k(x,\cdot)\mid x\in\Omega\}.\]
    In the same way
    \[L\coloneqq \text{span}\{\delta_x\mid x\in\Omega,\delta_x:H\to\R\}\]
    the linear space of all finite linear combinations of pointevaluation functionals\marginnote{It is important that elements of $L$ act on elements of $H$! These two spaces are paired in some sense.} actions on functions of $H$, where \[\delta_x(f)=f(x).\]
\end{*definition}

We can, by definition, write all Elements from $L$ and $H$ as 
\[\lambda_{a,X}\coloneqq \sum_{j=1}^Na_j\delta_{x_j}\]
\[f_{a,X}\coloneqq \sum_{j=1}^Na_j k(x_j,x)=\lambda_{a,X}^{(y)}k(x,y)\]
with $a\in\R^n,X=\{x_1,\dots,x_N\}\subset\Omega$ any arbitrary finite subset of $\Omega$.

\begin{remark}
    From $f_{a,X}=0$ or $\lambda_{a,X}=0$ it does not follow that $a=0$!\marginnote{There might be different representations of elements in $L,H$. While the representation is not unique, the element is}
\end{remark}

We now define a bilinear form on $L$
\[\langle \lambda_{a,X},\lambda_{b,Y}\rangle_L\coloneqq \sum_{j=1}^M\sum_{k=1}^N a_j b_k k(x_j,x_k)=\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)=\lambda_{a,X}(f_b,Y)\]

\begin{aremark}
    One has to be a bit careful here: $\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)$ does not mean point wise multiplication, but concatenation:
    \[\lambda_{a,X}^{(x)}\lambda_{b,Y}^{(y)} k(x,y)=\lambda_{a,X}^{(x)}(\lambda_{b,Y}^{(y)} k(x,y))\]
\end{aremark}

This is well-defined, 
since it is based on the actions of the functional 
and not the specific representation.

We can observe that  the bilinear form is psd, since the kernel matrices have this property.
\begin{align*}
    \left\vert \lambda_{a,X}(f_{b,Y}) \right\vert=\vert \langle \lambda_{a,X},\lambda_{b,Y}\rangle_L\vert\leq \Vert \lambda_{a,X}\Vert_L \Vert \lambda_{b,Y}\Vert_L \text{ }(\star)
\end{align*}

\begin{theorem}\label{thm:1.7}
    If $K$ is spsd Kernel on $\Omega$, 
    the bilinear form $\langle\cdot,\cdot\rangle_L$ 
    is positive definite in the space $L$ of functionals 
    defined on $H$. This $L$ is a pre-Hilbert-space.

\end{theorem}

\begin{proof}
$0=\langle \lambda_{a,X},\lambda_{a,X}\rangle_L$ for $a\in\R^n,X=\{x_1,\dots,x_N\}\subset \Omega$.

Then by $(\star)$ we have $\lambda_{a,X}=0$ as a functional on $H$\marginnote{Here we use that the functionals in $L$ are restricted to functions in $H$}. 
\end{proof}

\begin{theorem}\label{thm:1.8}
    The mapping $R:\lambda_{a,X}\mapsto f_{a,X}=\lambda_{a,X}(k(\cdot,y))$ is linear and bijective from $L$ onto $H$. Thus 
    \[\langle f_{a,X},f_{b,Y}\rangle_H\coloneqq \langle \lambda_{a,X},\lambda_{b,Y}\rangle_L=\langle R(\lambda_{a,X}),R(\lambda_{b,Y})\rangle_H\]
    is an inner product on $H$. $R$ acts as the Riesz map. 
\end{theorem}

\begin{proof}
    Linearity is obvious. If $f_{b,Y}=R(\lambda_{b,Y})\in H$ vanishes, the definition of 
    $\langle\cdot,\cdot\rangle_L$ implies that $\lambda_{b,Y}$ is orthogonal to all of $L$. Due to Theorem \ref{thm:1.7} it is zero.
    The Riesz property comes from the definition of $\langle\cdot,\cdot\rangle_L$:
    \[\lambda_{a,X}(f_{b,Y})=\langle \lambda_{a,X},\lambda_{b,Y}\rangle_L=\langle f_{a,X},f_{b,Y}\rangle_H=\langle R(\lambda_{a,X}),f_{b,Y}\rangle\]
\end{proof}

Specializing to $\lambda_{1,x}$, i.e. to a point $x\in \Omega$, we get:
\begin{align*}
    \langle \lambda_{1,x},\lambda_{b,Y}\rangle_L&=\lambda_{1,x}(f_{b,Y})=\delta_x(f_{b,Y})=f_{b,Y}(x)\\
    =\langle R(\lambda_{1,x}),R(\lambda_{b,Y})\rangle_H&=\langle R(\lambda_{1,x}),f_{b,Y}\rangle_H=\langle k(x,\cdot),f_{b,Y}\rangle_H
\end{align*}

In other words, for all $f\in H$, $x\in \Omega$, we have 
\[f(x)=\underline{\delta_x(f)}=\langle f, R(\delta_x)\rangle_H=\underline{\langle f,k(x,\cdot)\rangle_H}\]
which is the so-called \dhighlight{reproduction equation} for values of functions from the inner product.

\beginlecture{03}{16.04.24}

\begin{aremark}
    In this lecture $(\star)$ refers to the reproduction equation.
\end{aremark}

For $f=k(\cdot,y)$, we set $k(x,y)=\langle k(x,\cdot),k(y,\cdot)\rangle_H$.
We furthermore can observe $\forall f\in H,x\in\Omega$:
\[\vert \delta_x(f)\vert=\vert f(x)\vert=\vert \langle f,k(x,\cdot)\rangle_H\vert\leq \Vert f\Vert_H\Vert k(x,\cdot)\Vert_H=\Vert f\Vert_H \sqrt{K(x,x)}\]
and 
\[\langle \delta_x,\delta_y\rangle_L=\langle k(x,\cdot),k(y,\cdot)\rangle_H=k(x,y)\forall x,y\in \Omega\]
\begin{align*}
    \Vert \delta_x-\delta_y\Vert_L^2=\Vert \delta_x\Vert_L^2-2\langle \delta_x,\delta_y\rangle+\Vert \delta_y\Vert_L^2=k(x,x)-2\langle k(x,\cdot),k(y,\cdot)\rangle_H+k(y,y)
\end{align*}
is a \highlight{distance} on $\Omega$:
\[\text{dist}(x,y)\coloneqq \Vert \delta_x-\delta_y\Vert_L=\sqrt{k(x,x)-2\langle k(x,\cdot),k(y,\cdot)\rangle_H+k(y,y)}.\]

We see that for all $x,y\in\Omega$
\[\vert f(x)f(y)\vert\leq \Vert f\Vert_H\Vert\delta_x-\delta_y\Vert_L=\Vert f\Vert_H\text{dist}(x,y)\]
and therefore all functions in $H$ are continuous with respect to this distance.

\begin{theorem}\label{thm:1.9}
    Each symmetric positive definite kernel $k$ on a set $\Omega$ is the \dhighlight{reproducing kernel} of 
    a Hilbert space called the \dhighlight{native space} $\cH=\mathcal{N}_k$\marginnote{$\cH$ can be constructed as the closure of $H$} of the kernel.
    This Hilbert space is unique and it is a space of functions on $\Omega$. The kernel $k$ fulfills
    \begin{equation}\label{eq:002}
        \langle f,k(x,\cdot)\rangle_{\cH}=f(x)
    \end{equation}
\end{theorem}

\begin{proof}
    Citation: The existence of native spaces follows from standard Hilbert space arguments, see e.g. chapter 11 from the lecture notes of Schaback. 
\end{proof}

\begin{aremark}
    The good ideas are from Schaback, the errors are from me, Prof. Garcke\marginnote{The erros in this script are largely due to me :)}
\end{aremark}

Proof of uniqueness:

If $k$ is a reproducing kernel in a different Hilbert space $T$, we observe
\begin{align*}
    \langle k(x,\cdot),k(y,\cdot)\rangle_{\cH}=k(x,y)=\langle k(x,\cdot),k(y,\cdot)\rangle_T
\end{align*}
which shows that the inner products coincide on $H$.
Since $T$ is a Hilbert space, it must contain the closure $\mathcal{N}_k$ of $H$ as a closed subspace. 
For $T$ to be larger than $\cH$ non-zero element $f\in T$ must exist that is orthogonal to $\cN_k$ and in particular 
to $H$. We observe 
\begin{align*}
    f(x)=\langle f,k(x,\cdot) \rangle_T=0\hspace{2cm}\forall x\in\Omega.
\end{align*}
which is a contradiction to $f\neq 0$, because of ($\ref{eq:002}$) for $T$.

Dual spaces:

$\delta_x:\cN_k\to\R,f\mapsto f(x)$ for all $f\in\cN_k,x\in\Omega$.

The dual space $\cN_k^*$ of $\cN_k$ is again a Hilbert space.
\begin{align*}
    R:&\cN_k^*\to\cN_k\\
    \lambda(f)=&\langle f,R(\lambda)\rangle_{\cN_k}\forall f\in \cN_k \lambda\in \cN_k^*\\
    \langle \lambda,\mu\rangle_{\cN_k^*}&=\langle R(\lambda),R(\mu)\rangle_{\cN_k}\forall \lambda,\mu\in \cN_k^*
\end{align*}
Also via the reproducing equation $\ref{eq:002}$
\[\delta_x(f)=\langle f,k(x,\cdot)\rangle_{\cN_k}\forall f\in\cN_k,x\in\Omega.\]
So $k(x,\cdot)$ is the \highlight{Riesz representer} $R(\delta_x)$ of $\delta_x$ 
\begin{align*}
    \langle \delta_x,\delta_y\rangle_{\cN_k^*}=\langle R(\delta_x),R(\delta_y)\rangle_{\cN_k}=k(x,y)\hspace{2cm}&\forall x,y\in\Omega  \\
    \Vert\delta_x\Vert_{\cN_k^*}=\Vert k(x,\cdot)\Vert_{\cN_k}=\sqrt{k(x,x)}\hspace{2cm}&\forall x\in\Omega\\
    \lambda(f)=\langle f,\lambda^*k(x,\cdot)\rangle_{\cN_k}\hspace{2cm}&\forall f\in\cN_k,\lambda\in \cN_k^*
\end{align*}
so that $\lambda^* k(x,\cdot)$ is the Riesz representer of $\lambda$.

\section{Reproducing Kernel Hilbert Space (RKHS)}

\begin{definition}\label{def:RKHS}
    A Hilbert space $\cH$ of functions on a set $\Omega$ with an inner product 
    $\langle\cdot,\cdot\rangle_{\cH}$ is called \dhighlight{RKHS} if there is a kernel
    function $k:\Omega\times\Omega\to\R$ with $k(x,\cdot)\in\cH$ forall $x\in\Omega$ and 
    the reproducing kernel property 
    \[f(x)=\langle f,k(x,\cdot)\rangle_{\cH}\hspace{3cm}\forall x\in\Omega,f\in\cH\]
\end{definition}

This directly implies 
\begin{align*}
    k(y,x)=\langle k(y,\cdot),k(x,\cdot)\rangle_\cH=\langle k(x,\cdot),k(y,\cdot)\rangle_\cH=k(x,y).
\end{align*}

For positive semi-definiteness one can use a Gram matrix argument or 

take any $X=\{x_1,\dots,x_N\}\subset\Omega$ and $a\in\R^n$
\begin{align*}
    \sum_{j,k=1}^N a_j a_k k(x_j,x_k)&=\sum_{j,k=1}^N a_ja_k\langle k(x,\cdot),k(y,\cdot)\rangle_\cH\\
    &=\langle \sum_{j=1}^N a_jk(x_j,\cdot),\sum_{k=1}^N a_k k(x_k,\cdot)\rangle_\cH  \\
    &=\Vert \sum_{j=1}^N a_j k(x,\cdot)\Vert_\cH^2\geq 0
\end{align*}

\begin{theorem}\label{thm:1.11}
    Each Hilbert space $\cH$ of real valued functions on some set $\Omega$ with point evaluation functionals 
    \[\delta_x:f\mapsto f(x)\hspace{3cm}\forall f\in\cH\]
    is a  RKHS with a unique positive definite kernel $k$ on $\Omega$. The kernel is 
    uniquely defined by providing the Riesz representers of the (continuous) point evaluation functionals. The space $\cH$ is the 
    native space of the kernel.
\end{theorem}

\begin{proof}
    Under the given hypothesis, there must be a Riesz representer of $\delta_x$. By the definition of th Riesz map 
    it takes the form $k(x,\cdot)\in\cH$ satisfying the reproduction equation \ref{eq:002}.
    
    In other words, any such Hilbert space has a symmetric positive definite kernel.

    The final statement follows from theorem \ref{thm:1.9}, because the native space and $\cH$ are Hilbert spaces 
    that contain all $k(x,y)$.
\end{proof}

\begin{theorem}\label{thm:1.12}
    If a Hilbert (sub-)space of functions on $\Omega$ has a finite orthogonal basis
    $v_1,\dots,v_N$ the reproducing kernel is 
    \[k_N(x,\cdot)\sum_{j=1}^N v_j(x)v_j(\cdot)\hspace{1cm} \forall x\in\Omega\]
    In case of a subspace we have \marginnote{Which in some sense means that larger dimensions of the subspace can't add too much to the norm}
    \[\sum_{j=1}^N \vert v_j(x)\vert^2=k_N(x,x)\leq k(x,x)\hspace{1cm}\forall x\in\Omega\] 
\end{theorem}

\begin{proof}
    The kernel must have a representation in the ONB 
    \[k_N(x,\cdot)=\sum_{j=1}^N\langle k_N(x,\cdot),v_j\rangle_\cH v_j(\cdot)\stackrel{(\ref{eq:002})}{=}\sum_{j=1}^N v_j(x)v_j(\cdot)\]    
    For the subspace, % Weil pont eval
    \begin{align*}
        k_N(x,x)&=\langle k_N(x,\cdot),k_N(x,\cdot)\rangle_\cH&\\
        &\stackrel{\text{Hilbert subspace}}{=}\langle k_N(x,\cdot),k(x,\cdot)\rangle_{\cH}&\\
        &\leq\sqrt{k_N(x,x)}\sqrt{K(x,x)}&\forall x\in\Omega
    \end{align*}
\end{proof}

% TODO: THM 7/8 warum surjektiv
\begin{aremark}
    The subspace property does not hold for arbitrary Hilbert spaces, this tells us that a RKHS is really not the same as a normal Hilbert space!
\end{aremark}

\beginlecture{04}{18.04.24}

Remember: Kernels of the Mercer form 
\[k(x,y)=\sum_{i\in I}\lambda_i \varphi_i(x)\varphi_i(y)\]
with the summability condition 
\[k(x,x)=\sum_{i\in I}\lambda_i \varphi_i^2(x)<\infty.\]

Then observe 
\begin{align*}
    |f(x)|&=\left\vert \sum_{i\in I}\langle f,\varphi_i\rangle_\cH\phi_i(x)\right\vert\\
    &\leq\sum_{i\in I}\left\vert \frac{\langle f,\varphi_i\rangle_\cH}{\sqrt{\lambda_i}}\right\vert|\varphi_i(x)|\sqrt{\lambda_i}\\
    &\leq \sqrt{\sum_{i\in I}\frac{\langle f,\varphi_i\rangle_\cH}{\sqrt{\lambda_i}}}\sqrt{\underbrace{\sum_{i\in I}\varphi_i^2(x)\lambda_i}_{<\infty}}
\end{align*}

\[\cH\coloneqq\left\{f\in \cH: \Vert f\Vert_\lambda^2=\sum_{i\in I}\frac{\langle f,\varphi_i\rangle_\cH}{\sqrt{\lambda_i}}\right\}\]

\begin{equation}\label{eq:003}
    \langle f,g\rangle_\lambda = \sum_{i\in I}\frac{\langle f,\varphi_i\rangle_\cH\langle g,\varphi_i\rangle_\cH}{\lambda_i}\hspace{2cm} \forall f,g\in\cH_\lambda  
\end{equation}

Using \ref{eq:003} as the kernel, we have to check if all $f_x\coloneqq k(x,\cdot)\in\cH_\lambda$.

Observe 
\begin{align*}
    \langle f_x,\varphi\rangle_\cH=\lambda_i\varphi_i(x)
\end{align*}
and \begin{align*}
    \sum_{i\in I}\frac{\langle f_x,\varphi_i\rangle_\cH^2}{\lambda_i}=\sum_{i\in I}\lambda_i \varphi_i^2(x)<\infty
\end{align*}
to see $f_x\in \cH_\lambda$.

\highlight{Check the reproduction equation}

\begin{align*}
    \langle f,k(x,\cdot)\rangle_\lambda&= \sum_{i\in I}\frac{\langle f,\varphi_i\rangle_\cH\langle k(x,\cdot),\varphi_i\rangle_\cH}{\lambda_i}\\
    &=\sum_{i\in I} \frac{\langle f,\varphi_i\rangle\lambda_i\varphi_i(x)}{\lambda_i}=f(x)\hspace{2cm}\forall x\in\Omega
\end{align*}

The kernel therefore reproduces on $\cH_\lambda$. The proves theorem \ref{thm1.13}.

If a Hilbert space of functions on $\Omega$ has a countable ONB $\{\varphi_i\}_{i\in I}$, each summability condition (**) leads to a reproducing mercer kernel (*) for a
suitable subspace of functions with continuous point evaluations.
%TOFIX 3=*= ist mercer kernel of he form und die summe ist **

\begin{corollary}\label{cor:1.14}
    The spaces $\cH_\lambda$ defined above are the natives spaces of the corresponding
    Mercer kernels.
\end{corollary}

\begin{example}[Trigometric polynomials]
    Consider the space of trigometric polynomials $\frac{1}{\sqrt{2}},\cos(nx),\sin(nx),n\in\N$ 
    which are \highlight{orthonormal} in the inner product
    \[\langle f,g\rangle=\frac{1}{\pi}\int_{-\pi}^{\pi} f(t)g(t)dt.\]

    With $I=(0,0)\cup (\N,0)\cup(0,\N)$
    \[\varphi_i(x)=\begin{cases}
        \frac{1}{\sqrt{2}} & i=(0,0)\\
        \cos(nx) & i=(n,0), n\geq 1\\
        \sin(nx) & i=(n,0), n\geq 1
    \end{cases}\]
    So for $f\in \cH$
    \[f=\sum_{i\in I}\langle f,\varphi_i \rangle_\cH \varphi_i.\]

    All $\varphi_i$ are uniformly bounded, so the summability condition does hold when the weights are summable.

    Fixing some $m\geq 1$, we define 
    \[\lambda_i=\begin{cases}
        1 & i=(0,0)\\
        n^{-2m} & \text{otherwise}
    \end{cases}\]

    We set the Mercer kernel\marginnote{this can also be tought of as an \highlight{extension kernel}}
    
    \[k_{2m}(x,y)\coloneqq \frac{1}{\sqrt{2}}+\sum_{n=1}^\infty n^{-2m}(\cos(nx)\cos(ny)+\sin(nx)\sin(ny))\]\marginnote{This can be rewritten with the usual trigonometric rules}

    One can see $K_{2m}''=K_{2m-2}$, so $K_2m$ piecewise polynomial of degree $2m$, which is 
    $2m-2$ times differentiable.

    %Insert picture


\end{example}

\subsection{Kernels for subspaces}

Let us fix a nonempty set $X\subset\Omega$ and look at 
the closed subspace 
\[\cH_X\coloneqq \overline{\text{span}\{k(x,\cdot)|x\in X\}}\subseteq\cH\]

Projector for $\cH$ to the closed subspace $\cH_0$: $\pi_0:\cH\to\cH_0$ with properties \marginnote{This is NOT $\{\cH_0\}$, but a generic subspace}
\begin{itemize}
    \item $\pi_0^2=\pi_0$
    \item $\pi_0$ gives unique best approximation in $\cH_0$, $u\mapsto u_0$ %SEE theorem 1.17
    \item $u_0\perp u-u_0$
    \item $\text{Id}-\pi_0$ projects onto the orthogonal complement $\cH_0^\perp=\{u\in\cH\mid \langle u,v\rangle_\cH\forall v\in \cH_0\}$
    \item $\cH=\cH_0+\cH_0^\perp$
\end{itemize}

\begin{theorem}\label{thm:1.15}
    Let $\cH_0$ be a closed subspace of $\cH$ with reproducing kernel $k_0$ and 
    let $\pi_0:\cH\to\cH_0$ be the projection onto $\cH_0$.

    The subspace kernel is 
    \[k_0(x,\cdot)= \pi_0k(x,\cdot)\]
    for all $x\in\Omega$. The reproducing kernel for the orthogonal 
    complement $\cH_0^\perp$ is $k-k_0$.
\end{theorem}

\begin{proof}
    $\text{Id}=\pi_0+(\text{Id}-\pi_0)=\pi_0+\pi_0^\perp$.

    Thus $f(x)=(\pi_0\circ f)(x)+(\pi_0^\perp\circ f)(x)$ inserted into the
    reproducing equation 
    \begin{align*}
        f(x)&=\langle f,k(x,\cdot)\rangle_\cH\\
        &=\langle \pi_0f+\pi_0^\perp f,\pi_0k(x,\cdot)+\pi_0^\perp k(x,\cdot)\rangle_\cH \\
        &=\langle \pi_0 f,\pi_0 k(x,\cdot)\rangle+\langle \pi_0^\perp f,\pi_0^\perp k(x,\cdot)\rangle
    \end{align*}
    Using $f\in \cH_0$ and $f\in \cH_0^\perp$ eliminates on part of the sum each and the statements follow.
\end{proof}

\begin{remark}
    Orthogonal space decompositions correspond to additive kernel decompositions using the appropriate
    projections.
\end{remark}

\begin{theorem}\label{thm:1.16}\marginnote{This is a statement we would expect for our later use case of approximation and interpolation}
    Let $X\subseteq\Omega$ be nonempty. 
    For the closed subspace $\cH_X$ it holds 
    \[\cH_X^\perp=\{f\mid f\in \cH: f(X)=\{0\}\}.\]
\end{theorem}

\begin{proof}
    If $f(X)=\{0\}$, then $\langle f,v\rangle_\cH=0\forall v\in \cH_X$.
    \[f(x)=\langle f,k(x,\cdot)\rangle_\cH\]
    since $f\in\cH_X^\perp$ by the reproduction equation and conversely we set for 
    $f\in\cH_X^\perp$ that $f(X)=\{0\}$.
\end{proof}

With $\pi_X$ the projector from $\cH$ to $\cH_X$ we denote 
\[f_X\coloneqq \pi_X(f).\]
Standard results from Hilbert space theory gives us 
\begin{theorem}\label{thm:1.17}
    Each function $f\in\cH$ has an orthogonal decomposition 
    \[f=f_X+f_{X^\perp}\]
    with $f_X\in\cH_X$ and $f_{X^\perp}\in\cH_X^\perp$. In particular 
    each $f\in \cH$ has an interpolant $f_X\in\cH_X$ recovering the values of $f$ on $X$.
    Additionally 
    \begin{align*}
        \Vert f-f_X\Vert_{\cH}=\inf_{g\in\cH_X} \Vert f-g\Vert_\cH
    \end{align*}
    and 
    \[\Vert f_X\Vert_\cH=\inf_{g\in\cH:\stackrel{f(x)=g(x)}{\forall x\in X}}\Vert g\Vert_\cH=\inf_{v\in\cH_{X^\perp}}\Vert f-v\Vert_\cH\]
\end{theorem}

\begin{corollary}\label{cor:1.18}\marginnote{This is just the previous theorem in words}
    The interpolant $f_X\in\cH_X$ to a function $f$ on $X$ is at the same time 
    the best approximation to $f$ from all functions in $\cH_X$.    
\end{corollary}

\begin{corollary}\label{cor:1.19}\marginnote{This property is usefull, if the norm encodes smoothness as well. Penalizing unnecessary changes of the function}
    The interpolant $f_X\in\cH_X$ to a function $f$ on $X$ minimizes the norm 
    under all interpolants from the full space $\cH$.
\end{corollary}

\begin{corollary}\label{cor:1.20}
    For all sets $X\subseteq Y\subseteq \Omega$ and $f\in \cH$ we have 
    \[\Vert f_X\Vert_\cH\leq \Vert f_Y\Vert_\cH \leq \Vert f\Vert_\cH\]
    and 
    \[\Vert f\Vert_{\cH}\geq \Vert f-f_X\Vert_\cH\geq\Vert f-f_y\Vert_{\cH}\]
    where for completeness we define $f_\emptyset=0,f_{\emptyset^\perp}=f$ and 
    $\cH_{\emptyset}=\{0\}$ with $\cH_{\emptyset^\perp}=\cH$.
\end{corollary}


\beginlecture{05}{23.04.24}

Consider only $f(\cdot)=k(x,\cdot)$ for a fixed $f,x\in\Omega$.

\begin{definition}\label{def:2.1:power_function}
    The function \[P_X(x)\coloneqq \Vert k(x,\cdot)-k_X(x,\cdot)\Vert_\cH\hspace{2cm} x\in\Omega\]
    is called \dhighlight{power function} w.r.t. the set $X$ and the kernel $k$.

    A different definition goes with the \dhighlight{error functional} $\epsilon_{X,x}\in\cH^*$
    \[\epsilon_{X,x} f\mapsto f(x)-(\Pi_X(f))(x).\]

    The power function is then defined as $P_X(x)\coloneqq \Vert \epsilon_{X,x}\Vert_{\cH^*}$.
\end{definition}

\begin{theorem}\label{thm:2.2}
    The two definitions for the power function are equivalent. 
    $P_X$ has the following properties
    \begin{enumerate}
        \item $P_X(x)=0$ $\forall x\in X$
        \item $P_\emptyset(x)^2=k(x,x)$ $\forall x\in \Omega$
        \item $P_\Omega(x)=0$ $\forall x\in\Omega$
        \item $0=P_\Omega(x)\leq P_Y(x)\leq P_X(x)\leq P_\emptyset(x)$ for $X\subseteq Y\subseteq \Omega$
        \item $P_X(x)=\inf_{g\in\cH_X} \Vert k(x,\cdot)-g\Vert_\cH$ 
        \item $P_X(x)=\sup_{f\in\cH,\Vert f\Vert_\cH \leq 1,f(X)=\{0\}} f(x)$ $\forall x\in\Omega$
        \item $\forall x\in \Omega,f\in\cH$\[|f(x)-f_X(x)|=\vert f_X^\perp(x)\vert\leq P_X(x)\Vert f_X^\perp(x)\Vert_\cH= P_X(x)\Vert f-f_X\Vert_\cH\leq P_X(x)\Vert f\Vert_\cH\]
    \end{enumerate}
\end{theorem}

\begin{aremark}
    General approximation goal: Split the approximation error into an error of the space and an error of the function (similarly to SC1).


    One aim is to generalize 7. to not rely on a specific point.
\end{aremark}

\begin{proof}
    Due to $\langle\epsilon_{X,x},\epsilon_{X,x}\rangle_{\cH^*}=\langle R(\epsilon_{X,x}),R(\epsilon_{X,x})\rangle_\cH$
    we have to show that the Riesz representer of $\delta_x\circ \Pi_X$ is $K_X(x,\cdot)$.

    \begin{align*}
        \langle f,R(\delta_x\circ\Pi_X)\rangle &= \delta_x\circ \Pi_X(f) = f_X(x)=\langle f_X,k(x,\cdot)\rangle_\cH\\
        &=\langle f_X,k_X(x,\cdot)+k_{X^\perp}(x,\cdot) \rangle_\cH\\
        &\stackrel{f_X\perp K_{X^\perp}}{=}\langle f_X,k_X(x,\cdot)\rangle_\cH=\langle f-f_{X^\perp},k_X(x,\cdot)\rangle_\cH\\
        &\stackrel{f_{X^\perp}\perp K_X}{=}\langle f,k_X(x,\cdot)\rangle_\cH
    \end{align*}

    Proof of 7.:

    \begin{align*}
        f(x)-f_X(x)&=f_{X^\perp}(x)=\langle f_{X^\perp},k(x,\cdot) \rangle_\cH\\
        &=\langle f_{X^\perp},k(x,\cdot)-\underbrace{k_X(x,\cdot)}_{ f_{X^\perp}\perp}k_X(x,\cdot)\rangle\\
        &\stackrel{\text{C.S.}}{\leq}\dots=\Vert f_{X^{\perp}}\Vert\cH P_X(x) 
    \end{align*}

    Proof of 6.:

    We see from the first inequality 
    \begin{align*}
        P_X(x)\geq \sup_{\Vert f_{X^\perp}\Vert_\cH\leq 1} |f_{X^\perp}(x)|
    \end{align*}

    and equality must hold for the representer of $\epsilon_{X,x}$.\marginnote{Notice the connection to operator norm approaches to 6.}

\end{proof}

\begin{remark}
    Consider the subspace $\cH_X^*=\overline{\text{span}\{\delta_x\mid x\in X\}}$ of the 
    dual space of $\cH$. Then $5.$ can equivalently be given as 
    \begin{equation}\label{eq:004}
        P_X(x)= \inf_{\lambda\in \cH_X^*}\Vert \delta_x\Vert_{\cH^\perp}  
    \end{equation} 
\end{remark}


% Unique solution if in cH, else just a solution


Consider the interpolation of $f(x)=k(x,\cdot)\in\cH$.

For $x\in\Omega$ we get for the interpolant in $\cH_X$
\begin{equation}\label{eq:005}
  k(x_k,x)=\sum_{j=1}^Nu_j(x)k(x_j,x_k) \hspace{2cm} 1\leq k\leq N  
\end{equation}
which has solution coefficients $u_j(x)$ as a function on $\Omega$.

\begin{aremark}
    If the kernel matrix is invertible, $u_j$ is either 0 or 1. See Lagrange interpolation?

    In our setting it is enough to know that it exists, but might not be unique.
\end{aremark}

\begin{theorem}\label{thm:2.3}
    If the kernel matrix is non-singular the $u_j$ from \ref{eq:004} are $\in\cH_X$ and 
    there is a Lagrange basis $u_j(x_k)=\delta_{jk},1\leq j,k\leq N$.
    
    In general it still holds\marginnote{This is sometimes called quasi-interpolation} \[f_X(x)=\sum_{j=1}^N u_j(x)f(x_j)\]

    In the formula the influence of $X$ and $f$ are separated.

\end{theorem}


\begin{proof}
    The first statement follows from \ref{eq:005}. From the second:
    \begin{align*}
        f_X(x)&=\sum_{k=1}^N a_kk(x_k,x)\\
              &=\sum_{k=1}^N a_k\sum_{j=1}^N u_j(x)k(x_j,x_k)\\
              &=\sum_{j=1}^N a_j(x)\underbrace{\sum_{k=1}^{N} a_kk(x_j,x_k)}_{=f_{a,X}=f(x_j)\forall x_j\in X}=\sum_{j=1}^N u_j(x)f(x_j)\qedhere
    \end{align*}
\end{proof}

\begin{theorem}\label{thm:2.4}
    The power function has the following explicit representation:
    \[P_X(x)=k(x,x)-2\sum_{j=1}^N u_j(x)k(x_j,x)+\sum_{j=1}^N\sum_{k=1}^{N}u_j(x)u_k(x)k(x_j,x_k)=k(x,x)-k_X(x,x)\]
\end{theorem}

\begin{proof}
    For $K_X\in \cH_X$ $k_X(x,z)=\sum_{j=1}^{N}u_j(x)k(x_j,z)$
    \begin{align*}
        P_X^2(x)&=\langle k(x,\cdot)-k_X(x,\cdot),k(x,\cdot)-k_X(x,\cdot) \rangle_\cH\\
        &=k(x,x)-2\langle k(x,\cdot),\sum_{j=1}^N u_j(x)k(x_j,\cdot) \rangle_\cH+\sum_{j=1}^N\underbrace{\sum_{k=1}^{N}}_{a}u_j(x)\underbrace{u_k(x)k(x_j,x_k)}_{\text{with a } =k(x,x_j)}\\
        &=k(x,x)-\underbrace{\sum_{j=1}^N u_j(x)k(x_j,x)}_{k_X(x,x)}\qedhere
    \end{align*}    
\end{proof}

Consider $f_X(x)=\sum_{j=1}^Nu_j(x)f(x_j)$ the interpolant on $X$.

Let us also consider arbitrary estimation formulas 
\[(x,f)\mapsto \sum_{j=1}^N v_j(x)f(x_j)\]
with no assumptions on the scalars $v_j$. For fixed $x$ we get for the 
error functional $f\mapsto f(x)-\sum_{j=1}^{N} v_j(x)f(x_j)=\left(\delta_x \sum_{j=1}^N v_j(x)\delta_{x_j}\right)(f)$.

Ad for optimal estimation for all $f\in\cH$, we should choose 
$v_j$ to minimize the following expression:
\[V_{X,v}(x)\coloneqq \Vert \delta_x-\sum_{j=1}^{N} v_j(x)\delta_{x_j}\Vert_{\cH^*}.\]

Remember the dual form of the fifth property \ref{eq:004}:
\[P_X(x)=\inf_{\lambda\in\cH^*}\Vert \delta_x-\lambda\Vert_{\cH^*}\] 
we also saw that the function $u_j$ are the solution.

We also see that the optimal error, in the worst case sense, is described to be the power function.

\begin{theorem}\label{thm:2.5}
    In the above sense, kernel based approximation yields the best linear 
    estimation of unknown function values $f(x)$ from known function values $f(x_j)$
    at points $x_j$.
\end{theorem}

\beginlecture{06}{25.04.24}
% STATISTICS, Yay <3
\[k(s,t)=\text{cov}(X_s,X_t)\]
The kernel comes from a covariance, where 
for every $t$ in some $\Omega$, we have a random variable with finite  second moments.

Consider $X_t$ with zero mean. In this case, what we did is called 
\dhighlight{(simple) Krigihs}\marginnote{connection to geo-statistics!} % TODO: Nachschauen wie er hieß

$V_{X,v}^2(x)$ can be understood as the variance of the prediction error.

Now define the error of some general linear predictor at $x$
\[\mathcal{E}_{X,V,x}\coloneqq X_x-\sum_{j=1}^n V_j(x)X_{x_j}\] % Nachschauen ob v groß oder klein

\marginnote{Statistics pov: This is an ubiased estimator}

\begin{align*}
    \bE(\mathcal{E}_{X,V,x}^2)&=\underbrace{\cov}_{=k}(X_x,X_x)-2\sum_{j=1}^N V_j(x)\cov(X_x,X_{x_j})+\sum_{j=1}^N\sum_{k=1}^N v_j(x)v_k(x)\cov(X_{x_j},X_{x_k})\\
    &=\langle \delta_x, \delta_x\rangle_{\cH^*}-2\sum_{j=1}^{N}v_j\langle \delta_x,\delta_{x_j}\rangle_{\cH^*}+\sum_{j=1}^{N}\sum_{k=1}^N v_j(x)v_k(x)\langle \delta_{x_j},\delta_{x_k}\rangle_{\cH^*}\\
    &=V_{X,v}^2(x)
\end{align*}

Short revision of the condition number:

\begin{align*}
    \kappa(A)&=\frac{|\lambda_{\max} (A)|}{|\lambda_{\min} (A)|}\\
    Ax=b & \frac{\Delta x}{|x|}\leq \kappa(A)\frac{|\Delta b|}{|b|}
\end{align*}

\dhighlight{Power function and stability}

There is an uncertainty principle (Schabach 1995):\marginnote{We can somehow fix this via regularisation, which in this case is both usefull numerically and meaningful from a statistics point of view}

It is impossible to make the power function and the condition number 
of the kernel matrix small at the same time. 

To make this precise: enrich $X=\{x_1,\dots,x_n\}$ with another point $X_0=x$
and define $u_0(\cdot)=-1$
\[A=k(x_j,x_k)_{0\leq j,k\leq N}\]
\begin{align*}
    u^\perp A u&=\sum_{j=0}^N\sum_{k=0}^N u_j(x)u_k(x)k(x_j,x_k)\\
    &\stackrel{u_0=-1}{=}k(x,x)-2\sum_{j=1}^Nu_j(x)k(x,x_j)+2\sum_{j=1}^N\sum_{k=1}^Nu_j(x)u_k(x)k(x_j,x_k)\\
    &\stackrel{\text{thm. \ref{thm:2.4}}}{=}P_X^2(x)
\end{align*}

$A$ has $n+1$ non-negative real eigenvalues $\lambda_0\geq \lambda_1\geq \dots\geq \lambda_N\geq 0$.

\begin{align*}
    \lambda_N\Vert u\Vert_2^2\leq u^\perp A u\leq \lambda_0\Vert u\Vert_2^2
\end{align*}

gives 
\[P_X^2(x)\geq \lambda_N\left(1+\sum_{j=1}^N |u_j(x)|^2 \right)\geq \lambda_N\]

Elimination of the special value of the point $x$ gives 

\begin{theorem}\label{thm:2.6}
    The kernel matrix for $N$ points $\{x_1,\dots,x_n\}=X$ has a smallest\marginnote{This holds for every $x\in \Omega$}
    eigenvalue $\lambda$ bounded from above by \[\lambda\leq \min_{1\leq j\leq N}P_{X\setminus\{x_j\}}^2\]
\end{theorem}

Back to approximation

\begin{align*}
    \vert f(x)-f_X(x)\vert\leq\vert P_X(x)\vert \Vert f\Vert_\cH
\end{align*}

Assume that any directional derivative of both $f$ and $f_X$ is bounded 
by some $C$:
\begin{align*}
    \vert f(x)-f_X(x)\vert \leq \underbrace{\vert f(x_j)-f_X(x_j)\vert}_{=0} +2C\Vert x-x_j\Vert_2\leq 2Ch_{X,\Omega} 
\end{align*}

if the connecting line between $x$ and $x_j$ is in $\Omega$ % Bld einfügen!

\begin{definition}\label{def:2.7:fill_distance}\marginnote{How large are the holes in $\Omega$ such that we don't hit an $x\in X$.  It describes the size of the largest data free domain}
    The \dhighlight{fill distance} of a set of points $X\subseteq \Omega$ for a 
    bounded $\Omega$ is defined to be 
    \[h_{X,\Omega}=\sup_{x\in\Omega}\min_{1\leq j\leq N}\Vert x-x_j\Vert_2\]
\end{definition}


To get bounds on the optimal approximation, we can consider 
some functions $v_0,\dots,v_N$ instead of the optimal $u_j$.

\begin{equation}\label{eq:006}
    P_X^2(x)\leq k(x,x)-2\sum_{j=1}^N v_j(x)k(x_j,x)+\sum_{j=1}^{N}\sum_{k=1}^{N}v_j(x)v_k(x)k(x_j,x_k)
\end{equation}

The simplest case uses nearest neighbor reconstruction. 

Assume that for each $x\in\Omega$ we pick a $x_{nn(x)}$ and define 
\[v_j(x)=\begin{cases}
    1 & j=nn(x)\\
    0 & \text{else}
\end{cases}\]

Then 
\begin{align*}
    P_X^2(x)&\leq k(x,x)-2k(x_{nn(x)},x+k(x_{nn(x)},x_{nn(x)}))\\
    &\stackrel{}{(7)}d_k((x,x_{nn(x)}))^2=\text{dist}(x,x_{nn(x)})^2
\end{align*}

\begin{theorem}\label{thm2.8}
    For $k$ positive semi-definite the power function on 
    non-empty sets $X$ of interpolation points satisfies 
    \[p_X^2(x)\leq \min_{x_j\in X} d_k(x,x_j)\] 
    with $d_k$ as in (7). % TODO
\end{theorem}


Assume that we can prove $P_X(x)\leq CE(x,h)$ for all 
data sets $X$ with fill distance $h$.

This implies (by theorem \ref{thm:2.2})
\begin{align*}
    |f(x)-\sum_{j=1}^N u_j(x)f(x_j)|\leq CE(x,h)\Vert f\Vert_\cH
\end{align*}

\marginnote{This simplifies the notation, once one understands what is happening :)}

We know introduce the error operator 
\[E_x^y(f(y))\coloneqq f(x)-\sum_{j=1}^Nv_j(x)f(x_j)\]
to set 
\begin{align*}
    P_X^2(x)&\leq (\ref{eq:006})=k(x,x)+\sum_{j=1}^N v_j(x)\left(\sum_{k=1}^{N}v_k(x)k(x_j,x_k)-k(x_j,x)\right)\\
    &=E_x^z(k(z,x))-\sum_{j=1}^N v_j(x)E_x^z(k(z,x_j))\\
    &=E_x^yE_x^z(k(z,y))
\end{align*}

We are after a bound such as 
\[\vert E_x^y(f(y))\vert =\left\vert f(x)-\sum_{j=1}^N v_j(x)f(x_j) \right\vert\leq \mathcal{E}_{X,k}(h)\Vert L f\Vert \]
We then bound the power function by 
\[P_X^2(x)\left\vert E_x^yE_x^z k(z,y) \right\vert\leq \mathcal{E}_{x,k}(h)\Vert L^y E_x^z k(z,y)\Vert\leq \mathcal{E}_{X,k}^2\Vert L^y\Vert \Vert L^z k(y,z)\Vert \]
assume the final expression makes sense.


\beginlecture{07}{30.04.24}

First, univariate case, $\Omega=[a,b], X=\{x_1,\dots,x_n\}\subset\Omega$. For a 
given $x\in\Omega$
\[X_x=\{x_j\in X\mid j\in N(x)\subseteq \{1,\dots,N\}\}\]

$f\in C^k$, Taylor polynomial at $x_0$
\[p(x)=\sum_{j=0}^{k-1}\frac{f^{(j)}(x_0)}{j!}(x-x_0)^j\]
for $|x-x_0|\leq h$ we have the local approximation error 
\[|f(x)-p(x)|=\frac{\vert f^{(k)}\vert}{n!}|x-x_0|^k\leq C(f)h^k\]
\marginnote{This approximation error holds for approximation which recovers polynomials locally!} % TODO: Check

From formula for interpolation in Newton form 
\begin{align*}
    f(x)-p_{X}(x)&={[x,X_x]f} \prod_{x_j\in X_x} (x-x_j)\forall x\in [a,b]\\
    &\leq \frac{\Vert f^{(k)}\Vert_{\infty,[a,b]}}{k!}\underbrace{\prod_{x_j\in X_x} (x-x_j)}_{\leq C h^k}
\end{align*}

\marginnote{If we are careful, we can controll the constant $C$ above!}

We can now use this to bound 
\begin{align*}
    p_X^2(x)\leq (Ch^k)^2\sup_{a\leq z\leq b}\sup_{a\leq y\leq b}\left\vert \frac{\partial^k}{\partial z^k}\frac{\partial^k}{\partial y^k}k(a,b)  \right\vert %TODO
\end{align*}

\begin{theorem}\label{thm:29}
    Assume a psd kernel $k$ on $[a,b]$ that is $k$ times differentiable. Then 
    for every point set $X\subset [a,b]$ of at least $k$ points with 
    fill distance at most $h$ the power function can be bounded 
    \begin{equation}
        P_X(x)\leq C_k h^k
    \end{equation}
    with $C_k$ depending on $k$ and $X$. 
\end{theorem}

\begin{definition}\label{def:30}
A compact domain $\Omega\subset\R^d$ allows

\dhighlight{uniformly stable local polynomial reproduction}
of order $l\geq 1$, if there are positive constants $c_1,c_2,h_0$, s.t. 
for all finite sets of points $X\coloneqq \{x_1,\dots,x_n\}$ with fill distance 
$h_{X,\Omega}\leq h_0$ there are scalar functions $u_1(x),\dots,x_N(x)$ s.t. 
\begin{enumerate}
    \item \[\sum_{j=1}^{N} u_j(x)p(x_j)=p(x)\] for all polynomials $p\in \mathcal{P}_l^d$ and $x\in\Omega$.   
    \item $\sum_{j=1}^N \vert u_j(x)\vert\leq c_1$
    \item $u_j(x)=0$ if $\Vert x- x_j\Vert_2>C_s h_{X,\Omega}$.
\end{enumerate} 
\end{definition}

\begin{aremark}
    We will only handle positive definite kernels, the results generalizes for psd kernels with additional conditions on the kernel.
\end{aremark}

We know focus on positive definite kernels. 
\begin{theorem}\label{thm:31}
    Let $\Omega\subset \R^d$ and let $k:\Omega\to\Omega$ be a p.d. kernel. 
    Let $X$ be a set of $N$ distinct points of $\Omega$ and define the quadratic form 
    $\Omega:\R^n\to\R$ for any $x\in\Omega$ (see \ref{thm:2.4})
    \[Q(u)=k(x,x)-2\sum_{j=1}^N a_j(x,x_j)+\sum_{j=1}^{N}\sum_{k=1}^{N}u_ju_kk(x_k,x_j)\]
    The min of $Q(u)$ is given for the vector from \ref{thm:2.3} denoted as $u^*$ with 
    $u_j^*(x_k)=\delta_{jk}$ and we have \[Q(u^*(x))\leq Q(u)\]
\end{theorem}

\begin{proof}
    With $b=[k(x_,\cdot),\dots,k(x_k,\cdot)]^\perp$ and $A_{i,j}=k(x_i,x_j)$, $i,j=1,\dots,N$ 
    we have 
    \[ Q(u)=k(x,x)-2 b^\perp (x)u+u^\perp A u.\]
    The min is the solution of $Au=b$\remark{in the positive definite setup only!},
    which is fullfilled by $u=u^*(x)$.
\end{proof}

\begin{theorem}\label{thm:32}
    Assume $\Omega\subseteq \R^d$ bounded and satisfies an interior cone condition.
    Suppose $k\in C^{2k}(\Omega\times \Omega)$ is a symmetric positive definite kernel 
    with native space $\cH$. Let $f_X$ be the interpolant to $f\in\cH$ on the set $X=\{x_1,\dots,x_n\}$.
    Then there are positive constants $h_0,C$ (independent of $x,f,k$) s.t. 
    \begin{equation*}
        |f(x)-f_X(x)|\leq Ch_{X,\Omega}^k \sqrt{C_k(x)}\Vert f\Vert_\cH
    \end{equation*} 
    provided $h_{X,\Omega}\leq Ch_0$. Here 
    \[C_k(x)=\max_{|\beta|=2k} \sup_{x,y\in \Omega\cap B(x,C_2h_{X,\Omega})}\left\vert D_2^\beta (k(x,y))\right\vert\]
\end{theorem}

First some further notation:

For $\beta=(\beta_1,\dots,\beta_d)\in\N_0^d$ we define 
\[D^\beta=\frac{\partial^{|\beta|}}{(\partial x_1)^{\beta_1}\cdot\dots\cdot (\partial x_d)^{\beta_d}}\]
with $D_2^\beta$ we indicate that $D^\beta$ is applied to $k(x,\cdot)$. Multivariate 
Taylor expansion of $k(x,\cdot)$ centered at x:
\[k(x,y)=\underbrace{\sum_{|\beta|<2l}\frac{D_2^\beta k(x,x)}{\beta!}}_{\sum_\beta T(x,\beta)}(y-x)^\beta+R(x,y)\] 

\[R(x,y)=\sum_{|\beta|=2l}\frac{D_2^\beta k(x,\xi_{x,y})}{\beta}(y-x)^\beta\]
where $\xi_{x,y}$ lines on the line connecting $x,y$.

\begin{proof}
    Theorem \ref{thm:2.2} $|f(x)-f_X(x)|\leq P_X(x)\Vert f\Vert_\cH$. 
    We know $P_X^2(x)=Q(u^*(x))$. Given the interior cone condition, we 
    can obtain a $u(x)$ that has polynomial precision of degree $l\geq 2k-1$. 
    For those $u$ we see 
    \begin{align*}
        P_X^2(x)\leq Q(u)=k(x,x)-2\sum_{k=1}^{N}u_k(x,x_k)+\sum_{j=1}^{N}\sum_{k=1}^{N} u_ju_k k(x_j,x_k)
\end{align*}

Apply Taylor expansion centered at $x$ to $K(x,\cdot)$ and centered at $x_j$ to $k(x_j,\cdot)$

\begin{align*}
    Q(u)=k(x,x)-2\sum_{k=1}^{N}u_K\left[\sum T(x,\beta)(x_k-x)^\beta+R(x,x_k) \right]+\sum_{j=1}^{N}\sum_{k=1}^N u_j u_k \left[\sum T(x_j,\beta)(x_k-x_j)^\beta R(x_j,x_k) \right]
\end{align*}

We identify $p(z)=(z-x)^\beta$, so that $p(x)=0$ unless $\beta=0$.

With the polynomial reproduction of $u$, this simplifies to 
\[Q(u)=k(x,x)-\underbrace{2k(x,x)}_{\beta=0}-2\sum_{k=1}^N u_k R(x,x_k)+\sum_{j=1}^{N} u_j\sum T(x_j,\beta)(x-x_j)^\beta+\sum_{j=1}^N\sum_{k=1}^N u_ju_k R(x_j,x_k)\] 

Look at Taylor expansion 

\begin{align*}
    \sum T(x_j,\beta)(x-x_j)^\beta = k(x_j,x)-R(x_j,x)
\end{align*}

This gives 
\begin{align*}
    Q(u)=k(x,x)-\sum_{k}u_k\left[2R(x,x_k)-\sum_{j=1}^N u_j R(x_j,x_k) \right]+\sum_{j=1}^N u_j \left[k(x_j,x)-R(x_j,x)\right]
\end{align*}

Once more Taylor: $k(x_j,x)=k(x,x_j)=\sum_\beta T(x,\beta)(x_x-x)^\beta+R(x,x_j)$

\begin{align*}
    =0-\sum_k u_k \left[R(x,x_k)-\sum_{j=1}^N u_j R(x_j,x_k)+R(x_k,x) \right]
\end{align*}

The polynomial reproduction gives \marginnote{uniform stability}
\[\sum_{k}|u_k|\leq C_1\]
For $u_j\neq 0$ (with 3.) we have $\Vert x-x_j\Vert_2 \leq C_2 h_{X,\Omega}$
and it holds $\Vert x_j-x_k\Vert \leq 2C_2h_{X,\Omega}$. Thereby all three can be bounded by
an expression such as $Ch_{X,\Omega}^{2k}C_k(x)$.

The interior cone condition shows that the ball remains inside, so that 
$C_k(x)$ is well defined. Combining these bounds and taking the square root gives 
the bound for the power function.
\end{proof}

\beginlecture{08}{02.05.24}

Other interesting functionals:

Derivatives: $\lambda(f)=\frac{\partial f }{\partial x_j}(x)$ \marginnote{This is what we will focus on for now} %TODO: FIX

Integration: $\lambda(f)=\int_\Omega f(y)dy$\marginnote{This yields quasi monte carlo integration}%TODO: FIX

Consider $\Lambda\subseteq \cH^*$ that generalizes the role of the point set $X$ and the associated $\delta_{x_i}$.

\[\{(x_i,\lambda_i f)\}_{i=1}^N\]
more general data instead of $\{(x_i,f(x_i))\}$.

Dirichlet boundary value problem :
\begin{eqnarray}\label{eq:008}
    Lu&=f&\text{ on }\Omega\subset \R^d\\
    u&=g&\text{ on }\partial \Omega\nonumber
\end{eqnarray}

with $L$ a linear differential operator.

\dhighlight{Collocation} is a general approach that discretizes this by  
%\begin{eqnarray*}\label{eq:009}
%    Lu(x_j^{\Omega})&=f(x_i^{\Omega}) & x_j^{}\Omega}\in\Omega, &1\leq j \leq N^{\Omega}\\
%    u(x_j^{\Gamma})&=g(x_j^{\Gamma}) & x_j^{\Gamma} \in\Gamma, &1 \leq j \leq N^{\Gamma}
%\end{eqnarray*} 

The exact solution $u^*$ of \ref{eq:008} solves \ref{eq:009}. 

Consider $U\subseteq \cH$
of dimension at least $N$. Let $u\in U$.

Assume $\lambda_i\in\cH^*$ of the form $\lambda_i=\delta_{x_i}\circ D^{\alpha(i)}$. 
Further assume that $\alpha(i)\neq\alpha(k)$ if $x_i=x_k$.

Then the $\lambda_i$ are linearly independent on the native space of a p.d. kernel(Compare: Wendland). 

We can proceed via \dhighlight{Hermite interpolation}. We assume $\{(x_i,\lambda_i f)\}_{i=1}^N,x_j\in\Omega,\lambda_j\in\Lambda$
with $\Lambda=\{\lambda_1,\dots,\lambda_n\}$ linearly independent set of continuous linear functionals.

\begin{eqnarray*}
    u(x)=\sum_{j=1}^N a_j \lambda_j^{(j)}k(x_j,x) & x\in\R^d
\end{eqnarray*}
that satisfies 
\begin{eqnarray*}
    \lambda_j u = \lambda_j f & j=1,\dots,N & f\in \cH % TODO lambda_i?
\end{eqnarray*}
$\lambda_i^{(1)}$ indicates that the functional acts on the first argument of $k$.

The LES has entries $A_{jk}\lambda_j^{(2)}\lambda_k^{(1)}k(x_k,x_j)$ for $j,k=1,\dots,N$.

\begin{example}
    Denote the centers of radial basis functions (RBFs) by $\xi_j$ and denote the data location 
    $\underline{x_j}$.\marginnote{$\underline{x_j}$ is the multiindex, $x$ is the first entry: $\underline{x_j}=(x,y)$} Given  \[\{(\underline{x_j},f(\underline{x_j}))\}_{i=1}^p\] 
    and 
    \[\{(\underline{x_j},\frac{\partial f}{\partial x})(\underline{x_j})_{j=p+1}^N\}\]
    Thus 
    \[\lambda_j = \begin{cases}
        \delta_{\underline{x_j}} & j=1,\dots,p\\
        \delta_{\underline{x_j}}\frac{\partial }{\partial x} & j=p+1,\dots,N
    \end{cases}\]
    with $k(\underline{x_j},\underline{x_k})=\varphi(\Vert \underline{x_j}-\underline{x_k\Vert})$
    \begin{eqnarray*}
        u(\underline{x})=\sum_{j=1}^N a_j k(\cdot, \underline{x})&=\sum_{j=1}^p a_j k(\cdot, \underline{x})+\sum_{j=p+1}^N a_j \frac{\partial}{\partial \xi_1} k(\xi,\underline{x})\\
        &=\sum_{j=1}^N a_jk(\xi_j,\underline{x})-\sum_{j=p+1}^N a_j \frac{\partial}{\partial x} k(\xi_j,\underline{x})
    \end{eqnarray*}
    since system matrix after ... $u$ into $\lambda_ju=\lambda_j f$\marginnote{CAREFUL: In this calculation a lot of stuff (everything with a non-constant index?) should have an underline? He added them inconsistently and often corrected himself, so I couldn't keep up. }
    \[\begin{bmatrix}
        K & K_\xi\\ K_x & K_{xx}
    \end{bmatrix}\]
    with $K_{jk}=K(\xi_k,x_j)=\varphi(\Vert \xi_k-x_j\Vert)$
    and 
    \begin{align*}
        K_{\xi,jk}&=\frac{\partial \varphi}{\partial \xi}\varphi(\Vert \xi_k-x_j\Vert) = -\frac{\partial \varphi}{\partial x}(\Vert \xi_k-x_j\Vert)\\
        K_{x,jk}&=\frac{\partial \varphi}{\partial x}(\Vert \xi_k,x_j\Vert) \\
        K_{xx,jk}&=\frac{\partial^2\varphi}{\partial x^2}(\Vert \xi_k,x_j)
    \end{align*}
\end{example}


\begin{aremark}
    We can do everything we did, but replacing point evaluations with point evaluations of
    (weak) derivatives to get a similar theory weak derivatives, without relying on point evaluations.
\end{aremark}

To measure errors we use generalized power functions 
\[P_\Lambda(\mu)=\Vert\mu_\mu\circ\Pi_\Lambda\Vert_{\cH^*}\]
where we use $\mu\in\cH^*$ to measure the error instead of 
point evaluation functionals
\[\mu(f-f_\lambda)=(\mu-\mu\circ\Pi_\Lambda)f.\]

\begin{eqnarray*}
    Lu&=f & \Omega\subseteq\R^d\\
    u&=g&\gamma=\partial\Omega
\end{eqnarray*}
with kernel based collocation method. 

We use 
\begin{equation}\label{eq:010}
    u(x)=\underbrace{\sum_{j=1}^p a_j k(x_j,x)}_{\text{boundary } B}+\underbrace{\sum_{j=p+1}^N a_j L^{(1)}k(x_j,x)}_{\text{interior }I}
\end{equation}

$X=B\cup I$.

We get a block matrix 
\[A=\begin{bmatrix}
    K & L^{(1)}K\\
    L^{(2)}K & L^{(2)}L^{(1)}K 
\end{bmatrix}\]
where $Au=\begin{bmatrix}
    g\\f
\end{bmatrix}.$
% Kernel differentiability factor 2 of what we want for the function space?

with 
\[\begin{cases}
    \\
    
\end{cases}  \]
\begin{eqnarray*}
    K_{j,k} &= k(x_j,x_k) & x_j,x_k\in B\\
    L^{(1)}K_{j,k} &=L^{(1)}K(\tilde{x}_k,x_j)&x_j\in B \tilde{x}_k\in I \\
    L^{(2)}K_{j,k} &=L^{(1)}K(x_k,\tilde{x}_j)&x_k\in B \tilde{x}_j\in I \\
    L^{(2)}L^{(1)}K_{j,k} &= L^{(2)}L^{(1)}K(\tilde{x}_k,\tilde{x}_j) & \tilde{x}_j,\tilde{x}_k\in U
\end{eqnarray*}

Same structure as for Hermite interpolation: Non-singular if 
$\delta_{x_i}K,LK$ are linearly independent holds for suitable $K$. %TODO: Check matrix or kernel

\begin{theorem}\label{thm:1.32}
    Let $\Omega\subseteq \R^d$ be a polygonal and open domain. Let $L$
    be a second order elliptic differential operator with coefficients 
    ch $C^{2(k-2)}(\bar{\Omega})$ that either vanishes on $\partial \Omega$ or have no zero here.

    Suppose that $k\in C^{2k}(\R^d\times\R^d)$ is a positive definite kernel. Assume 
    that $Lu=f$ in the system \ref{eq:008} has a unique solution $u\in \mathcal{N}_k(\Omega)$
    for some $f\in C(\Omega,g\in C(\Gamma))$. Let $\hat{u}$ be the approximation \ref{eq:010}. Then 
    \[\Vert u-\hat{u}\Vert_{L_\infty(\Omega)}\leq C h_{I,\Omega}^{k-2}\Vert u\Vert_{\mathcal{N}_k(\Omega)}\]
    and 
    \[\Vert u-\hat{u}\Vert_{L_\infty(\partial\Omega)}\leq C h_{B,\partial\Omega}^k \Vert u\Vert_{\mathcal{N}_k}\]
\end{theorem}

\begin{proof}[Sketch of the proof]

    For interior essentially as before for  $L(u)-L\hat{u}$ and 
    $LLk$ is p.d. for p.d. $k$.

\end{proof}

\beginlecture{09}{07.05.24}



\section{Kernel Methods for prediction}\beginlecture{10}{14.05.24}

\begin{aremark}
    Statistics: finding structure in the data, while ML tries to use the same math to make predictions
\end{aremark}

\begin{definition}\label{def:43}
    Let $\Omega,\Sigma$ be a measurable space and $Y\subseteq\R$ be a closed subset. 
    Denote by $(x,y,f(x))\in\Omega\times Y\times \R$ the triplet consisting of 
    \dhighlight{attributes (or features)} $x$, an \dhighlight{observation} $y$ and a \dhighlight{prediction} $y$.
    
    A function $l:\Omega\times Y\times \R\to [0\infty)$ is called a \dhighlight{loss function} if it is 
    measurable and $l(x,y,y)=0$ holds for all $x\in\Omega,y\in Y$.
\end{definition}

\begin{example}
    \begin{itemize}
        \item $l_2$ loss, which relates to the mean 
        \item $l_1$ loss, which relates to the median
        \item $l_H=\begin{cases}
            \frac{1}{2}\xi^2 & |\xi|\leq \sigma\\
            \sigma |\xi|-\frac{1}{2}\sigma^2 & \text{otherwise}
        \end{cases}$ hubert loss
        \item $l_\epsilon(\xi)=\max(|\xi|-\epsilon,0)\eqqcolon |\xi|_\epsilon$ $\epsilon$-sensitive loss
    \end{itemize}
    Weighting loss functions might be useful to emphasize important data points!
    For Classification:
    \begin{itemize}
        \item $l(x,y,f(x))=\begin{cases}
            0 & y=f(x)\\
            1 & \text{otherwise}
        \end{cases}$
        \item $l(x,y,f(x))=\begin{cases}
            0 & y=\text{sgn}(f(x))\\
            1 & \text{otherwise}
        \end{cases}$
        \item $l(x,y,f(x))=l_1(1+\exp(-y f(x)))$ logistic loss, relates to probability
        \item soft margin / hinge loss $\max(1-yf(x),0)$, important for SVMs \marginnote{much of the past research efforts where focused on SVMs}
    \end{itemize}
\end{example}

\begin{aremark}
    $l_1,l_2$ losses penalize overestimation in classification problems, therefore the other losses might be a better idea.
    
    Both hinge loss and logistic loss functions give a way to rank the data.
\end{aremark}

\begin{definition}\label{def:44}
    Let $l:\Omega\times Y\times \R\to[0,\infty)$ be a loss function and $\mathbb{P}$ be a probability measure 
    on $\Omega\times Y$. Then, for a measurable function $f:\Omega\to\R$, the \dhighlight{expected $l$-risk} is defined by 
    \[R_{l,p}(f)\coloneqq \int_{\Omega\times Y} l(x,y,f(x))d\bP(x,y)=\int_\Omega\int_Y l(x,y,f(x))d \bP(y|x)d\bP_\Omega(x).\]
\end{definition}

For a given data set $D\coloneqq\{x_j,y_j\}_{j=1}^N$ with $x_j\in \Omega,y_j\in Y$ we can define 
the empirical measure 
\[P_{\text{imp}}(x,y)=\frac{1}{N}\delta_{x_j,y_j}\]

\begin{definition}\label{def:45}
    The \dhighlight{empirical l-risk} of a function $f_\Omega\to\R$ is defined as 
    \[R_{l,\text{emp}}(f)=\int_{\Omega\times Y} l(x,y,f(x))d P_{\text{imp}}(x,y)=\frac{1}{N}\sum_{j=1}^N l(x_j,y_j,f(x_j))\]
\end{definition}

\begin{aremark}
    Regularization via penalty terms or by enforcing some sparsity condition on the $\alpha_j$.
\end{aremark}

We will assume that $R_{l,\text{emp}}(f)$ is continuous on $f$. \marginnote{Operator inversion lemma}

\[R_{l,\text{reg}}(f)=R_{l,\text{emp}}(f)+\lambda s(f)\]
Here, the smoothness or sparsity is enforced by the regularization term $s$.

Often $s$ is convex. The regularization parameter $\lambda$ balances 
the empirical error and the regularization.

\begin{theorem}[Representer theorem]\label{thm:46}
    Let $s:[0,\infty)\to\R$ be a strictly monotone increasing function, $\Omega$ 
    be a set, $\cH$ a RKHS over $\Omega$ and let $l:\Omega\times Y \times \R$ he a continuous loss 
    function. Then, for given data $D=\{(x_j,y_j)\}_{j=1}^N,x_j\in\Omega,y_j\in Y$ and 
    $\lambda>0$, each minimizer $f\in\cH$ of the regularized empirical risk 
    \[R_{l,\text{reg}}(f)=\frac{1}{N}\sum_{j=1}^N l(x_k,y_j,f(x_j))+\lambda s(\Vert f\Vert_\cH)\]
    admits a representation $f(x)=\sum_{j=1}^N \alpha_j k(x_j,x)$,
    that is $f\in \cH_X,X=\{x_1,\dots,x_N\}$.
\end{theorem}

\begin{proof}
    w.l.o.g. we assume $s(\Vert f\Vert_\cH)=\overline{s}(\Vert f \Vert_\cH)$.

    We decompose any $f\in\cH$ into $f_X\in\cH_X$ and $f_{X^\perp}\in \cH_{X^\perp}$ (Theorem \ref{thm:1.17})
    \[f=f_X+f_{X^\perp}=\sum_{j=1}^N \alpha_j k(x_j,x)+f(x)_{X^\perp}\]
    We know 
    \begin{align*}
        \langle f_{X^\perp},k(x_k,\cdot)\rangle_\cH=0
    \end{align*}
    with the reproduction equation we write 
    \[f(x_i)=\langle f,k(x_i,\cdot)\rangle_\cH = \sum_{j=1}^N \alpha_j k(x_i,x_j)+\langle f_{X^\perp},k(x_i)\rangle_\cH=\sum_{j=1}^N \alpha_j k(x_i,x_j)\]

    The loss term part does not depend on $f_{X^\perp}$. Further, for all $f_{X^\perp}$ it holds 
    \[s(\Vert f\Vert_\cH)=\overline{s}(\Vert f\Vert_\cH^2+\Vert f_{X^\perp}\Vert^2)\geq \overline{s}(\Vert f_X\Vert_cH^2)\]
    Therefore for any fixed $\alpha\in\R^N$ the objective is minimal if $f_{X^\perp}=0$. This has to hold 
    for any minimizer $f$.

\end{proof}

\begin{remark}
    $f+q$, $f\in\cH$, $q\in \text{span}\{\varphi_p\}$ For this setup a corresponding 
    representer theorem does hold. 
\end{remark}


\begin{remark}
    If both loss function and $s$ are convex, one has a single minimum.
\end{remark}

Consider regularized least squares regression,
\[R_{l_2,\text{reg}}(f)=\frac{1}{N}\sum_{j=1}^N (f(x_j)-y_j)^2+\frac{\lambda}{2}\Vert f\Vert_\cH^2\]
where $f(x)=\sum_{j=1}^N \alpha_j k(x_j,x)$.

\begin{align*}
    \frac{1}{N}\sum_{j=1}^N\left(\sum_{k=1}^N \alpha_k k(x_k,x_j)-y_j\right)^2 +\frac{\lambda}{2}\sum_{j,k=1}^N \alpha_j\alpha_k k(x_j,x_k)
\end{align*}
derivation w.r.t. $\alpha_k$ yields:
\begin{align*}
    \frac{2}{N}\sum_{j=1}^N \left(\sum_{k=1}^N \alpha_k k(x_k,x_j)-y_j\right)k(x_k,x_j) +\frac{\lambda}{2}\sum_{j=1}^N \alpha_j(x_j,x_k)
\end{align*}
All together for all $a_k$ this holds and gives 
\begin{align*}
    0&=\frac{2}{N}K(K\alpha-Y)+\frac{\lambda}{2}K \alpha\\
    \implies & K(K+\lambda N I)\alpha = KY \\
    \implies & (K+\lambda N I)\alpha = Y
\end{align*}

\beginlecture{11}{16.05.24}

In $L^2(\Omega)$ we have $\langle f,g\rangle=\int_\Omega fgdx$.
We aim to write $\langle f,g\rangle_\cH=\langle Sf,Sg\rangle_{L^2(\Omega)}=\int_{\Omega}Sf(x)\cdot Sg(x)dx$,
where $S$ is called a \dhighlight{regularization operator}.

\begin{definition}\label{def:47}
    A \dhighlight{regularization operator} $S$ is defined as a linear map 
    from the space of functions \[\{f\mid f:\Omega\to \R\}\] into 
    a space $D$ equipped with a scalar product. The regularization 
    term $s(f)$ takes the form\marginnote{sometimes we multiply $\frac{1}{2}$ to $s$} \[s(f)\coloneqq \langle S(f),S(f)\rangle_D.\]
\end{definition}

\begin{remark}
    Since we can always define $\tilde{S}=(S^*S)^{\frac{1}{2}}$ and 
    \[\langle f, S^*S f\rangle_D=\langle Sf,Sf\rangle_D\]
    we can assume $S$ is a positive semidefinite (regularization) operator.
\end{remark}

\begin{definition}\label{def:48}
    Let $k:\Omega\times\Omega\to\R$ be continuous, $\Omega$ be a compact domain,
    $\nu$ be a Borel measure and $L_2^{\nu}(\Omega)$ be the Hilbert space of square integrable functions on $\Omega$.
    
    We define the \dhighlight{integral operator} $T_k:L_2^\nu(\Omega)\to L_2^\nu(\Omega)$ by 
    \[T_k(f)(\cdot)=\int_\Omega k(x,\cdot)f(x)d\nu\]
    and we call $k$ the kernel of $T_k$.
\end{definition}

Mercer kernels 
\[K(x,y)=\sum_{i=1}^\infty \lambda_j\phi_j(x)\phi_j(y)\]\marginnote{Fredhom integration operator of the second kind}
with eigenvalues $\lambda_j$ and eigenfunctions $\phi_j$ w.r.t. the eigenproblem
\[\langle k(x,\cdot),\phi\rangle_\sigma=\int_\Omega k(x,y)\phi(x)\sigma(x)dx=\lambda\phi(y)\iff (T_k\phi)(y)=\lambda\phi(y)\]
\begin{definition}\label{def:49}
    Given a linear ... or partial differential operator $\mathcal{L}$ on $\Omega\subseteq\R^d$, the %TODO
    \dhighlight{Green's kernel} $g$ of $\mathcal{L}$ is defined as the solution of 
    \[\mathcal{L}(g)(x,z)=\delta(x-z)\]
    $\int f(z)\delta(x-z)dx=f(x)$.
\end{definition}

The Green kernel is not uniquely defined this way, so one adds 
homogenous boundary conditions e.g. $g(x,z)=0$ for $x\in\partial\Omega$,
$\lim_{\Vert x\Vert \to\infty} g(x,z)=0$.

Solutions of $\mathcal{L}u=f$ with appropriate boundary conditions can 
now be given as 
\[u(x)=\int_\Omega g(x,z)f(z)dz\]
Check 
\begin{align*}
    \mathcal{L}u(x)&=\mathcal{L}\int_\Omega g(x,z)f(z)dz\\
    &=\int_\Omega \mathcal{L}g(x,z)f(z)dz\\
    &=\int_\Omega \delta(x-z)f(z)dz=f(x)
\end{align*}

We can ... %TODO

\[Gf(x)=\int_\Omega g(x,z)f(z)dz\]
as the ``inverse'' of the differential operator $\mathcal{L}$, i.e.
\[\mathcal{L}u=f\iff u=Gf\]

\begin{example}[Brownian Bridge kernel]
    $\Omega=[0,1]$, consider bvp (boundary value problem)
    \[-u''(x)=f(x),u(0)=0=u(1)\]
    The corresponding Green's kernel is 
    \[g(x,z)=\min(x,z)-xz=\begin{cases}
        x(1-z) & x\leq z\\
        z(1-x) & x\geq z
    \end{cases}\]

    We an observe that for $g$ it must hold
    \[\mathcal{L}g(x,z)=0\] for $x\neq z$, $z$ fixed. 
    \begin{itemize}
        \item $g(0,z)=g(1,z)=0$
        \item $g$ is continuous along the diagonal $x=z$
        \item for fixed $z\in (0,1)$ one observes for $\frac{dg}{dx}$ a jump disc. at $x=z$ of the form \marginnote{There is a further connection to the Brownian bridge of stochastic analysis? Not just the end points}
        \[\lim_{x\to z^-}\frac{dg}{dx}(x,z)=1+\lim_{x\to z^+}\frac{dg}{dx}(x,z)\]
    \end{itemize} 
\end{example}

\begin{remark}
    Whenever $\mathcal{L}$ is a self adjoint differential operator, 
    the corresponding Green's kernel is symmetric and the integral operator $G$ is 
    self adjoint. 
\end{remark}

\begin{theorem}\label{thm:50}\marginnote{The second statement is much more interesting, the first follows from $S=\text{Id}$}
    For every RKHS $\cH$ with reproducing kernel $k$ there exists a corresponding regularization operator 
    $S:\cH\to D$ s.t. for all $f\in\cH$
    \begin{equation}\label{eq:010}
        f(x)=\langle Sk(x,\cdot),Sf(\cdot)\rangle_D
    \end{equation}
    In particular \[\langle S k(x,\cdot),Sk(y,\cdot)\rangle_D=k(x,y)\]
    likewise, for every regularization operator $S:\mathcal{F}\to D$, where $\mathcal{F}$ is some 
    function space equipped with a scalar product and with corresponding Green's kernel 
    $f$ on $S^*S$, %TODO
    there exists a corresponding RKHS $\cH$ with reproducing kernel $K$, s.t.
    both equations are fulfilled.
\end{theorem}

\begin{proof}
    First direction: $S=\text{Id},D=\cH$.

    Second direction: 

    \begin{align*}
        f(x)&=\langle f,\delta(x-z)\rangle_{\mathcal{F}}=\langle f,\mathcal{L} g_x\rangle_{\mathcal{F}}=\langle f, S^*S g_{x}\rangle_{\mathcal{F}}=\langle Sf,Sg_x\rangle_{D}
    \end{align*}
    for all $f\in S^*S\mathcal{F}$, where $g_x$ is the Green's kernel for 
    $S^*S$ and natural boundary conditions. Further we have with $f=g_z$
    \begin{align*}
        g_z(x)=\langle S g_x, Sg_z\rangle=\langle S g_z, S g_x\rangle=g_x(z)
    \end{align*}

    In this sense $g$ is symmetric and we write 
    \[k(x,z)=g_z(x).\]

    We observe that $x\to S g_x$ is actually a feature map,
    i.e. $\langle S g_x,S g_z\rangle_D$. Since kernels
    arising from feature maps result in Gram matrices for the kernel matrix 
    we set that $K$ is a p.s.d..

    It can be seen that the corresponding RKHS is the closure of 
    \[\{f\in S^* S\mathcal{F}\mid \Vert Sf\Vert_D^2 <\infty\}\]

\end{proof}

To simplify, we consider the full space kernel, i.e. 
without boundary  / decay conditions. For $g_z(x)=k(x,z)$, $g$ for the differential operator
$\mathcal{L}$, we observe 
\begin{align*}
    \mathcal{L}\int_\Omega k(x,z)\sigma(x) dx&=\mathcal{L}\lambda\lambda \phi(z)\\
    \iff \underbrace{\int_\Omega \delta(x-z)\phi(x)\sigma(x)dx}_{\phi(z)\sigma(z)}&=\lambda\mathcal{L}\phi(z)\\
    \implies \dots & \implies L\phi(z)=\frac{1}{\lambda}\phi(z)\sigma(z)
\end{align*}

For simplicity we assume that $\mathcal{L}$ has no Eigenvalue $0$.

\begin{example}
    We have $\int_\Omega k(x,z)\phi(x)\sigma(x)dx=\lambda\phi(z)$
    with $\sigma=1,k(x,z)=\min(x,z)-xz$ on $\Omega=[0,1]$.

    This gives
    \[\int_0^z x\phi(x)dx+\int_z^1 z\phi(x)dx-\int_0^1 xz\phi(x)dx=\lambda\phi(z)\]

    Now apply $\mathcal{L}=-\frac{d^2}{dz^2}$ to the equation:
    \begin{align*}
        \frac{d}{dz}\left(z\varphi(z)-\int_1^z \phi(x)dx-z\phi(z)-\int_0^1x\phi(x)dx\right)&=\lambda \phi''(z)\\
        \iff \frac{1}{\lambda}\phi(z)&=\phi''(z)
    \end{align*}

\end{example}

\begin{theorem}\label{thm:51}
    Given a regularization operator $S$ with an expansion of $S^*S$ into a 
    discrete normalized eigendecomposition with eigenvalues and eigenfunctions $ \gamma_i,\phi_i$, we define
    a kernel with 
    \[k(x,y)=\sum_{i,\gamma_i\neq 0}\frac{d_i}{\gamma_i}\phi_i(x)\phi_i(z)\] 
    where $d\in\{0,1\}$ for all $i$ and $\sum_{i=1}^\infty\frac{d_i}{\gamma_i}$ is ... %TODO 
    Then $k$ satisfies theorem \ref{thm:50}
    \[\langle Sk(x,\cdot),Sk(z,\cdot)\rangle_D=k(x,z)=\langle k(x,\cdot),k(z,\cdot)\rangle_\cH\]
    where the RKHS is given by $\text{span}\{\phi_i|d_i=1\}$
\end{theorem}


\begin{proof}
    \begin{align*}
        \langle Sk(x,\cdot),Sk(z,\cdot)\rangle &=\langle k(x,\cdot),S^*Sk(z,\cdot)\rangle\\
        &=\langle\sum_i \frac{d_i}{\gamma_i}\phi_i(x)\phi_j(\cdot),S^*S\sum_i \frac{d_i}{\gamma_i}\phi_i(x)\phi_j(\cdot)\rangle\\
        &=\sum_{j,k}\frac{d_j}{\gamma_j}\frac{d_k}{\gamma_k}\phi_j(x)\phi_k(z)\langle \phi_j(\cdot),\underbrace{S^*S\phi_k(\cdot)}_{\gamma_k\phi_k}\rangle\\
        &\stackrel{\text{ONB}}{=}\sum_{j}\frac{d_j}{\gamma_j^2}\gamma_j\phi_j(x)\phi_j(z)=k(x,z)
    \end{align*}

    From the construction of $k$ follows the statement of the span.
\end{proof}

\beginlecture{12}{28.05.24}

\section{Model selection}

To estimate the predictive performance of a learned model, we use train- and test/ validation data.
Where $D_t,D_V$\marginnote{80/20, 70/30 are common splits, this does depend on the size of the dataset!} are the training and the validation data sets, respectively. 
The \dhighlight{empirical risk} on $D_V$ is used to measure the generalization performance.

In case of hyperparameters, a simple strategy is to use a selection of hyperparameters, train models, and 
pick the hyperparameters with the best empirical risk.\marginnote{Might not be the best idea in practice, where training may take weaks! Notice how the run time scales exponetially in the number of hyperparameters}

\begin{aremark}
    For time series data a random split might be bad (if there is a drift), therefore we might prefer taking years up to year x!
\end{aremark}

\begin{aremark}
    If we use hyperparameter search, we also have to split the data into 3 parts. One for training, 1 for hyperparameter search and one to estimate the predictive power.
\end{aremark}

If the number of data is small, the measured performance on the validation data can have a large uncertainty. A 
strategy to use is $k$-fold cross validation.

We use $k$ disjoint equally sized subsets, use each one once for $D_V$ and the $k-1$ other ones as 
training data $D_t$. The average performance over the $k$ runs is then the performance measure.

\begin{aremark}
    In the deep learning setting run the same training with different seeds, then take the average.
\end{aremark}

For $k=N$ is known as \dhighlight{leave-one-out} cross validation.

%Definition in the script

\begin{definition}\label{def:52}
    There is a exact definition in the script (this was not done in the lecture).
\end{definition}

\begin{definition}\label{def:53}
    A \dhighlight{Gaussian process} is a collection of random variables,
    any finite number of which have a joint Gaussian distribution.

    Mean $m(x)=\bE(f(X))$ and covariance $k(x,z)=\bE((f(x)-m(x))(f(z)-m(z)))$
    \[f(x)=\text{GP}(m(x),k(x,z))\]
\end{definition}

Key example: Gaussian kernel:
\[\cov(f(x),f(z))=k(x,z)=\exp(-\frac{1}{2\sigma^2}\Vert x-z\Vert^2)\]

\begin{aremark}
    We can then sample from this space of functions! The better fitted the space is to our data, the better our predictions.
\end{aremark}

Choose $X_*\coloneqq\{x_1,\dots,x_N\}$ and generate a random process with a covariance matrix
\[f_*=\cN(0,k(X_*,X_*))\]
Consider noisy data with additive gaussian noise $\epsilon$, with variance $\sigma_N^2$;
\[y=f(x)+\epsilon\]

The prior on the noisy observations becomes \[\cov(y_p,y_q)=k(x_p,x_q)+\sigma_N^2\delta_{pq}.\]
or \[\cov(Y)=k(X,X)+\sigma_N^2I\]

Joint distribution of observed target and $f$ at test locations
\begin{align*}
    \begin{bmatrix}
        y\\f_*
    \end{bmatrix}\sim\cN\left(0,\begin{bmatrix}
        k(x,x)+\sigma_N^2I & k(x,x_*)\\
        k(x_*,x) & k(x_*,x_*)
    \end{bmatrix}\right)
\end{align*}

We need to restrict this joint distribution to contain only those functions that ``agree''
with the observed data.

\[f_*\mid X,Y,x_*\sim \cN\left(k(x_*,X)[k(X,X)+\sigma_N^2I]^{-1}Y,k(x_*,x_*)-k(x_*,X)[k(X,X)+\sigma_N^2I]^{-1}k(X,x_*)\right)\]

Boil down to one evaluation $x_l$ we have
\[f(x_l)=k(X,x_l)^\intercal(k(X,X)+\sigma_n^2I)^{-1}Y=\sum_{i=1}^N \alpha_i k(x_i,x_l)\]
\[V(f)=k(x_l,x_l)-k(X,x_l)^\intercal[k(X,X)+\sigma_N^2I]^{-1}k(X,x_l)\]
\begin{aremark}
    What is a length scale? Variance in data points?
\end{aremark}

Consider the marginal likelihood or evidence 
\[p(Y\mid X)=\int \underbrace{p(Y\mid f,X)}_{\text{likelihood}}\underbrace{p(f\mid X)}_{prior}df\]

One can obtain for the log marginal likelihood 
\[\log(p(Y\mid X))=-\frac{1}{2}Y^\intercal (K+\sigma_N^2)^{-1}Y\frac{1}{2}\log(K+\sigma_N^2I)-\frac{N}{2}\log(2\pi)\]

$\theta$ hyperparameters of K:
\[\log(p(Y\mid X,\theta))=-\frac{1}{2}\underbrace{Y^\intercal (K_\theta+\sigma_N^2)^{-1}Y}_{\text{data fit}}\underbrace{\frac{1}{2}\log(K_\theta+\sigma_N^2I)}_{\text{complexity penalty}}-\underbrace{\frac{N}{2}\log(2\pi)}_{\text{Normalization parameter}}\]
One aims to optimize the evidence of the data, in this Bayesian optimization one can also include the hyperparameters.

\begin{aremark}
    Bayesian optimization. Optimize which data point would minimize unexplained variance?
\end{aremark}


$k(x,y)=\sum_{i=1}^\infty\gamma_i\phi_i(x)\phi_i(y)$ a mercel kernel. For $f\in \cH_K$
\[f(x)=\sum_{i=1}^\infty c_i \phi_i(x)\]
\begin{align*}
    \langle f,f\rangle_{\cH_K}=\langle Sf, Sf\rangle_{L^2}=\langle f,Sf\rangle_{L^2}=\sum_{i=1}^\infty \frac{c_i^2}{\gamma_i}<\infty
\end{align*}

The rough terms $\phi_i$ have smaller prior variance and are therefore more strongly penalized
by the regularization.

\beginlecture{13}{04.06.24}

\begin{aremark}
    \begin{itemize}
        \item Motivation: Finding a classifier, specifically by separating data by a hyperplane!
        \item Idea for SVMs: find a separating hyperplane while maximizing the margin between the hyperplane and the data points.
        \item Outliers or not linearly separable data is a problem!
        \item For SVMs slack variables can be a good idea to counteract some of those problems (where $C$ is another regularization parameter)
        \item We can also use feature maps to embed our data into a higher dimensional space, where it might be linearly separable
        \item Not a unique way to choose such a feature map
        \item Sadly the number of support vectors in linear in the number of data points
    \end{itemize}
\end{aremark}

\begin{align*}
    \xi_i\geq 1-y_i(\langle w_i,\Phi(x_i)\rangle+b),\text{ } & \xi_i\geq 0\\
    \xi_i\geq \max(0,y_i(\langle w_i,\Phi(x_i)\rangle+b))&\\
    &=l_h(y_i,\langle w_i,\Phi(x_i)\rangle+b)
\end{align*}

It is therefore related to the hinge loss!

$(w,b)$:
\begin{align*}
    f_{(w,b)}(x)&=\langle w,\Phi(x)\rangle+b\\
    \min_{(w,b)}\underbrace{\langle w,w\rangle}_{\Vert f\Vert_{\cH}} &+ C\sum_{i=1}^N l_h(y_i,\langle w_i,\Phi(x_i)\rangle+b)
\end{align*}


This gives the RKHS view on SVMs.

This, once more, motivates 

\[k(x,y)=\langle \Phi(x),\Phi(y)\rangle\]

\begin{align*}
    \Vert \Phi(x)-\Phi(y)\Vert^2&=\langle\Phi(x),\Phi(x)\rangle-2\langle\Phi(x),\Phi(y)\rangle+\langle\Phi(y),\Phi(y)\rangle\\
    &=k(x,x)-2k(x,y)+k(y,y)
\end{align*}

Shifting data: $x\mapsto x-t$, $\Vert x-y\Vert^2$ is invariant under the shift, but 
$\langle x,y\rangle$ is not!\marginnote{There was a mistake here in the lecture!}
\begin{align*}
    \langle x-t,y-t\rangle & = \langle x,y\rangle-\langle x,t\rangle-\langle t,y\rangle+\langle t,t\rangle\\
    &=\frac{1}{2}\left(\langle x,x \rangle -2\langle x,t\rangle +\langle t,t\rangle+\langle y,y\rangle-2\langle y,t\rangle+\langle t,t\rangle+2\langle x,y\rangle-\langle x,x\rangle - \langle y,y\rangle\right)\\
    &=\frac{1}{2}\left(\Vert x-t\Vert^2+\Vert y-t\Vert^2-\Vert x-y\Vert^2\right)\\
    &\sim \tilde{k}(x,y)
\end{align*}

This $\tilde{k}(x,y)=\langle x-t,y-t\rangle$ is still psd.


\[\sum_{j,k} a_ja_k\langle x_j-t,x_k-t\rangle = \Vert \sum_{j}c_j(x_k-t)\Vert^2\]

So, for any choice of $t\in\R$ we get a similarity measure $\tilde{K}(x,y)$ associated with the dissimilarity measure $\Vert x-y\Vert$


Now replace $\langle \cdot,\cdot\rangle$ by $k$.

\begin{lemma}\label{lem:54}
    Let $t\in\Omega$ and $K$ be a symmetric kernel on $\Omega\times\Omega$. Then $\tilde{k}(x,y)=\frac{1}{2}(k(x,y)-k(x,t)-k(t,y)+k(t,t))$
    is positive semidefinite if and only if $K$ is cond. psd. of order 1.

    If $k(t,t)\leq 0$, then 
    \[\hat{K}(x,y)\coloneqq \frac{1}{2}(k(x,y)-k(x,t)-k(t,y))\]
    is psd iff $K$ is cpsd. of order 1.
\end{lemma}

\begin{proof}
    1. $\tilde{k}$ psd $\implies K$ cpsd:

    Let $a\in\R^n$ s.t. $\sum a_i=0$ and let $(x_i)_{i=1}^N$, then 
    \begin{align*}
        0\leq \sum_{i,j=1}^N a_ia_j \tilde{k}(x_i,x_j)=\sum_{i,j} \sum_{i,j=1}^N a_ia_j k(x_i,x_j)
    \end{align*}
    Since \[\sum_j a_j\underbrace{\sum_i a_i k(x_i,t)}_{=c}=0\]

    2. $K$ cpsd. $\implies \tilde{k}$ psd.:

    Let $a\in\R^n$, $(x_i)_{i=1}^N$, let $x_0=t,a_0=-\sum_{i=1}^Na_i$.
    Then \[0\leq \sum_{i,j=1}^N a_ia_j k(x_i,x_j)=\sum_{i,j=1}^N a_ia_j \tilde{k}(x_i,x_j)\]
    Thus $\tilde{k}$ is psd.
\end{proof}

If we take negative square distance, here $\sum_{i}a_i=0$ implies 
\[-\sum_{i,j} a_ja_k \Vert x_j-x_i\Vert^2=-\sum_{j}a_j\sum_{k}\Vert x_k\Vert^2-\sum_{k}a_k\sum_{j}\Vert x_j\Vert^2+-\sum_{j}a_j\sum_{k}\langle x_j,x_k\rangle\]
so $-\Vert x-y\Vert$ is cpsd. 

Therefore kernels that are cspd. of order 1 are sometimes called negative definite kernels for $-K$.

From Mercers theorem we have that $\tilde{k}$ (psd) has a feature map $\phi:\Omega\to\cF$, $\tilde{k}(x,y)=\langle \Phi(x),\Phi(y)\rangle$.
Therefore $\Vert \Phi(x)-\Phi(y)\Vert^2 = \tilde{k}(x,x)+\tilde(k)(y,y)-2\tilde(k)(x,y)$.
For fixed $t\in\R$\marginnote{$t\in\cF$ prob. ???} we use the definition of $\tilde{k}$.

\begin{align*}
    \Vert \Phi(x)-\Phi(y)\Vert^2&=\frac{1}{2}(k(x,x)-k(x,t)-k(t,x)+k(t,t)+k(y,y)-k(y,t)-k(t,y)+k(t,t)-2(k(x,y)-k(x,t)-k(t,y)+k(t,t)))\\
    &=-k(x,y)+\frac{1}{2}(k(x,x)+k(y,y))
\end{align*}

This shows

\begin{theorem}\label{thm:55}
    Let $k$ be a cspd kernel of order 1 on $\Omega$. Then there exists a Hilbert space 
    $\cF$ and a mapping $\Phi:\Omega\to\cF$ s.t. 
    \[-k(x,y)+\frac{1}{2}(k(x,x)+k(y,y))=\Vert \Phi(x)-\Phi(y)\Vert_{\cF}^2\]
    If $k(x,x)=0$ for all $x\in\Omega$, we have \[k(x,y)=-\Vert \Phi(x)-\Phi(y)\Vert_{\cF}^2\]
    and $\sqrt{-k(x,y)}$ is a semi-metric and a metric if $k(x,y)\neq 0$ for $x\neq y$. 
\end{theorem}

\beginlecture{14}{06.06.24}

Consider the 

\[F[f](\omega)=\frac{1}{(2\pi)^{-d/2}}\int_{\R^d}f(x)\exp(-i\langle x,\omega\rangle)dx\]

and the \dhighlight{inverse Fourier transform} 
\[F^{-1}[f](x)=\frac{1}{(2\pi)^{-d/2}}\int_{\R^d}f(\omega)\exp(i\langle i,\omega\rangle)\]
where
\[f(x)=F^{-1}[F[f]](x)\]
Consider $S$, where $S^*$ is diagonalizable on th Fourier basis.\marginnote{Consider the regularization operator ...}

Denote $v(\omega)$ a nonnegative, symmetric function on $\R^d$:
\[v(\omega)=v(-\omega)\geq 0\]
further assume $v(\omega)\to0$ for $\Vert\omega\Vert\to\infty$ and denote by 
$\Omega$ the support of $v$.
\begin{align*}
    \langle Sf,Sg\rangle_D=(2\pi)^{\frac{d}{2}}\int_\Omega \frac{\overline{F[f](\omega)}F[g](\omega)}{v(\omega)}d\omega
\end{align*}

Small values of $v(\omega)$ correspond to strong damping of the corresponding frequencies.
This is desirable for large $\omega$, i.e. high frequency components that correspond to 
rapid changes in $f$.

Now look at $g(x,z)=(2\pi)^{d/2}\int_\Omega e^{i\omega(x-z)}v(\omega)d\omega$. Let $f$
have the support of its Fourier transform contained in $\Omega$. We see 
\begin{align*}
    \langle Sg(x,\cdot),Sf\rangle &= (2\pi)^{d/2}\int_\Omega \frac{\overline{F[g(x,\cdot)](\omega)}F[f](\omega)}{v(\omega)}d\omega\\
    &=(2\pi)^{-d/2}\int_\Omega \frac{\overline{v(\omega)}\exp(i\langle x,\omega\rangle)F[f](\omega)}{v(\omega)}d\omega\\
    &=(2\pi)^{-d/2}\int_\Omega \exp(i\langle x,\omega\rangle)F[f](\omega)d\omega=f(x)
\end{align*}

According to theorem \ref{thm:50} $g$ is the kernel corresponding to $S$.

This is a special case of 
\begin{theorem*}[Bocher's theorem]
    For given $\kappa:\R^d\to\C$ the expression
    \[k(x,y)=\kappa(x-y)\] defines a kernel if and only if there exists 
    a unique finite Borel measure $\mu$ on $\R^d$ s.t.
    \[\kappa(x)=\int_{\R^d}\underbrace{\exp(i\langle x,y\rangle)}_{\cos(\langle x,y\rangle)\text{ for the real case}}d\mu(y)\] 
\end{theorem*}

\begin{aremark}
    random kitchen sink paper ... 20007
\end{aremark}