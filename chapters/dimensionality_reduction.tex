\begin{aremark}
    Idea: High dimensional data $Y\to$ find low dimensional data $X$ which represents $Y$ to a good degree.

    For example: Represent vectors w.r.t. to a subset of the basis!
\end{aremark}

Set $Y$ of $y_i\in\R^d$, $i=1,\dots,N$\marginnote{We are now in the unsupervised setting}
goal is to find 
\[x_i\in\R^p, p\ll d\]
$d$ is called the \dhighlight{extrinsic} dimension, while $p$ is the \dhighlight{intrinsic} dimension.

\section{Linear dimensionality reduction}

$\{y_i\}_{i=1}^N$ are samples of a random variable $Y(\omega)\in\R^d$\marginnote{He writes $Y\in\R^d$, which seems to be the wrong space for a random variable \dots}.
We assume $Y$ stems from $p$ unknown latent variables $X(\omega)\in\R^p$ by a linear transformation $W$.
\[Y=WX\]
which can be read as a statement about vectors (a single realization of the random variable) or in terms of matrices (the sampled realizations as a whole)-

We assume that $Y,X$ are mean centered, i.e. $\bE(Y)=0$. We further assume 
$W$ to be an axis change, i.e. the columns $w_i$ of $W$ are orthogonal to each other and unit norm:
\[W^\intercal W = I_p\]
We write the data in matrix form\marginnote{with slightly abusive notation, as the matrix $Y$ consists of $N$ samples of the random variable $Y$} 
\[Y=\begin{bmatrix}
    \vert &&\vert\\
    y_1 & \dots & y_N\\
    \vert &&\vert
\end{bmatrix}\]

For any $W$, consider the pseudo inverse 
\[W^\dagger = (W^\intercal W)^{-1}W^\intercal=W^\intercal\]
and therefore 
\[X_i=W^T y_i.\]
We aim for a good reconstruction of $Y$ by $X$:
\begin{align*}
    &\bE(\Vert y-W(W^T y)\Vert_2^2)\\
    &=\bE(y^\intercal y-2y^\intercal W W^\intercal y+y^\intercal W W^\intercal W W^\intercal y)\\
    &=\bE(y^\intercal y-y^\intercal WW^\intercal y)
\end{align*}
where 
\begin{align*}
    \bE(Y^\intercal WW^\intercal \underbrace{Y}_{\in\Omega\times\R^d})&\approx\frac{1}{N}\sum_{i=1}^N y_i^\intercal W W^\intercal y_i\\
    &=\frac{1}{N}\text{tr}(Y^\intercal W W^\intercal \underbrace{Y}_{\in \R^{d\times N}})\\
    &=\frac{1}{N}\text{tr}(W^\intercal YY^\intercal W)
\end{align*}

Adding the constraint, we set the lagrangian 
\[\mathcal{L}=\text{tr}(W^\intercal YY^\intercal W)+\tr((I_p-W^\intercal W)\Lambda)\]
where $\Lambda=\Lambda^T\in\R^{p\times p}$.

Conditions for an extrema:
\begin{align*}
    (\star):YY^\intercal W = W\Lambda\implies \Lambda=W^\intercal YY^\intercal W
\end{align*}
The objective reduces to $\tr(\Lambda)$. We can rotate $W$ and have the same \dhighlight{reconstruction error},
i.e. we can use $W'=WR$ giving $\Lambda'=R\Lambda R^T$ for some rotation matrix $R$.
$\implies \Lambda=\Lambda^T$ is diagonalizable with orthogonal matrices, so we choose $R$ s.t. 
$\Lambda'$ is diagonal, w.l.o.g. $\Lambda$ is diagonal.

From $(\star)$ follows that the columns of $W$ must be eigenvectors of $YY^\intercal$ with 
corresponding eigenvalues as the diagonal of $\Lambda$. Since we maximize $\tr(\Lambda)$, we 
take the $p$ largest eigenvalues of $YY^\intercal$ and the corresponding eigenvectors.

SVD of $Y$: $U\Sigma V^\intercal$, we take the first $p$ columns of $U$ for $W$, i.e. $W=UI_{d\times p}$.
Furthermore, as $U$ is orthogonal,
\[X=W^\intercal Y=W^\intercal U \Sigma V^\intercal=I_{p\times d}\Sigma V^\intercal.\]
Using $\Lambda=W^\intercal U\Sigma^2 U W$, we get 
\[\tr(\Sigma^2)-\tr(I_{p\times d}\Sigma^2)=\sum_{i=p+1}^d \sigma_i^2=\sum_{i=p+1}^d \lambda_i\]

\begin{theorem}\label{thm:2.1}
    Let $Y=[y_1,\dots,y_N]\in\R^{d\times N}$ be a matrix of zero mean data points. Denote the SVD of 
    $Y$ by $Y=U\Sigma V^\intercal$. Then for given $p<d$, the minimizer $W$ for 
    \[\min_{\stackrel{W}{W^\intercal W=I_p}}\sum_{i=1}^N\Vert y_i-WW^\intercal y_i\Vert_2^2\]
    is given using $W=[u_1,\dots,u_p]$. The lower dimensional embedding is given by 
    $X=I_{p\times d}\Sigma V^\intercal=I_{p\times d}U^\intercal Y$ and the \dhighlight{reconstruction error} 
    is \[\sum_{i=p+1}^N\sigma_i^2\]
\end{theorem}

\beginlecture{15}{11.06.24}

To get compactness: Take a\dhighlight{ball of functions: $\Vert f\Vert<=r$} and use ArzelÃ -Ascoli. % TOFIX


\subsection{Alternative derivations of PCA}

\begin{itemize}
    \item Projection: We aim for $y=\sum_{i=1}^p x_i w_i$ with $y_i,w_i\in\R^d, w_i^\intercal,w_j=\delta_{ij}$. Then $x_i=\langle y_i,w_i\rangle$.
    \item Approximation with rank constraints:\[\min_{A}\Vert Y-A\Vert_{F}^2\] s.t. rank $A=p$
    \item From the statistical perspective: 
\end{itemize}

\begin{definition}\label{def:2.2}
    Given a zero mean multivariate random variable $Y\in\Omega\times \R^d$. The $p$ \dhighlight{principle components} of $Y$
    are defined as the $p$ uncorrelated linear components of $Y$:\[x_i=w_i^\intercal y\in\R,\text{ } w_i\in\R^d,\text{ }i=1,\dots,p\] s.t. the variance of $x_i$ maximized subject to $w_i^\intercal w_i=1$  and $\Var(x_1)\geq \Var(x_2)\geq\dots\geq \Var(x_p)$
\end{definition}

\begin{theorem}\label{thm:2.3}\marginnote{In the exercise we show that the greedy approach works in this specific setting?}
    Assume that the rank of the covariance matrix $\bE(YY^\intercal)$ is larger than $p$. Then the first 
    $p$ principle components of a zero mean, multivariate random variable $Y$, denoted by $x_i,i=1,\dots,p$ are given by 
    \[x_i=w_i^\intercal Y\]
    where $\{w_i\}_{i=1}^p$ are the $p$ orthonormal eigenvectors of $\bE(YY^\intercal)$ associated 
    its $p$ largest eigenvalues $\{\lambda_i\}_{i=1}^p$. Moreover $\lambda_i=\Var(X_i)$.
\end{theorem}

\begin{aremark}
    $Y$, which has $N$ columns and $d$ rows ... % BILD 
\end{aremark}

\begin{aremark}
    In general we might want to work with tensors (for example if we have a time dependent structure). This is possible, but can be generalized 
    in multiple, non-equivalent, ways. The sum of vectors representation is not so nice, there are counter examples ... 
\end{aremark}

We aim for embeddings that approximately preserve distances. \marginnote{Here, we always talk about euclidean distances!}
\[d^d(y_1,y_2)\approx d^p(x_1,x_2)\]

\begin{definition}\label{def:2.4}
    A $N\times N$ symmetric matrix $D$ is called \dhighlight{Euclidean distance matrix (EDM)}, if there exists 
    an integer $d>0$ and a vector set $Y=\{y_1,\dots,y_N\},y_i\in\R^d$, s.t. $D_{i,j}=d_E^2(y_i,y_j)$,\marginnote{Careful! Some authors use the euclidean distance matrix with a square, and some without the square!}
    where $d_E$ is the euclidean distance. The vector set $Y$ is called the \dhighlight{configuration} of $D$. We write $D\in\text{EDM}$.
\end{definition}

\begin{definition}\label{def:2.5}
    A $N\times N$ symmetric matrix $D$ with non-negative entries $d_{i,j}$ is called \dhighlight{distance matrix}, if $d_{ii}=0$ for all $i$ and 
    \[\sqrt{d_{ij}}\leq \sqrt{d_{ik}}+\sqrt{d_{kj}}\]
    for all $i,j,k$. We write $D\in \DM$. 
\end{definition}

Obviously $\EDM\subset\DM$.

\[(\star)\text{ } d_E^2(y_i,y_j)=\underbrace{\langle y_i,y_i\rangle}_{G_{ii}}-2\underbrace{\langle y_i,y_j\rangle}_{G_{ij}}+\underbrace{\langle y_j,y_j\rangle}_{G_{jj}}\]
where $G_{ij}=\langle y_i,y_j\rangle = (Y^\intercal Y)_{ij}$.

As for PCA, we aim for mean centered data, where we use the centering matrix
\[H=I-\frac{1}{N}1_N,\]
where $1_W=1_N\cdot 1_N^\intercal$ is the matrix of all ones, where $1_N$ is the vector of all ones. %TODO: Fix 
with \[Y^c=Y-(\frac{1}{N}Y1_N)1_N^{\intercal}=YH.\]
We set the centered data. The centered Gram matrix
\[G^c=(Y^c)^\intercal Y^c=H^\intercal Y^\intercal Y H =H^\intercal G H\]

\begin{theorem}\label{thm:2.6}
    For the Euclidean distance matrix $D$ and the centered Gram matrix $G1 c$ of a data set $Y$, it holds 
    \[G^c=-\frac{1}{2}H D H\]
\end{theorem}

\begin{proof}
    Straight forward calculation from $(\star)$.
\end{proof}


\begin{lemma}\label{lem:2.7}
    Assume that the matrix $D\in\EDM$ and let $G^c=-\frac{1}{2} HDH$. If the rank of $G^c$ is $r$, then 
    there is a centered $r$ dimensional configuration $Y=\{y_1,\dots,y_N\}\in\R^r$, s.t. $d_E(y_i,y_j)=d_{ij}$.
\end{lemma}

\begin{proof}
    Since $D\in\EDM$, there exists a set $Z=\{z_1,\dots,z_n\}\in\R^d$, s.t. $d_{ij}=d_E(z_i,z_j)$,
    $G^c$ is the Gram matrix of $Z$ and therefore psd. The rank is $r$, so we have $G^c=Y^\intercal Y$ with a centered 
    data matrix (via EDM). The centered data satisfies $d_{i,j}=d_E(y_i,y_j)$. We call $r$ the \dhighlight{intrinsic configuration dimension} and 
    $Y$ is the \dhighlight{exact configuration} of $D$.
\end{proof}

\subsection{(classical) multidimensional scaling (MDS)}

Instead of exact configuration of dimension $r$, we seek a lower dimensional configuration $Y\subset\R^p$, $p<r$;
\[X=\argmin_{X\subset\R^{p\times N}}\sum_{i,j}^N |d_{ij}^2-d_E^2(x_i,x_j)|\text{ } (\star\star)\]
s.t. $X=T(Y)$, with $T$ an orthogonal projection from $\R^r$ to a $p$-dimensional space $S_p\subset\R^r$ and $Y$ is an exact configuration. 

\begin{lemma}\label{lem:2.8}
    Let $Z\subset\R^r$ be a given data set with $D_Z=[d_E^2(z_i,z_j)]_{i,j=1}^N$ and let $G_Z^c=-\frac{1}{2}HD_ZH$.
    Then \[\tr(G_Z^2)=\frac{1}{2N}\sum_{i,j=1}^N d_E^2(z_i,z_j)\]
\end{lemma}

\begin{proof}
    Write out $G_Z^2$, look at diagonal, straight forward calculation.
\end{proof}

\begin{lemma}\label{lem:2.9}
    Let $D_Z$ as before. $ZH\eqqcolon\tilde{Z}=[\tilde{z}_1,\dots,\tilde{z}_n]$. Then $\Vert \hat{Z}\Vert_F=\frac{1}{\sqrt{2N}}\Vert D_Z\Vert_F$.
\end{lemma}

\begin{proof}
    With $\Vert D_Z \Vert_F^2=\sum d_E^2(z_i,z_j)$ and $\Vert\hat{Z}\Vert_F^2=\tr(\hat{Z}^\intercal \hat{Z})=\tr(\hat{G_z^2})$. The result follows from lemma \ref{lem:2.8}.
\end{proof}

\begin{theorem}\label{thm:2.10}
    Let $Y\subset\R^r$ be an exact configuration of $D\in\EDM$. The SVD of $Y$ is given by $U\Sigma V^\intercal$. For a given $p\leq r$, let 
    $U_p=[u_1,\dots,u_p]$. Then 
    \[X=U_p^\intercal Y\] 
    is a solution of the MDS minimization problem $(\star\star)$ with an error of \[2N\sum_{i=p+1}^r\sigma_i^2.\]
\end{theorem}


\begin{proof}
    Let $S_p$ be a $p$-dim subspace of $\R^r$. Let $B$ be a $r\times p$ orthogonal matrix, whose 
    columns form an orthonormal basis of $S_p$. We have $T(y)=BB^\intercal y$ and observe 
    \[d_E(B^\intercal y_i B^\intercal y_j)=\Vert B^{\intercal}(y_i-y_j)\Vert T(y_i-y_j)=d_E(Ty_i,Ty_j).\] 
    Wih $\Vert y_i-y_j\Vert\geq \Vert T (y_i-y_j)\Vert$. We get for the objective function 
    \begin{align*}
        \sum_{i,j=1}^N d_E^2(y_i,y_j)-d_E^2(B^\intercal y_i,B^\intercal y_j)&=\sum_{i,j=1}^N \langle y_i-y_j,y_i-y_j\rangle-\langle B^\intercal (y_i-y_j),B^\intercal(y_i-y_j)\rangle\\
        \stackrel{\langle B^\intercal y,B^\intercal y\rangle=\langle B^\intercal By, B^\intercal y\rangle}{=}&\dots - 2 \langle BB^\intercal (y_i-y_j),(y_i-y_j)\rangle + \langle BB^\intercal (y_i-y_j),(y_i-y_j)\rangle\\
        &=\sum_{i,j=1}^N \vert (I-BB^\intercal)(y_i-y_j)\vert^2=\Vert D_Z\Vert_F^2
    \end{align*}
    wih $Z=(I-BB^\intercal)Y$. Using Lemma \ref{lem:2.9}, we get $\Vert D_Z\Vert_F^2=2N\Vert ZH\Vert_F^2=2N\Vert Z\Vert_F^2$, 
    since $Y$ is centered $Z$ is also centered. Therefore we have to solve 
    \[\argmin_{\stackrel{B\in\R^{r\times p}}{B^\intercal B=I_p}}\Vert Y-BB^\intercal Y\Vert_F.\]
    Again we use Schmidt-Eckart-Yanns for the SVD. THe matrix $U_p \Sigma_pV_p^\intercal$ ??? So getting $B=U_p$ gives 
    \[U_pU_p^\intercal U\Sigma V^\intercal  = U_p\Sigma_pV_p^\intercal\] and 
    $X=U_p^\intercal Y$. The error estimate follows from ??? SVD and Lemma \ref{lem:2.9}.
\end{proof}

It is important to understand the flip: You can choose the minimum of the number of dimensions and the number of data points.

\beginlecture{16}{13.06.24}

Recap:

$G^c=Y^\intercal Y=(W^\intercal X| W X)=X^\intercal W^\intercal W X = X^\intercal X$

EVD of $G^c$:\marginnote{EVD= Eigen value decomposition}
\begin{align*}
    G^c=V\Lambda V^\intercal &= (V \Lambda^{\frac{1}{2}})(\Lambda^{\frac{1}{2}} V^\intercal)\\
    &=(\Lambda^{\frac{1}{2}} V^\intercal)^\intercal (\Lambda^{\frac{1}{2}} V^\intercal)
\end{align*}

Taking the top $p$ eigenvalues gives 
\[X_{\text{MDS}}=I_{p\times N}\Lambda^{\frac{1}{2}}V^\intercal\]
Also $Y=U\Sigma V^\intercal$ for PCA:
\begin{align*}
    X_{\text{PCA}}&=I_{p\times d} U^\intercal Y = I_{p\times d} \Sigma V^\intercal\\
    &=I_{p\times d} (\Sigma^\intercal \Sigma)^{\frac{1}{2}}V^\intercal=I_{p\times d}\Lambda^{\frac{1}{2}}V^\intercal
\end{align*}



We can use the following approaches:\marginnote{The SVD is most commenly used and ist most stable. If you have extreme differences in $d$ and $N$, it might be worth to use on of the other two approaches}

\begin{itemize}
    \item SVD of $Y$: $(d\times N)$: Reconstruct $Y$
    \item EVD of $YY^\intercal$ $(d\times d)$: maximal variance 
    \item EVD of $Y^\intercal Y$ $(N\times N)$: preserving similarity 
\end{itemize}


\begin{algorithm}[H] % TODO: FIx numbering
    \caption{MDS}\label{alg:mds}
 \textbf{Input:} EDM $D,P$\\
 \textbf{Output:} embedding $X$ in $p$ dimensions
 \begin{algorithmic}
   \State $G=-\frac{1}{2} H D H$
   \State $[V_p,\Lambda_p]=\text{EVD}(G,p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V^\intercal$
 \end{algorithmic}
\end{algorithm}

\begin{aremark}
    There are multiple generalizations!
\end{aremark}

\subsection{Strange effects in high dimensions}

\subsubsection*{Angles}

First we look at angles between vectors. In 2d the angle between the diagonal and an axis is $\frac{\pi}{4}$.

In general: 
\[\varphi=\arccos\frac{\langle x,y\rangle}{\Vert x\Vert\Vert y\Vert}\]
Then $\alpha=\frac{1}{\sqrt{d}}$.

\subsubsection*{Volume}

The volume of the hypersphere in $d$ dimensions:
\[\frac{r^d\pi^{\frac{d}{2}}}{\Gamma(1+\frac{d}{2})}\]
The Volume of the hypercube is $(2r)^d$.

Therefore in higher dimensions the sizes of a unit hypersphere and its bounding hypercube are not close, their ratio goes to $0$!
\subsubsection*{Concentration of measure}
If we increase the radius of the sphere by just some small $\epsilon$, we can see that most of the mass 
lies in the outer shell of hyperspheres!
\[\frac{V_{r} }{V_{r(1-\epsilon)}}=\frac{1}{(1-\epsilon)^d}\]
\subsubsection*{Curse of dimensionality}\marginnote{If we do datascience in higher dimensions, we always implicitly assume that there is more structure, otherwise most problems would not be feasable}
The higher the dimension, the more points I need to approximate functions of the same complexity as the dimension increases. This scaling is exponential!
Therefore we need lots of data to recover functions in higher dimensions.
\subsubsection*{ANOVA decomposition}
However the dimensions might be highly correlated! We can than use the ANOVA decomposition:
\[f(x)=\sum_{\tilde{d}_i={1,\dots,D}} f_{\tilde{d}_i}(x_{\tilde{1}},\dots,x_{\tilde{p}})\]

\subsection{Properties in dimensionality reduction approaches}

\begin{enumerate}
    \item Estimate the intrinsic dimensionality
    \item What kind of properties of the original data does one (implicitly or explicitly) assume to hold and to approximately preserve
    \item One often aims for latent variable separation (statistical independence, orthogonality)\footnote{this is also useful for interpretability!}
\end{enumerate}

For PCA/MDS, we can consider the rank of the matrix as the intrinsic dimensionality.\marginnote{This assumes no noise, otherwise we can be reasonably sure that the intrinsic dimensionality is smaller than the rank}

We can use criteria such as 
\[\frac{\sum_{i=1}^p\lambda_i}{\sum_{i=1}^d\lambda_i}\geq 0.95\]
or 
\[\lambda_{p+1} \leq 0.01\sum_{i=1}^d \lambda_i\]

Another procedure: The $L$-curve: Linearly approximate the eigenvalues as a function of the index from both the left and the right and take the intersection as the cutoff point.

In the latter approaches we assume a fast decline in eigenvalues.

\section{nonlinear dimensionality reduction}

We use manifolds instead of linear subspaces. Ideally we use geodesic distances $d_M(x,y)$ and 
preserve $d_M$ in the embedding. But both $M$ and $d_M$ are unknown, only 
$D\subset M$ is given. 

\dhighlight{Approach:} Build an undirected neighborhood graph $[Y,E]$ and use the graph distances $d_G$
instead of $d_M$ and aim to preserve it.

\begin{definition}\label{def:11} % TODO: Footnote not in colorbox
    Given a graph $[Y,E]$ for a data set $Y\subset\R^d$, s.t. $(y_i,y_j)\in E$ if and only if $y_i,y_j$ are adjacent\footnote{in some geometric senes}. 
    We define the \dhighlight{graph distance $d_G$} between two points $y_i,y_j\in Y$ by 
    \begin{enumerate}
        \item If $(y_i,y_j)\in E$ then $d_G(y_i,y_j)=d_E(y_i,y_j)$
        \item If $(y_i,y_j)\not\in E$ then let $\Gamma\{\gamma\mid \gamma=(\gamma_0,\dots,\gamma_s),\gamma_i\in Y, \gamma_0=y_i,\gamma_s=y_j\}$ and define 
        \[d_G(y_i,y_j)=\min_{\gamma\in\Gamma} \sum_{i=0,\dots,s-1}d_E(\gamma_i,\gamma_{i+1})\]
    \end{enumerate}
\end{definition}

We assume $Y\subset M\subset \R^d$ and that an isometric\marginnote{isometric means distance preserving} mapping 
\[f:M\to\R^p\]
exists $f(y)=x$ for $y\in M$ and \[d_E(f(y_i),f(y_j))=d_M(y_i,y_j)\]
for all $y_i,y_j\in M$.

Assume $Y$ is sampled densely enough from $M$, we expect that \[d_G(y_i,y_j)\approx d_M(y_i,y_j).\]

$D_G=[d_G^2(y_i,y_j)]_{i,j=1}^N$, we aim for $D\in\EDM$, with a configuration $X\in\R^p$ s.t. 
$D_G\approx D=[d_E^2(x_i,x_j)]_{i,j=1}^N$.

\begin{algorithm}[H]
    \caption{Isomap}\label{alg:isomap}
 \textbf{Input:} Dataset  $Y$, $p$\\
 \textbf{Output:} data set embedding $X$ in $p$ dimensions
 \begin{algorithmic}
    \State Build a neighborhood graph $[Y,E]$
    \State $D_{ij}=[d_E^2(x_i,x_j)]$ using Dijkstra's algorithm 
    \State $G=-\frac{1}{2} H D H$
    \State $[V_p,\Lambda_p]=\text{EVD}(G,p)$
    \State Return $\Lambda_p^{\frac{1}{2}} V^\intercal$
 \end{algorithmic}
\end{algorithm}

Assume $D_G\in\EDM$ we can invoke the MDS theorem \ref{thm:2.10}. We get 
\[\sum_{,j=1}^N \vert d_G^2(y_i,y_j)-d_E^2(x_i,x_j)\leq 2 N \sum_{l=p+1}^N \lambda_l.\] 

For the analysis we will use $r$-ball graphs: 

$(y_i,y_j)\in E\iff \Vert y_i-y_j\Vert_E\leq r$.

Furthermore we use the Hausdorff distance between $Y$ and $M$

\[\epsilon=H(M\mid Y)=\sup_{y\in M}\min_{y_i\in Y}\Vert y-y_i\Vert\]

\begin{theorem}\label{thm:2.12}
    Consider $M\subset\R^d$ compact and a sample $Y=\{y_1,\dots,y_N\}\subset M$ and let 
    $\epsilon=H(M\mid Y)$. For $r>0$ form the corresponding $r$-ball graph. When $\epsilon\leq \frac{r}{4}$, we 
    have for any $x,z\in Y$
    \[d_G(x,z)\leq \left(1+\frac{4\epsilon}{r}\right)d_M(x,z)\] 
\end{theorem}


\beginlecture{17}{18.06.24}

In practice we often choose $k$-nearest neighbors instead of $r$-ball neighborhoods!

\begin{aremark}
    CMDS is the same as MDS in our lecture. This is relevant for the sheets.
\end{aremark}

\begin{proof}
    For $d_E(x,z)\leq r$ we have that $(x,z)\in E$ in the $r$-ball graph and so 
    \[d_G(x,z)=d_E(x,z)\leq d_M(x,z).\]
    Now consider $d_E(x,z)>r$ and let $a=d_M(x,z)$ and let $\gamma:[0,a]\to M$ be parametrized by 
    arc length s.t. $\gamma(0)=x=y_{i_0}$ and $\gamma(a)=z=y_{i_s}$. Let $\hat{y}_{i_j}=\gamma\left(\frac{j a}{s}\right)$
    for $j=0,\dots,s$ where $s=\lceil 2a/r\rceil\geq 2$, where $\hat{y}_0=a,\hat{y}_s=z$.
    Let $y_j=\argmin_{y\in Y} d_E(y,\hat{y}_j)$. Clearly $\max_s d_E(y_{i,j}\hat{y}_j)\leq \epsilon$ for any $j=0,\dots,s-1$.
    \begin{align*}
        d_E(y_{i_j}, y_{i_{j+1}})&\leq d_E(y_{i_j},\hat{y}_j)+d_E(\hat{y}_j,\hat{y}_{j+1})+d_E(\hat{y}_{j+1},y_{i_{j+1}})\\
        &\leq \epsilon +d_M(\hat{y_j},\hat{y}_{j+1})+\epsilon\\
        &=a/s + 2\epsilon\leq r/2 + 2\epsilon< r
    \end{align*}
    Therefore $(y_{i_0},\dots,y_{i_s})$ forms a path in the $r$-ball graph.\marginnote{This might not be the shortest path in the graph}
    \begin{align*}
        d_G(x,z)&\leq \sum_{i=j}^{s-1} d_E(y_j,y_{j+1})\\
        &\leq d_M(\underbrace{\hat{y}_0}_{=x},\hat{y}_1)+\epsilon + \sum_{j=1}^{s-2} d_M(\hat{y}_j,\hat{y}_{j+1})+2\epsilon+d_M(\hat{y}_{s-1},\underbrace{\hat{y}_s}_{=z})+\epsilon\\
        &=d_M(x,z)+2(s-1)\epsilon\\
        &\leq \left(1+\frac{4\epsilon}{r}\right)d_M(x,z)
    \end{align*}
    we use that $s-1\leq \frac{2a}{r}$ with $a=d_M(x,z)$.
\end{proof}

\begin{aremark}
    Here we use that the subset of a shortest path is still a shortest path for the specific start and end points.
\end{aremark}

\begin{lemma}\label{lem:2.13}
    Let $\gamma:[0,a]\to\R^d$ be a unit speed curve with curvature bounded by $\kappa$. Then $d_E(\gamma(s),\gamma(t))\geq \frac{2}{\kappa}\sin\left(\frac{\kappa|t-s|}{2}\right)$
\end{lemma}

\begin{proof}[Proof of a weaker statement]
    Let $c$ denote the unit-speed parametrization of a circle of radius $\frac{1}{\kappa}$. From Darbins (1957) we obtain 
    $|t-s|\leq \frac{\pi}{\kappa}$:
    \[\langle \gamma(s), \gamma(t)\rangle \geq \langle c(s),c(a)\rangle\]
    This leads to \begin{align*}
        \Vert \gamma(t)-\gamma(s)\Vert \Vert \dot{\gamma}(s)&\stackrel{\text{C.S.}}{\geq}\langle \dot{\gamma(s)},\gamma(t)-\gamma(s)\rangle\\
        & = \int_s^t\langle \dot{\gamma}(s),\gamma(u)\rangle du\\
        &\geq \int_s^t\langle c(s),c(u)\rangle du\\
        &=\langle c(s),c(t)-c(s)\rangle\\
        &=\frac{1}{\kappa}\sin(\kappa\cdot(|t-s|))
    \end{align*}
    when $0\leq t-\leq \frac{\pi}{\kappa}$.

    We now assume $M\subset \R^d$ is compact and connected $C^2$-manifold with empty or $C^2$-boundary. In particular, shortest path on $M$ have curvature bounded by some $\kappa$.
\end{proof}

\begin{lemma}\label{lem:2.14}
    Suppose $M\subset \R^d$ has the above properties. Then for any $x,z\in M$ s.t. $d_M(x,z)\leq \frac{\pi}{\kappa}$
    \begin{enumerate}
        \item $d_M(x,z)\max\left(\frac{2}{\pi},1-\frac{\kappa^2}{24}d_M(x,z)^2\right)\leq d_E(x,z)\leq d_M(x,z)$. Moreover there is $\tau>0$ depending on $M$ s.t. for all $x,z\in M$ with 
        $d_E(x,z)\leq \tau$ it holds
        \item $d_E(x,z)\leq d_M(x,z)\leq d_E(x,z)\min\left(\frac{\pi}{2}+C_0\kappa^2d_E(x,z)^2\right)$ where $c_0$ is a constant that can be taken to be $c_0=\frac{\pi^2}{50}$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    For 1.:
    
    Take $x,z\in M$ and let $a=d_M(x,z)\leq \frac{\pi}{\kappa}$. Let $\gamma:[0,a]\to M$ be a unit-speed 
    shortest path on $M$ s.t. $\gamma(0)=x,\gamma(a)=z$. By assumption $\gamma$ has curvature bounded by $\kappa$. Applying lemma \ref{lem:2.13} gives 
    \[d_E(x,z)=d_E(\gamma(0),\gamma(a))\geq \frac{2}{\kappa}\sin\left(\kappa \frac{a}{2}\right)\geq \max\left(a-\frac{\kappa^2}{24}a^3,\frac{2}{\pi}a\right)\]
    and $\sin(t)\geq t-\frac{t^3}{6},\sin(t)\geq \frac{2t}{\pi}\forall t\leq \frac{\pi}{2}$.

    For 2.: by 1. $d_M(x,z)\leq \frac{\pi}{\kappa}$, then $d_M(x,z)\leq \frac{\pi}{2}d_E(x,z)$ and 
    \begin{align*}
        d_M(x,z)&\leq \frac{d_E(x,z)}{\left(1-\frac{\kappa^2}{24}d_M(x,z)^2\right)}\\
        &\leq \frac{d_E(x,z)}{\left(1-\frac{\kappa^2}{24}\left(\frac{\pi}{2}d_E(x,z)\right)^2\right)}\\
        &\leq d_E(x,z)(1+C_0\kappa^2 d_E(x,z)^2)
    \end{align*}
    where the last inequality holds if $\kappa d_E(x,z)$ is small enough. Therefore, it is enough to show that 
    there is $\tau>0$ s.t. \[d_M(x,z)\leq \frac{\pi}{\kappa}\]
    when $d_E(x,z)\leq \tau$. This is true, because the smoothness of $M$ guarantees that $d_M$  
    be continuous as a function  of the compact set $M\times M$.
\end{proof}
\begin{theorem}\label{thm:2.15}
    Suppose $M\subset\R^d$ has the above properties. Consider a sample $Y=\{y_1,\dots,y_m\}\subset M$. For $r>0$ from 
    the corresponding $r$-ball graph. Let $c_0$ and $\tau$ be defined per lemma \ref{lem:2.14} and $\kappa r\leq \frac{1}{3}$. We have 
    \[d_M(x,z)\leq (1+C_0\kappa^2r^2)d_G(x,z)\ \forall x,y\in Y\]
\end{theorem}

\begin{proof}
    Fix $x,z\in Y$. Let $x=y_{i_0},\dots,z=y_{i_s}$ define a shortest path in the graph joining $x,z$, so that $d_G(x,z)=\sum_{j=0}^{s-1}\Delta_j$, where 
    $\Delta_j=d_E(y_{i_j},y_{i_{j+1}})$. Define $a=d_M(x,z)$ and $a_j=d_M(y_{i_j},y_{i_{j+1}})$. 
    Since $\Delta_j\leq r \leq \tau$ by lemma \ref{lem:2.14} we see 
    \[\Delta_j\min\left(\frac{\pi}{2}\,c_0\kappa^2 \Delta_j^2 \right)\geq a_j.\]
    By assumption, $\kappa r\leq \frac{1}{3}$, and this can be seen to imply $1+C_0\kappa^2 r^2\leq \frac{\pi}{2}$, which then implies that $a_j\leq \Delta_j+C_0\kappa^2\Delta_j^3$, we thus have 
    \[a\leq \sum_{j=1}^{s-1} a_j\leq \sum_{j=0}^{s-1}(\Delta_j + C_0\kappa^2\Delta_j^3)\leq \sum_{j=0}^{s-1}\Delta_j (1+C_0\kappa^2r^2)=(1+C_0 \kappa^2 r^2)d_G(x,z)\qedhere\]
\end{proof}

\begin{aremark}
    This is the justification for using Isomap. One should consider, that the assumptions are NOT always met in practice, which might result our algorithm to fail. In particular we don't know 
    $M$ in practice, which makes our assumptions hard to check \dots

    Luckily the assumptions are often met ``naturally''.
\end{aremark}

\subsection{Parallel transport unfolding}

Underlying idea: % Bild 
Systematically overestimating the distance (by doing unnecessary turns on the graph). We then think about the tangent spaces and try to 
connect the local views of the tangent spaces. Main idea we find a better path (closer to the geodesic).

\beginlecture{18}{20.06.24}

We will now consider the computational ingredients using discrete parallel transport.
We are using a $l$-nearest neighbor graph.
\begin{enumerate}
    \item Approximate tangent spaces: Take $q$-NN in the graph, $q\geq k$\marginnote{separate $k,q$ to avoid unwanted connection while still getting good approximations}, denoted \[\{\tilde{y}_{l}^i\}_{l=1}^q,\text{ }\hat{y}_l^i = \tilde{y}_l^i-y_i\ \hat{Y}^i=[\hat{y}_1^i,\dots,\hat{y}_q^i]\]\marginnote{The space spanned by the columns of $T_i$}The $p$ left singular vectors of $\hat{Y}^i$ to the $p$ largest singular values give an orthonormal basis for $T_i$, spanning tje approximate tangent space\[T_i=[t_1^i,\dots,t_p^i]\in\R^{d\times p}\]
    \item Now given $y_o$\marginnote{$O(p)$ are the $p\times p$ orthogonal matrices} % TODO: FIX
    and $y_j$ connected in $G$ \[R_{j,i}=\argmin_{R\in O(p)}\Vert T_i-T_jR\Vert_F \]
    $R_{j,i}$ is the discrete metric connection between $y_i,y_j$, it best aligns $T_i,T_j$. By definition $R_{i,j}=R_{j,i}^{-1}=R_{j,i}^T$. With $T_i^TT_j=U\Sigma V^\intercal$ we can show $R_{j,i}=VU^T$.
    \item  Consider $(y_i,y_j,y_k)$ and start w.l.o.g. $y_i$. The first edge \[l_i=y_j-y_i\]
    is projected onto $T_i$: $v_i=T_i^\intercal e_i$.\marginnote{Projected in the $L^2$ sense}
    We set $z_i=0$ and $z_j=z_i+v_i$ defines the first modified edge. Now, $e_j=y_k-y_i$ and $T_j^\intercal e_j$ is parallel transported 
    into the tangent space at $y_i$\[v_j=R_{i,j}[T_j^\intercal e_j]\] and $z_k=z_j+v_j$%TODO: FIX
    Under some assumptions 
    \[\Vert v_i+v_j\Vert_2=d_E(z_i,z_k)\approx d_M(y_i,y_k)\]
    \item For longer paths $y_{i_1},\dots,y_{i_m}$ we iteratively project the edges $l_{i_s}=y_{i_{s+1}}-y_{i_s}$ onto $T_{i_j}$ before parallel transporting back to $T_{i_1}$\[V_{i_s}=\left(\prod_{j=1}^{s-1} R_{i_j,i_{j+1}}\right)[T_{i_s}^\intercal l_{i_s}]\]
    with $V=\sum_{j=1}^M v_{I_S}$ WE GET $\Vert v \Vert_2\approx d_M(y_1,y_{i_m})$
\end{enumerate}
\[(\star)\ 1-\xi \frac{d_G(y_i,y_j)}{d_M(y_i,y_j)}\leq 1+\xi\]
We saw in PTU 
\[\min_{R\in O(p)}\Vert Y^\intercal - X^\intercal R\Vert_F.\]
Generally aligning two point sets using an orthogonal transformation is called \dhighlight{procrustes problem}.

Pointwise:

\[\min_{R\in O(p)}\sum_{i=1}^N\Vert y_i - Rx_i\Vert_2^2.\]

Consider two configurations $X,Y$\marginnote{which stem from two different EDMs, which are somehow alligned / comparable}, given
\[\Vert Y^\intercal Y-X^\intercal X\Vert_F=\epsilon^2\]
and 
\[\Vert X^\dagger\Vert_2\epsilon\leq \frac{1}{\sqrt{2}}\]
it holds 
\[\min_{R\in O(d)}\Vert Y^\intercal X^\intercal R\Vert_F\leq (1+\sqrt{2})\Vert X^\dagger\Vert_2 \epsilon^2\]

For $D,\tilde{D}\in \EDM$ it holds using $\epsilon^2=\frac{1}{2}\Vert H(\tilde{D}-D)H\Vert_F$ a corresponding result.

Using $(\star)$ for Isomap amd $X$ the exact embedding using $d_M$ into $\R^p$ one has 
\[\min_{R\in O(p)}\left(\frac{1}{N}\sum_{i=1}^N\Vert z_i-R x_i\Vert^2\right)^{\frac{1}{2}}\leq \frac{36\sqrt{p}\rho^3}{w^2}\xi\]
for $Z$ the approximate embedding and $\rho,w$ are singular values of our datamatrix:

\[\rho=\rho(X)=\frac{\sigma_1(X)}{\sqrt{N}}\]
radius of $X$ largest deviations along any direction in space.
\[w=w(X)=\frac{\sigma_p(w)}{\sqrt{N}}\]
half width of $X$, smallest deviations along any direction in space.

\subsection*{Using kernels again ...}

Nonlinear PCA

Reminder: $C=\frac{1}{N}Y\cdot Y^\intercal=\frac{1}{N}\sum_{i=1}^N y_i\cdot _i^T$
we consider some \dhighlight{feature map} $\phi:R^d\to\R^r\eqcolon \cF$
For simplicity, we assume to be mean-centered on the feature space:
\[\sum_{i=1}^N\phi(y_i)=0.\]
Now, do PCA in $\cF$, $C_\phi=\frac{1}{N}\sum_{i=1}^N\phi(y_i)\phi(y_i)^\intercal = \frac{1}{N}\phi(Y)\phi(Y)^\intercal=\frac{1}{N}\Phi\cdot\Phi^\intercal$.

EVD gives $C_\phi u_i=\lambda_i u_i$ and by theorem \ref{thm:2.3}  the nonlinear principal components are 
\[X_i=u_i^\intercal \Phi.\]
IGNORE:
\[\Phi\Phi^\intercal\to \Phi^\intercal \Phi\]
\[U^\intercal=V^\intercal \Phi^\intercal\]
\[X=V^\intercal\Phi^\intercal \Phi=\Lambda^{-\frac{1}{2}}\Phi^\intercal \Phi=\Lambda^{-\frac{1}{2}}V^\intercal V \Lambda V^\intercal=\Lambda^{\frac{1}{2}}V^\intercal\]
STOP IGNORING
\[x_I=V^\intercal \Phi^\intercal\Phi(y_i)\]
\[K_{i,j}=\langle \Phi(y_i),\Phi(y_j) \rangle_{\cF}\eqqcolon K(y_i,y_j)\]
A projection of a point $y$ with image $\phi(y)$ gives 
\begin{align*}
    x&=V^\intercal \Phi^\intercal \phi(y)\\
    &=\sum_{i=1}^N v_i\langle \phi(y_i),\phi(y)\rangle_{\cF}\\
    &=\sum_{i=1}^N v_i K(y_i,y)
\end{align*}

This kernelized form we call \dhighlight{Kernel MDS}, but it is usually called 
\dhighlight{Kernel PCA} in the literature.\marginnote{The interpretation step of PCA gets a lot harder. Especiially, since the feature maps are not unique \dots} 

To consider non-centered data, we need to center it: 
\[\tilde{\phi}(y_i)\coloneqq \phi(y_i)-\frac{1}{N}\sum_{j=1}^N\phi(y_j)\]
with associated kernel $\tilde{K}$.
\[\tilde{\Phi}(Y)=(\Phi^c(Y)-\frac{1}{N}\Phi(Y)1)1^\intercal = \Phi(Y)H\]
Action of $H$ goes through the SP 
\[\tilde{K}=K^c=HKH\]
\begin{algorithm}[H] 
    \caption{Nonlinear PCA}\label{alg:nl_pca}
 \textbf{Input:}  nonlinear data set $Y$ $\phi:\R^d\to\R^r,p$\\
 \textbf{Output:} configuration $X$ in $p\leq \min(r,N)$ dimensions
 \begin{algorithmic}
   \State $K=-H\Phi^\intercal(Y)\Phi H$
   \State $[V_p,\Lambda_p]=\text{EVD}(K,p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V_p^\intercal$
 \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
    \caption{Nonlinear MDS}\label{alg:nl_mds}
 \textbf{Input:}  nonlinear data set $Y$, Kernel $K:\R^d\times\R^d\to\R$\\
 \textbf{Output:} configuration $X$ in $p\leq N$ dimensions
 \begin{algorithmic}
   \State $K_{i,j}=K(y_i,y_j)$
   \State $\hat{K}=HKH$
   \State $[V_p,\Lambda_p]=\text{EVD}(\hat{K},p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V_p^\intercal$
 \end{algorithmic}
\end{algorithm}

\beginlecture{19}{25.06.24}

Connection of PCA and MDS (motivated by the kernelized versions):

$X_{\text{PCA}}=I_{p\times d} U^\intercal Y=I_{p\times N}\Lambda^{\frac{1}{2}}V^\intercal$

\subsection{A view on Kernel MDS embedding}

$K=V\Lambda V^\intercal\iff V^\intercal K V = \Lambda$

\begin{align*}
    \lambda_i \delta_{i,j}&=V_i^\intercal L V_j=\sum_{l_1,l_2}^N V_{l_1,i} k(y_{l_1},y_{l_2}) V_{l_2,j}\\
    &=\langle \sum_{l_1=1}^N V_{l_1,i} k (y_{l_1},\cdot),\sum_{l_2=1}^{N} V_{l_2,j} k(y_{l_2},\cdot)\rangle
\end{align*}

This tells uns that the 
\[f_i(\cdot)=\frac{1}{\sqrt{\lambda_i}}\sum_{l=1}^N V_{l,i}k(y_{l_i},\cdot)\]
is a orthonormal basis of $\cH_Y$ if rank$K=N$.

The first $p$ $f_i(y)$ give an embedding into $\R^p$ of $y$.

\begin{remark}
    Let $\tilde{K}=HKH$. It can be seen that $\tilde{K}$ is psd iff $K$ is cpsd. The centering removes 
    the dependence on the origin in the feature space.
    Reminder: $G^c=-\frac{1}{2}HDH$, for comparison. We also considered negative distance as a cpsd kernel.
\end{remark}

\begin{remark}
    Just using any kernel an actually increase the embedding dimension. Take the Gaussian kernel, it will give a matrix with full rank $N$.
    Looking at the decay of EV of the kernel matrix shows a weak decay!     
\end{remark}

\subsection{Maximum Variance Unfolding}

\begin{aremark}
    he uses $\vert\vert\vert$ for matrix norms, while I still use $\Vert$!
\end{aremark}

\begin{enumerate}
    \item Euclidean embedding space Gram matrix $K$ is to be computed, so a constraint is \highlight{$K\in$ SPSD}, i.e. the symmetric positive semi-definite matrices.
    \item We use Kernel MDS and aim for a \highlight{centered} $K$. 
    \item We want to \highlight{locally preserve distances}: Locally euclidean distances are ok \[d_E(y_i,y_j)=d_E(\phi(y_i),\phi(y_j))\]
    \item We \highlight{maximize the variance} by maximizing the pairwise distance in the feature space for $y_i,y_j$ with $(y_i,y_j)\notin E$:\[\frac{1}{2N}\sum_{i,j=1}^Nd_E^2(\phi(y_i),\phi(y_j))\stackrel{\text{Lemma \ref{lem:2.8}}}{=}\tr(G_\phi^c)\]
\end{enumerate}

\begin{definition}\label{def:2.16}
    Given a data set $Y$ and a neighborhood graph $[Y,E]$. The solution of 
    \begin{eqnarray}
        \max_{K\in\text{SPSD}} \tr(K)\text{ s.t. } \sum_{i,j}^N K_{ij}=0 \label{eq:2.1}
    \end{eqnarray}
    and with $(y_i,y_j)\in E$: $d_E^2(y_i,y_j)=K_{ii}-2K_{ij}+K_{jj}$ as a second condition,
    where $\sum_{i,j}^N K_{ij}=0$ is the centering condition
\end{definition}


\begin{algorithm}[H] 
    \caption{MVU}\label{alg:mvu}
 \textbf{Input:}  data set $Y$, its pairwise distances, $p$\\
 \textbf{Output:} Embedding in their $p$ dimensions
 \begin{algorithmic}
    \State Build their neighborhood graph $[V,E]$
   \State Solve (\ref{eq:2.1}) to get $K$
   \State $\hat{K}=HKH$
   \State $[V_p,\Lambda_p]=\text{EVD}(K,p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V_p^\intercal$
 \end{algorithmic}
\end{algorithm}

We can write the MVU problem also as 
\begin{eqnarray}
    \max_{D\in \EDM}\sum_{i,j=1}^N D{ij}\text{ s.t. }D_{ij}=d_E^2(y_i,y_j)\text{ for } (y_i,y_j)\in E\label{eq:2.2}  
\end{eqnarray}

\begin{theorem}\label{thm:2.17}
    Let $C\subset\DM$ and $[Y,E]$ a data graph with weights $d_W$. If the graph is connected,
    the following two constraint optimization problems are equivalent:
    \begin{enumerate}
        \item (\ref{eq:2.2})\[\max_{D\in \EDM}\sum_{i,j=1}^N D{ij}\text{ s.t. }D_{ij}=d_E^2(y_i,y_j)\text{ for } (y_i,y_j)\in E\]
        \item \begin{equation}\min_{D\in C}\Vert D-D^G\Vert_1  \min_{D\in C}\sum_{i,j=1}^N\vert D_{ij}-D_{ij}^G\vert\label{eq:2.3}\end{equation} s.t. $D_{ij}=d_W(y_i,y_j)$ if $(y_i,y_j)\in E$
            where $D_{ij}^G=d_G^2(y_i,y_j)$ is the (squared) graph distance matrix for the edge weights $d_W$. 
    \end{enumerate}
\end{theorem}

\begin{proof}
    Let $D\in C$. Then for all $1\leq i,j\leq N$ and paths $\gamma\in[Y,E]$ connecting 
    $y_i=\gamma_0,y_j=\gamma_{s+1}$ the triangle inequality implies 
    \begin{align*}
        \sqrt{D_{ij}}\leq \sum_{k=0}^s\sqrt{D_{\gamma_k\gamma_{k+1}}}=\sum_{k=0}^s\sqrt{d_W^2(\gamma_k\gamma_{k+1})}=\Vert\gamma\Vert =l
    \end{align*}
    In particular this holds for the shortest paths between $y_i,y_j$, $D_{ij}\leq d_G^2(y_i,y_j)$ for all $1\leq i,j\leq N$.

    This gives \begin{align*}
        \sum_{i,j=1}^N D_{ij}-\underbrace{\sum_{i,j=1}^N D_{ij}^G}_{\text{const}}&=\sum_{i,j=1}^N(D_{ij}-G_{ij}^G)\\
        &=-\sum_{i,j=1}^N \vert D_{ij}-D_{ij}^G\vert =-\Vert D-D^G\Vert_1 \qedhere
    \end{align*}

\end{proof}

\begin{corollary}\label{cor:2.18}\marginnote{Note that we are optimizing over distance matrices, not euclidean distance matrices!}
    Let $[Y,E]$ be a connected graph with weights $d_W$. Then $D^G$ is the unique solution of 
    \[\max_{D\in \DM}\sum_{i,j=1}^N D_{ij}\]
    s.t. $D_{ij}=d_W^2(y_i,y_j)$ if $(y_i,y_j)\in E$.
\end{corollary}


\begin{corollary}
    Let $[Y,E]$ be a connected graph with weights $d_W$. Then
    \begin{enumerate}
        \item If $D^G\in\EDM$, then $D^G$ is the unique solution of the MVU problem (\ref{eq:2.1})
        \item The problem from definition \ref{def:2.16} or (\ref{eq:2.1}) is equivalent to 
            \[\min_{D\in \EDM}\Vert D-D^G\Vert_1\] s.t. $D_{ij}=d_E^2(y_i,y_j)$ if $(y_i,y_j)\in E$. 
    \end{enumerate}
\end{corollary}

Therefore MVU can be considered as a regularized shortest path problem.

\begin{remark}
    Isomap can be seen to use the best $\EDM$ Approximation to $D^G$ in the sense of 
    \[\min_{D\in \EDM}\Vert H(D-D^F)H\Vert_F.\] We saw in the exercises that this relates to cutting of eigenvalues!    
\end{remark}

We denote for a set $S$ by $\R^S$ the set of real valued functions on $S$. The restriction of a function 
$f\in \R^S$ to a subset $\tilde{S}$ of $S$ is denoted by $f\vert_{\tilde{S}}$.

\begin{lemma}\label{lem:2.20}
    Let $S$ be a set, $\tilde{S}\subset S$,$C\subseteq\R^{\tilde{S}}$, $f\in\R^S$ and $\tilde{f}\in\R^{\tilde{S}}$. Let $\Vert \cdot\Vert$ be a 
    norm on $\R^{\tilde{S}}$ and $c,\epsilon\geq 0$. If \begin{equation}
        \Vert \tilde{f}-f\vert_{\tilde{S}}\Vert \leq c\epsilon
    \label{eq:2.4}\end{equation} and  \begin{equation}
        (1-\epsilon)f\vert_{\tilde{S}}\in C\label{eq:2.5}
    \end{equation},
    then 
    \[\Vert \hat{f}-f\vert_{\tilde{S}}\Vert \leq (2c+\Vert \tilde{f}\Vert)\epsilon\]
    for all $\hat{f\in\argmin_{\bar{f}\in C}}\Vert\bar{f}-\tilde{f}\Vert $
\end{lemma}


\begin{proof}
\begin{align*}
    \Vert \hat{f}-f\vert_{\tilde{S}}\Vert&\leq \Vert \hat{f}-\tilde{f}\Vert+\Vert\tilde{f}-f\vert_{\tilde{S}}\Vert\\
    &\stackrel{(\ref{eq:2.4})}{=} ... 
\end{align*}    
\end{proof}


\begin{theorem}\label{thm:2.21}
    Let $[Y,E]$ be a connected graph, $Y\subset M$, where $M$ is a convex and compact manifold. For $D_G=[d_G^2(y_i,y_j)]_{i,j=1}^N$.
    We assume it holds that it holds for some $\epsilon>0$ that 
    \[(1-\epsilon)d_M^2(y_i,y_j)\leq d_G^2(y_i,y_j)\leq (1+\epsilon)d_M^2(y_i,y_j)\]
    Then the solution $D$ of the MVU problem from definition \ref{def:2.16} satisfies 
    \[\Vert D-D_M\Vert_1 \leq 3\Vert D_M\Vert_1\epsilon\leq 3(N\cdot\text{diam}(M))^2\epsilon\]
    where diam$(M)=\sup_{x,y\in M}d_M(x,y)$.
\end{theorem}

\begin{proof}
    Let $S=M\times M$, $\tilde{S}=Y\times Y$, $C=\{\hat{f}\in\R^{\tilde{S}}\mid [\hat{f}(y_i,y_j)]_{i,j=1}^N\in \EDM\}$
    $f(y,z)=d_M^2(y,z)$, $\tilde{f}(y_i,y_j)=d_G^2(y_i,y_j)$, $\Vert x\Vert = \Vert x(y_i,y_j)_{i.j=1}^N\Vert_1$, $c=\Vert D^M\Vert_1$.

    Condition 1 of \ref{lem:2.20} \begin{align*}
        \Vert \tilde{f}-f\vert_{\tilde{S}}\Vert&=\Vert D^G-D^M\Vert_1\\
        &\leq \sum_{i,j=1}^N \epsilon d_M^2(y_i,y_j)\\
        &=\epsilon \Vert D_M\Vert_1=\epsilon c
    \end{align*}
Condition 2: We observe $d_M\in \EDM$. Since $\EDM$ is a cone, it follows that $(1-\epsilon)D_M\in\EDM$, so (\ref{eq:2.5}) holds in 
lemma \ref{lem:2.20}. This implies 
\[\Vert \tilde{f}-f\vert_{\tilde{S}}\Vert\leq (2c+\Vert f\Vert_{\tilde{S}})\epsilon=3\Vert D_M\Vert_1\epsilon\]
Therefore $\Vert DD_M\Vert_1\leq 3\Vert D_M\Vert_1 \epsilon$.

Additionally $\Vert D_M\Vert_1=\sum_{i,j=1}^N d_M^2(y_i,y_j)\leq N^2 \max_{1\leq i,j\leq N} d_M^2(_i,y_j)\leq (N\text{diam M})^2$ 
\end{proof}

\beginlecture{20}{27.06.24}

\section{Spectral clustering}

CLustering $\triangleq $ automatically finding groups in unlabeled data.

Let $[Y,E]$ be an undirected graph with \dhighlight{weight adjacency matrix} $w=[w_{jk}]_{j,k=1}^N$.
In particular: if $(y_i,y_j)\in E\iff w_{ij}>0$. The \dhighlight{degree of a node} $y_j\in Y$ is $d_j=\sum_{k=1}^N w_{jk}$ \marginnote{If $w$ is just the adjacency matrix, this coincides with the original definition of the degree of a node: the number of adjacent nodes}
and the degree matrix is given by $D=\text{diag}(d_1,\dots,d_N)$. Often we consider 
\[w_{jk}=\exp\left(\frac{-\Vert y_j-y_k\Vert_2^2}{2\sigma^2}\right)\]
With that we define the \dhighlight{graph Laplacian} 
\[L=D-W\]

\begin{theorem}\label{thm:2.22}
    The matrix $L=D-W$ satisfies the following properties
    \begin{enumerate}
        \item For any $f\in\R^n$ it holds \[f^T L f = \frac{1}{2}\sum_{j,k=1}^N w_{jk}(f_j-f_k)^2\]
        \item $L$ is spsd 
        \item The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the vector of ones $1$.
        \item $L$ has $N$ nonnegative, real-valued eigenvalues $0\leq\lambda_1\leq \dots\leq \lambda_N$.
    \end{enumerate}
\end{theorem}

\begin{proof}

    1.:
    \begin{align*}
        f^T L f = f^T D f - f^TWf &= \sum_{j=1}^N d_j f_j^2 - \sum_{j,k=1}^N f_j f_k w_{jk}\\
        &=\frac{1}{2} \left(\sum_{k=1}^N \underbrace{d_j}_{=\sum_{k} w_{jk}} f_j^2-2\sum_{j,k=1}^N f_j f_k w_{jk}+ \sum_{j=1}^N d_k f_k^2 \right)\\
        &=\frac{1}{2}\sum_{j,k=1}^N w_{jk}(f_j-f_k)^2
    \end{align*}
    2.: $D,W$ are symmetric, so is $L$. psd follows from 1., since $w\geq 0$

    3.: psd $\implies$ nonnegative eigenvalues. The fact that $0$ is an eigenvalue follows from the definition of $D$ via $W$
    
    4.: follows from 1. - 3..
\end{proof}

\begin{theorem}\label{thm:2.23}\marginnote{This makes sense, since eigenfunctions for the eigenvalue $0$ of the Laplace-Beltrami operator are constant functions, in the sense of constant on each connected component}
    Let $[Y,E]$ be an undirected graph with nonnegative weights. Then the multiplicity $k$ of 
    the eigenvalue $0$ of $L$ gives the number of connected components $A_1,\dots,A_k$ in the graph.
    The eigenspace of the eigenvalue $0$ is spanned by the indicator vector $1_{A_1},\dots,1_{A_k}$
    of these components.
\end{theorem}

\begin{proof}
    For $k=1$, assume $f$ is eigenvector for $\lambda=0$. So 
    \[0=f^T L f = \sum_{j,k=1}^N \underbrace{\underbrace{w_{jk}}_{\geq 0}\underbrace{(f_k-f_k)^2}_{\geq 0}}_{\stackrel{!}{=}0}\]
    by $w_{jk}>0\implies f_{j}=f_k\implies f$ is constant for all nodes can be connected by a path in $[Y,E]$.
    
    Since all nodes of a connected component can be connected b a path, $f$ is constant on the component. 
    So for one connected component $1_{A_1}$ is an eigenvector for $\lambda=0$, which is 
    the indicator function of $A_1$.

    $k>1$:\marginnote{Here we only used the distinction $w_{ij}>0$, which means for this purpose it would be fine to just use the adjacency matrix!}
    \[L=\begin{bmatrix}
        L_1 & &&\\
        & L_2 &&\\
        &&\ddots &\\
        &&&L_N
    \end{bmatrix}\]

    Wlog the nodes are ordered according to the component. So L is of block-diagonal form.

    For a block diagonal matrix, the spectrum is given by the 
    union of the spectra of the blocks. The eigenvectors of $L$ are those of the blocks 
    filled with zeros. Each $L_i$ is a graph laplacian (GL), so the argument for $k=1$ carries over.
    Together, $L$ has eigenvalue $0$ with multiplicity $k$ and the indicator functions are eigenvectors. 

\end{proof}
%TODO: FIX
\begin{algorithm}[H] 
    \caption{Spectral clustering}\label{alg:spec_clust}
 \textbf{Input:}  $L,k$\marginnote{We can determine $k$, but if we can also set $k$, if our graph is connected}\\
 \textbf{Output:} Clusters $A_1,\dots,A_k$
 \begin{algorithmic}
    \State Compute $U_k=EVD(L,k)$
    \For $i=1\dots N$
        \State $x_j$ $j$th component of $U$ in $\R^k$
    \EndFor
    \State Cluster the points $\{x_i\}_i$ with $k$-means into clusters $c_1,\dots c_k$
    \State return $(A_i)_{i=1}^k$ with $A_i=\{j\mid x_j\in c_i\}$
 \end{algorithmic}
\end{algorithm}


\begin{algorithm}[H] % TODO: FIX BOTH
    \caption{Kmeans}\label{alg:k_means}
 \textbf{Input:}  $X,k$\\
 \textbf{Output:} $k$ clusters that segment the data
 \begin{algorithmic}
    \State Pick randomly $k$ points $x_j$ as cluster centers
    \While {not converged}
        \State for each $x_j$ determine the closest cluster center $x_j$ and assign it to it 
        \State for each cluster, update the cluster center to be the mean of the cluster
    \EndWhile
    \State Cluster the points $\{x_i\}_i$ with $k$-means into clusters $c_1,\dots c_k$
    \State return $(A_i)_{i=1}^k$ with $A_i=\{j\mid x_j\in c_i\}$
    \State while cluster assignments do choose???
 \end{algorithmic}
\end{algorithm}

Consider now \highlight{random walks on graphs}, stochastic process that 
randomly jumps from node to node.

\dhighlight{Transition probability} $p_{jk}$ pf jumping in one step from 
$y_j$ to $y_k$
\[p_{jk}=\frac{w_{jk}}{d_j}\]

The \dhighlight{transition matrix} $P=[p_{jk}]_{j,k=1}^N$ of the random walk is 
\[P=D^{-1}W.\]

Let $[Y,E]$ be connected amd non-bipartite.\marginnote{A graph is bipartite if $\exists U\cup V=Y$, $U\cap V=\emptyset$ with $(x,y)\in E\implies x\in U,y\in V$ or vice versa}

The corresponding random walk always processes a unique stationary distribution $\pi=[\pi_1,\dots,\pi_N]^\intercal$, since $[Y,E]$ is connected.
\[\pi_i=\frac{d_i}{\text{Vol}(y)}\]
where Vol$(y)=\sum_{i=1}^N d_i$.
\marginnote{stationary means $P\pi=\pi$}
We introduce the \dhighlight{normalized graph laplacian} or \dhighlight{random Walk laplacian}
\[\Lrw=D^{-1}L=I-D^{-1}W=I-P\]
\marginnote{Theorem \ref{thm:2.23} also works for $\Lrw$}
and observe \[\Lrw u = \lambda u \iff Pu = (1-\lambda)u\iff Lu = \lambda Pu.\]
We can also consider $\Lsym=D^{-\frac{1}{2}}L D^{-\frac{1}{2}} = I-D^{-\frac{1}{2}} W D^{-\frac{1}{2}}$,
the \dhighlight{symmetric normalized graph Laplacian}.

We now use the \dhighlight{normalized cut} (\highlight{Ncut}) as a criteria to partition the graph, s.t. 
edges between partitions have very low weight and edges inside partitions have large weights.

\begin{definition}\label{def:2.24}
    Let $G=[Y,E,W]$ be a connected, undirected graph with symmetric, nonnegative edge weight matrix.
    The \dhighlight{normalized cut partition} of $G$ into $k$ \dhighlight{partitions} is given by 
    \[\min_{\stackrel{A_1,\dots,A_k\subset V:\bigcup A_j=V}{A_j\cap A_k=\emptyset}} \text{Ncut}(A_1,\dots,A_k)\]
    where Ncut$(A_1,\dots,A_k)=\frac{1}{2}\sum_{i=1}^k \frac{W(A_i\overline{A}_i)}{\text{vol}(A_i)}$
    and $W(A,B)=\sum_{j\in A,k\in B}w_{j,k}$, $\overline{A}_i = V\setminus A_i$.
\end{definition}

\begin{theorem}\label{thm:2.25}
    Let $[Y,E,W]$ be a connected and non-bipartite graph. Assume that we run the random walk 
    $(X_t)_{t\in \N}$ starting with $x_0$ in the stationary distribution $\pi$, i.e. $x_0\sim\pi$. For disjoint subsets $A,B\subset V$, we denote 
    \[P(B\mid A)=P(X_1\in B\mid X_0\in A).\]
    Then Ncut$(A,\overline[A])=\frac{1}{2}\left(P(\overline{A}\mid A)+P(A\mid \overline{A})\right)$
\end{theorem}

This tells us that when minimizing Ncut, we look for a cut, through the graph s.t. a random walk rarely transitions 
from $A$ to $\overline{A}$ and vice versa.

\beginlecture{21}{02.07.24}

\subsection*{Review: Markov Chain / Random Walk}

Family $(X_t)_{t\in\N}$ of random variables on a countable set $X=\{x_1,\dots,x_N\}\subseteq \Omega\subseteq \R^d$
is a \dhighlight{Markov chain}, if 
\[\bP(X_t=x_{i_t}\mid X_{t-1}=x_{i_{t-1}},\dots,X_1=x_{i_1})=\bP(X_t=x_{i_t}\mid X_{t-1}=x_{i_{t-1}})\ \forall t\in\N,i_j\in\{1,\dots,N\}\]
Now if $\bP(X_t=x_i\mid X_{t-1}=x_j)=p_{ji}$ is the same for all times $t$, we have a \marginnote{This is not assumed to be symmetirc}
\dhighlight{homogen Markov Chain} and call it a \dhighlight{random walk} on the data.

Further, for such a random walk, $\pi=[\pi_1,\dots,\pi_N]^\intercal$ is the stationary distribution, if 
\begin{enumerate}
    \item $\pi\geq 0$ and $|\pi_1|_1=1$
    \item $\pi=P^\intercal\pi$, i.e. $\pi_i=\sum_{j=1}^NP_{ji}\pi_j$
\end{enumerate}
As said, we collect the $p_{ji}$ in the \dhighlight{probability transition matrix}
\[P\coloneqq [p_{ji}]_{j,i=1}^N\]

\begin{proof}[Proof of theorem \ref{thm:2.25}]
    \begin{align*}
        P(x_0\in A\mid x_1\in B)&=\sum_{j\in A,k\in B} P(X_0=j,X_1=k)=\sum_{j\in A,k\in B}\pi_{j}p_{jk}\\
        &=\sum_{j\in A,k\in B} \frac{d_j}{\text{Vol}(Y)}\frac{w_{jk}}{d_j}=\sum_{j\in A, k\in B} w_{jk}
    \end{align*}
    The stationary distribution of the random walk can be seen to be 
    \[\pi_i=\pi(x_i)=\frac{d(x_i)}{\text{Vol}(Y)}=\frac{d(x_i)}{\sum_{i=1}^N d(X_i)}\]
    With this, we obtain 
    \begin{align*}
        \bP(X_1\in B\mid X_0\in A)&=\frac{\bP(X_0\in A,X_1\in B)}{\bP(X_0\in A)}\\
        &=\left(\frac{1}{\text{Vol}(Y)}\sum_{j\in A,k\in B}w_{jk}\right)\left(\frac{\text{Vol}(A)}{\text{Vol}(Y)}\right)^{-1}\\
        &=\sum_{j\in A,k\in B}\frac{w_{jk}}{\text{Vol}(A)}=\frac{W(A,B)}{\text{Vol}(A)}
    \end{align*}
    The result follows from the definition of Ncut.
\end{proof}

\section{Diffusion Maps}

\dhighlight{Idea:} consider longer paths, apply $P$ a couple of times. probability of transition from $y_i$ to $y_j$
in $t$ steps is $[P^t]_{ij}$.

Further the weights in $P^t$ specify local geometry, we can capture with $P^t$ geometric features
at different scales. In other words, the markov chain shows fast and slow directions of propagation.


\begin{definition}\label{def:2.26}
    Let $P$ be the transition matrix of a random walk on a graph. The \dhighlight{diffusion distance} is defined as 
    \[\dd_t^2(y_i,y_j)=\sum_{k=1}^N \left\vert [P^t]_{ik}-[P^t]_{jk}\right\vert^2\frac{1}{\pi(y_k)}\]
\end{definition}

One interpretation of $dd_t$ is as the weighted $L_2$ distance between the two probability distributions $([P^t]_{ik})_{k=1}^N$ and $([P^t]_{jk})_{k=1}^N$.

For increasing $t$ the distributions become wider and flatter, therefore more overlap and the points become closer in the 
$\dd_t$ sense, with .... % TODO


Multiplying $\dd_t$ gives:\marginnote{While $P$ is not always symmetric, $\dd$ is!}
\begin{align*}
    \dd_t^2(y_i,y_j)&= \sum_{k=1}^N[P^t]_{ik}^2\frac{1}{\pi_k}-2\sum_{k=1}^N \left([P^t]_{ik}[P^t]_{jk}\right)\frac{1}{\pi_k}+\sum_{k=1}^N [P^t]_{jk}^2\frac{1}{\pi_k}\\
    &=\langle y_i,y_i\rangle_{\P^t\pi}-2\langle y_i,y_j\rangle_{P^t\pi}+\langle y_j,y_j\rangle_{P^t\pi}\\
    &=\sum_{k=1}^N\left([P^t]_{ik}-[P^t]_{jk}\right)\frac{1}{\pi_k}
\end{align*}

This is a weighted scalar product of a feature map that involves probabilities of sums from a point $y_i$ to any other $y_k$ in $t$ steps.

Consider the weighted graph $[Y,E]$, where the weights are given by a kernel 
$k(x,y)$\marginnote{The original choice is the gaussian kernel!}
that is non-negative. 

We define $d(y_j)\coloneqq \sum_{k=1}^N k(y_j,y_k)$ and observe 
\[p(y_j,y_k)=\frac{k(y_j,y_k)}{d(y_i)}.\]
Then $p(y_j,y_k)\geq 0, P$ is not symmetric, but $\sum_{k=1}^N p(y_j,y_k)=1$. Now 
\begin{align*}
    (\star\star)\ \tilde{p}(y_j,y_k)=p(y_i,y_k)\sqrt{\frac{d(y_j)}{d(y_k)}}=\frac{k(y_j,y_k)}{\sqrt{d(y_i)}\sqrt{d(y_k)}}
\end{align*}

$\tilde{p}$ is symmetric and we can obtain a spectral decomposition of it:
\[\tilde{p}(y_j,y_k)=\sum_{i=1}^n\lambda_i\phi_i(y_j)\phi_i(y_k)\]
and from spectral graph theory one can obtain
\[\lambda_0=1\geq \lambda_1\geq \lambda_{N_1}.\]
Furthermore $\phi_0=\sqrt{\pi}$, where $\pi(y_i)=\frac{d(y_i)}{\sum_{j=1}^N d(y_j)}$
is the stationary distribution of the associated random walk.

With $(\star\star)$ we get:
\begin{align*}
    p(y_j,y_k)=\sum_{k=1}^N\lambda_i\psi_i(y_j)\chi_i(y_k)
\end{align*}
where 
\[\phi_i(y_j)=\frac{\phi_i(y_j)}{\pi(y_j)}\]
and 
\[\chi_i(y_k)=\phi_i(y_k)\sqrt{\pi(y_k)}.\]
In particular $\psi_0=1$. For repeated applications of $p$, we obtain, due to the orthogonality of the eigendecomposition:
\[(\star)\ P_t(y_j,y_k)=\sum_{i=0}^{N-1}\lambda_i^t\underbrace{\psi_i(y_j)}_{\text{Coefficient}}\chi_i(y_k)\]

\begin{definition}\label{def:2.27a}
    We define the \dhighlight{family of diffusion maps} $\{\Psi_t^s\}_{t\in\N,1\leq s\leq N_1}$ by 
    \[\Psi_t^s(y_k)=\begin{bmatrix}
        \lambda_1^t\psi_1(y_k)\\
        \lambda_2^t\psi_2(y_k)\\
        \vdots\\
        \lambda_s^t\psi_s(y_k)
    \end{bmatrix}\]
    where $\lambda_i,\psi_i$ are from $(\star)$. We also set $\Psi_t=\Psi_t^{N-1}$. Each component 
    $\lambda_i^t\psi_i$ is called a \dhighlight{diffusion coordinate}.
\end{definition}

\begin{aremark}
    For higher $t$ we need less eigenfunctions for a good representation, since $\lambda_i\leq 1$!
    There is a coarsening effect for larger $t$.
\end{aremark}

\beginlecture{22}{04.07.24}

\begin{equation}\label{eq:diffmap}
    P_t(y_k,y_k)=\sum_{i=0}^{N-1}\lambda_i^t\psi_i(y_j)\chi_i(y_k)
\end{equation}

\begin{*definition}\label{def:2.27b}\marginnote{Here again $\psi_i$ are vectors, $\psi_i(y_k)=(\psi_i)_k$ is the $k$th entry}
    Let $P$ be the transition map of a random walk on a graph $[V,E]$ and $\{\psi_i\}_{i=0}^{N-1}$ be the 
    eigenvector of $P$ with eigenvalues $1=\lambda_0\geq\lambda_1\geq \dots\geq\lambda_{N-1}$. The $\psi$ are orthonormal w.r.t.
    \[\langle \psi_i,\psi_j\rangle_{\pi}= \sum_{k=1}^N\psi_i(y_k)\psi_j(y_k)\pi(y_k)\]
    \dots
\end{*definition}

\begin{theorem}\label{thm:2.28}
    The diffusion distance $\dd_t$ is equal to th euclidean distance in the diffusion map space:
    \begin{align*}
        \dd_t^2(y_j,y_l)&=\Vert \Psi_t(y_j)-\Psi_t(y_l)\Vert^2\\
        &=\sum_{i=1}^{N-1}\lambda_i^{2t}\left(\psi_i(y_j)-\psi_i(y_l)\right)^2
    \end{align*}    
\end{theorem}

\begin{proof}
    Inserting \ref{eq:diffmap} into $\dd_t$ gives 
    \begin{align*}
        \dd_t^2(y_j,y_l)&=\sum_{k=1}^{N}\underbrace{\left(\sum_{i=1}^{N-1}\lambda_i^t(\psi_i(y_j)-\psi_i(y_l))\chi_i(y_l)\right)^2}_{\text{independent of }k}\frac{1}{\pi(y_k)}
    \end{align*}
    Multiplying out and observe that 
    \[\langle\chi_i,\chi_k\rangle_{\frac{1}{\pi}}=\delta_{jk}\]
    gives 
    \begin{align*}
        \dd_t^2(y_j,y_l)&=\sum_{k=1}^{N-1}\lambda_i^{2t}\left(\psi_i(y_j)-\psi_i(y_l)\right)^2\qedhere
    \end{align*}
\end{proof}

Besides the underlying manifold view, one cane see the data as 
samples from the \dhighlight{equilibrium distribution} of stochastic dynamical systems.
\begin{enumerate}
    \item Manifold view $\to $ recover the manifold structure regardless of the data distribution
    \item Stochastic dynamical system view $\to$ density of points is the quantity of interest
\end{enumerate}

To generalize diffusion maps one can add a density normalization step:
1.: with
\[q_k(y_i)=\sum_{l=1}^{N}k(y_i,y_l)\]
leads to the density normalization
\[K^{(\alpha)}(x,y)=\frac{K(x,y)}{q_k^{\alpha}(x)q_k^{\alpha}(y)}.\]
2.: set $d^{(\alpha)}(y_i)=\sum_{l=1}^N k^{(\alpha)}(y_i,y_l)$
\[P^{(\alpha)}(y_i,y_j)=\frac{K^{(\alpha)}(x,y)}{d^{(\alpha)}(y_i)}\]

\begin{remark}\marginnote{$\alpha=1$ is a reasonable choice, it captures the strucutre of the manifold}
    One can show that $\alpha$ corresponds to a specific type of flow field on the submanifold the data lies on:
    \begin{itemize}
        \item $\alpha=0$: flow field follows the normalized graph laplacian for gaussian kernel 
        \item $\alpha=1$: leads to a finite sample approximation of Brownian motion random process, flow fields follows the Laplace (-Beltrami) operator on the manifold 
        \item $\alpha=\frac{1}{2}$: any $\alpha$ between $0$ and $1$ introduces a drift term in addition to the Brownian motion. $\alpha=\frac{1}{2}$ reflects the 
              Folker-Plank dynamics of the random walk   
    \end{itemize}
\end{remark}

\begin{aremark}
    A bit more detail and information can be found in his script, since we are running low on time!    
\end{aremark}

\section{t-Stochastic Neighborhood Embedding (t-SNE)}

\begin{itemize}
    \item two $d$ curved manifold
    \item ten $d$ curved manifold 
\end{itemize}

It is problematic to use pairwise distances in two dimensions to approximate pairwise distances in ten dimensions!

\begin{itemize}
    \item in ten dimensions it is possible to have $11$ points that are mutually equidistant 
    \item the volume of a sphere centered on a point $y$ scales as $r^d$
\end{itemize}

t-SNE minimizes the (KL) divergence between two distributions. One measures pairwise similarities in the input 
objects, the other in the embedding of the corresponding points. 

Given $\{y_i\}_{i=1}^N$ and a distance function $d(y_i,y_j)$, commonly euclidean distance. We use 
\[p(y_j\mid y_i)=\frac{\exp(\frac{-d(y_i,y_j)^2}{2\sigma_i^2})}{\sum_{k\neq i}}\exp(-d(y_i,y_k)^2/(2\sigma_i^2))\]
$y_j$ would get picked as a neighbor of $y_i$ in proportion to a Gaussian probability density with variance $\sigma_i$

Further, $p(y_i\mid y_i)=0$. To obtain symmetry we use define $p_{ij}=\frac{p(y_j\mid y_i)+p(y_i\mid y_j)}{2N}$.

The probability in the embedding space is modelled by a Student-t distribution (which will be a Cauchy distribution).
\[q_{ij}=\frac{(1+\Vert x_i-x_j\Vert^2)^{-1}}{\sum_{k}\sum_{k\neq l}(1+\Vert x_i-x_j\Vert^2)^{-1}}\]

t-SNE computes $\{x_1,\dots,x_n\}$ that minimize the \dhighlight{Kullback-Leibner-divergence}\footnote{or \dhighlight{relative entropy}} between distributions $p$ for $Y$ and $q$ for $X$.
\[C=\KL(P\mid Q)=\sum_{i}\sum_{j}p_{ij}\log\frac{p_{ij}}{q_{ij}}\]
the gradient of the KL-divergence between $p$ and $q$ is 
\begin{align*}
    \frac{\partial C}{\partial x_i}=4\sum_{j}(p_{ij}-q_{ij}) (x_i-x_j)(1+\Vert x_i-x_j\Vert^2)^{-1}
\end{align*}

\begin{aremark}
    Interpreting t-SNE is hard, distances between clusters might not be meaningful! We will use quadtrees to organize the data. We also use the Barnes-Hut approximation.
\end{aremark}

\begin{aremark}
    U-map and t-SNE are both used in practice!
\end{aremark}

\beginlecture{23}{09.07.24}

I was ill, no notes. Content: Finishing t-SNE and start of 

\section{Autoencoders}


\beginlecture{24}{11.07.24}

\subsection{Variational Autoencoder}

In \dhighlight{VAE} we use a probabilistic model $p(Y\mid X=x)$. \marginnote{$X,Y$ are random variables, $\overline{X},\overline{Y}$ are in the dataset}
and aim to approximate it. We generate a generative decoder by \begin{itemize}
    \item  modelling the latent space distribution
    \item approximating the conditionals $p(Y\mid X)$ and $p(X\mid Y)$
\end{itemize}

In the latent space we assume 
\[P(X)=\mathcal{N}(0,I).\]
The decoder density is modelled as \marginnote{He does not really care about $X$ vs $x$ \dots}
\[P(Y\mid X=x)=\mathcal{N}(f(x),\sigma^2I)\]
where $f$ is a \dhighlight{deep neural network}.
\[f:\R^q\to\R^d\]
As the optimization criteria we use maximum likelihood of obtaining $\overline{Y}=\{y_1,\dots,y_N\}$
\begin{align*}
    \log(P(\overline{Y}))&=\log(P(Y=\overline{Y}))\\
    &\stackrel{\text{$y_i$ independent realizations of $Y$}}{=}\log\left(\prod_{i=1}^N p(Y=y_i)\right)\\
    &=\sum_{i=1}^N\log\left(p(Y=y_i)\right)\\
    &=\sum_{i=1}^N\log\left(\underbrace{\int_{\R^q} p(y_i\mid X=x)p(x)dx}_{=\bE_p\left(P(y\mid X=x)\right)}\right)
\end{align*}

To compute the integral we could use a Monte Carlo estimator, bu that is slow to converge.

\begin{enumerate}
    \item Compute $x_j\sim P(x)$, $\frac{1}{N}\sum_{j=1}^N\sum_{i=1}^N \log p(y_i\mid X=X_j)$
\end{enumerate}

We can be seen to use \dhighlight{important sampling}, where the $x_i$ are drawn, where $P(y_i\mid X=X_j)$ is large.\marginnote{I think it should be $P(y_j\mid X=x_j)$ here \dots}
We construct $\nu(X\mid Y=y)$ to be able to sample those $x$ that are likely to produce $y$.

So instead we compute a sampling estimate of 
\[\int_{\R^q}P(Y\mid X=x)\nu(x\mid Y=y)dx=\bE_\nu(P(Y\mid X=x))\]
Using the Kullback-Leibner divergence between $\nu$ and the true probability distribution $p(x\mid Y=y)$
\begin{align*}
    K_{\nu,P}=\text{KL}(\nu(x\mid Y=y)\Vert P(x\mid Y=y))=\mathbb{E}_\nu\left(\log(\nu(x\mid Y=y))-\log(P(x\mid Y=y))\right)
\end{align*}

Using Bayes  theorem we see 
\begin{align*}
    P(x\mid Y=y)=\frac{P(y\mid X=x)P(X=x)}{P(Y=y)}
\end{align*}

\begin{align*}
    K_{\nu,P}&=\mathbb{E}_\nu\left(\log(\nu(x\mid Y=y))-\log\left(\frac{P(y\mid X=x)P(X=x)}{P(Y=y)}\right)\right)\\
    &=\mathbb{E}_\nu\left(\log(\nu(x\mid Y=y))-\log\left(P(y\mid X=x)P(X=x)\right)\right)+\underbrace{\log\left(P(Y=y)\right)}_{\text{Constant}}\\
\end{align*}

We can rearrange this:
\begin{align*}
    \underbrace{\log\left(P(Y=y)\right)}_{\text{log-evidence of model}}-\underbrace{K_{\nu,P}}_{\geq 0}=\underbrace{\mathbb{E}_{\nu}\left(\log(P(y\mid X=x))\right)-\text{KL}\left(\nu(x\mid Y=y)\Vert p(X=x)\right)}_{\text{Evidence lower bound (ElBo)}}
\end{align*}

This was the goal to optimize.

If $\nu(x\mid Y=y)$ is a good approximation for of $p(x\mid Y=y)$, $K_{\nu,P}$ will be small.

If we are maximizing the sum of the ElBos for each $y_i$ we are maximizing $\log(P(\overline{Y}))$.

Maximizing ElBo achieves
\begin{itemize}
    \item maximizing the log-likelihood of $\overline{Y}$ and therefore the generative model $p(Y\mid X)$ becomes better 
    \item minimizing the KL-divergence between $\nu$ and the posterior $P$ and therefore the so called \dhighlight{inference model} $\nu$ becomes better
\end{itemize}

\begin{align*}
    \nu(x\mid Y=y)\sim\mathcal{N}(g_1(y),\exp(g_2(y))^\intercal I)
\end{align*}
\marginnote{Here $\exp(g_2(y))$ is meant componentwise which than is taken as the diagonal?}
with $g_1,g_2:\R^d\to\R^q$ two DNNs encoding the mean and variance of $\nu$,
where $\exp(g_2(y))$ is meant componentwise (on the diagonal). Usually $g_1,g_2$ have the same network architecture.

\begin{align*}
    (\tilde{f},\tilde{g_1},\tilde{g_2})=\argmin_{f,g_1,g_2}\sum_{i=1}^N \underbrace{\mathbb{E}_\nu\left[\log(P(y_i\mid X=x))\right]-\text{KL}\left(\nu(x\mid Y=y_i)\Vert P(X=x)\right)}_{L_i(f,g_1,g_2)}
\end{align*}

\begin{remark}
    The $\sigma$ from $P(Y\mid X=x)\sim\mathcal{N}(f(x),\sigma^2I)$ can be interpreted as a regularization parameter.
    It changes $P(y\mid X=x)$ without affecting the $\text{KL}(\nu\Vert p(X))$, 
    it balances the two ElBo terms. $\sigma$ reflects how accurately we expect the model to reconstruct 
    $Y$. 

\end{remark}

One can see \begin{align*}
    \text{KL}(\nu(x\mid Y=y)\Vert p(X=x))&=\frac{1}{2}\left(-\sum_{i=1}^q(g_2(y))_i-q+\sum_{i=1}^q \exp(g_2(y))_i+\sum_{i=1}^q(g_1(y))_i^2\right)
\end{align*}
The derivative can be computed by automatic differentiation. For the first term we could 
perform a MC-estimator, with slow convergence. Instead we draw a sample $X_i\sim\nu(X\mid Y=y_i)$ for each $y_i$ in the minibatch
and use the \highlight{one-shot estimator} $\log(P(y_i\mid X=x))$ as an unbiased estimator. We then just average 
over all samples in the minibatch to evaluate the loss. Essentially $\nabla_{g_1,g_2}\bE_\nu(\dots)\neq \bE_\nu(\nabla_{g_1,g_2}\dots)$.

We can use the \dhighlight{reparametrization trick}:

We draw $x(Y)$ according to $\nu(X\mid Y=y)$ and 
parametrize via 
\[x(Y)=g_1(y)+\sqrt{\exp(g_2(y))}IZ\]
with $Z\sim\mathcal{N}(0,1)$.\marginnote{Here we decouple our drawing process from taking the derivative}
Then \begin{align*}
    \nabla_{g_1,g_2}\mathbb{E}_Z(\log(P(y\mid X=x))) &=\nabla_{g_1,g_2}\mathbb{E}_\nu(\log(y\mid X=g_1(y)+\sqrt{\exp(g_2(y))}IZ))\\
    &=\bE_Z(\nabla_{g_1g_2}\log(P(y\mid X=x))) \\
    &=\nabla_{g_1,g_2}\mathbb{E}_Z(\log(y\mid X=g_1(y)+\sqrt{\exp(g_2(y))}IZ))
\end{align*}

\beginlecture{25}{16.07.24}

We define for $B\subseteq\{1,\dots,N\}$ the \dhighlight{minibatch loss} on $B$
\[L_B(f,g_1,g_2)=\frac{1}{|B|}\sum_{i=1}^{|B|}L_{B_i}(f,g_1,g_2)\]

\begin{algorithm}[H] % TODO: FIX BOTH
    \caption{Stochastic minibatch gradient descent}\label{alg:sbgd}
 \textbf{Input:}  $Y$, learning rate $\eta>0$, minibatch size $\kappa\leq N$, number of epochs $S\in \N$\\
 \textbf{Output:} Updated weights and biases
 \begin{algorithmic}
    \State Initialize all weights $W$ and biases $b$
    \For {$s=1,\dots,S$}
        \State subdivide $\{1,\dots,N\}$ into $\lceil \frac{N}{\kappa}\rceil$ disjoint subsets $B_i$
        \For {$B_{1},\dots,B_{\lceil \frac{N}{\kappa}\rceil}$}
        \State calculate $f,g_1,g_2$ for all $i\in B$\marginnote{forward propagation}
        \State calculate $\nabla_{W,b} L_B(f,g_1,g_2)$\marginnote{w.r.t. all weights and biases}
        \State update each weight $w_{i,j}^{(l)}\leftarrow w_{i,j}^{(l)}-\eta\frac{\partial}{\partial w_{i,j}^{(l)}}L_B(f,g_1,g_2)$
        \State update each bias $b_{j}^{(l)}\leftarrow b_{j}^{(l)}-\eta\frac{\partial}{\partial b_{j}^{(l)}}L_B(f,g_1,g_2)$
        \EndFor
    \EndFor
 \end{algorithmic}
\end{algorithm}

One can also 
\begin{itemize}
    \item use momentum or Nestorov update
    \item adapt step size / learning rate
    \item use ADAM adaptive moment estimation
\end{itemize}

\begin{aremark}
    Definition, theorems, connections! Also kernels in both topics, there is a connection! Can you tell me something about? 30min +x 

    Bring student ID and government ID. Questions are not always precise. Don't talk to long about one topic. How quick do we come to the point. He won't ask questions on the exercises, but they can help.

\end{aremark}

\section{Generative Diffusion Models}

\begin{definition}\label{def:.29}
    A time-discrete \dhighlight{forward diffusion process} starting at a data point $y^{(i)}\in\R^d$, which 
    has been drawn according to some unknown distribution $p$, is defined by 
    \[y^{(i+1)}=\sqrt{1-\beta_{i+1}}+\sqrt{\beta_{i+1}}+\epsilon\ (\star)\]
    for each $i\in W$. Here $0<\beta_i<1$ is called the noise level and $\epsilon\sim\mathcal{N}(0,I)$, $\epsilon\in\R^d$.
\end{definition}
For $i\to \infty$, the $y^{i+1}$ become completely random, normally distributed vectors. Consider the stochastic differential equations (SDE) in the 
form of 
\[\frac{d Y(t)}{dt}=\mu(y(t),t)+\sigma((y(t),t))\frac{dW(t)}{dt}\ (\star\star\star)\]
with $Y,W:[0,\infty)\to\R^d$ are stochastic processes. The drift coefficient $\mu:\R^d\times [0,\infty)\to\R^d$ and 
the diffusion coefficient $\sigma:\R^d\times [0,\infty]\to\R$. $W$ is modelled as a multidimensional Brownian Motion / Wiener process,
where $W(\tilde{t})-W(t)$ are independent and modelled as $\mathcal{N}(0,(\tilde{t}-t)I)$.

In particular, \[\frac{dY(t)}{dt}=-\frac{1}{2}\beta(t)Y(t)+\sqrt{\beta(t)}\frac{dW(t)}{dt}\ (\star\star\star\star)\]
with $\beta(t):[0,\infty)\to\R$ noise variance. 

We use for discretization the Euler-Maruyama-scheme with step size $\delta t$:
\[Y(t+\delta_t)=Y(t)-\frac{1}{2}\beta (t)Y(t)\delta t+\sqrt{\beta(t)}\sqrt{\delta t}\epsilon \ (\star\star)\]
Consider $(\star)$ with $y^{(i)}=Y(i\delta t)$ and $\beta_i=\beta((i+1)\delta t)\delta_t$
\[Y((i+1)\delta t)=\sqrt{1-\beta(i\delta t)}Y(i\delta t)+\sqrt{\beta(i\delta t)}\epsilon\]

Use Taylor expansion on the drift part around $\delta t=0$: 
\begin{align*}
    \sqrt{1-\beta(i\delta t)\delta t}=1-\frac{\beta(i\delta t)}{2}\delta t+ O((\delta t)^2)
\end{align*}

So up to a second order term in $\delta t$, $(\star\star)$ resembles $\star$.

\subsection{Reversed diffusion process}
To draw according to the data distribution, we need the reverse diffusion process $\overline{Y}$. It transforms $Y(t)$ for $t$ very large, back.

Generally, it can be shown that a forward SDE $(\star\star\star)$ can be reversed in time to get the reverse time SDE. In case of 
$(\star\star\star)$ one gets \[\frac{d\overline{Y}}{d t} = \frac{1}{2}\beta(t)(\overline{Y}(t)+\underbrace{g(\overline{Y}(t),t)}_{\text{Score function}})+\sqrt{\beta(t)}\frac{d\overline{W}(t)}{dt}\]
for some $g:\R^d\times [0,\infty)\to\R^d$. Euler-Maruyama gives 
\[\overline{Y}(t-\delta t)=\overline{Y}(t)-\frac{1}{2}\beta(t)(\overline{Y}(t)+g(\overline{Y}(t),t))+\sqrt{\beta(t)\delta t}\epsilon\]
Observations: So in both directions we compute terms depending on the current iterate and a Gaussian increment.

\begin{align*}
    P^\star(y^{(i)}\mid y^{i-1})&=\mathcal{N}(\sqrt{1-\beta_i}y^{(i-1)},\beta_i I)\\
    P^\star(y^{i-1}\mid y^{(i)},y^{(0)})&=\mathcal{N}(\tilde{m}_i,\tilde{\sigma}_iI)
\end{align*}
where $\tilde{m}_i,\tilde{\sigma}_i$ only depend on $y^{(i)},y^{0}$ and $\beta_j,1\leq j\leq i$.

But $y^{(0)}$ is not known when reversing the process for new images, so we aim to approximate this by 
\begin{align*}
    P(y^{(i-1)}\mid y^{(i)})=\mathcal{N}(m_i,\sigma_iI) 
\end{align*}
with $m_i,\sigma_i$ can be learned or set. The noise level is typically fixed $\sigma_i=\beta_i$ and $m_i$ is the output of a 
neural network given $y^{(i)}$, learned from training data $Y_1,\dots,Y_N$.

\beginlecture{26}{18.07.24}

As for VAE
\[\mathbb{E}_{P^\star(Y(0))}[P(Y(0))]\]
This cannot directly be computed, so we use the ElBo 
\begin{align*}
    \log P(Y(0))&=\log\left(\int P^\star(y(1:T)\mid y(0))\frac{P(y(0:T))}{P^\star(y(1:T)\mid y(0))}dy(1:T)\right)\\
    y(1:T)&=(y(1),\dots,y(T))
\end{align*}
Now with $p(y(0:T))=\underbrace{P^\star(y(T))}_{\mathcal{N}(0,I)}\prod_{i=1}^{T}P(Y(i-1)\mid Y(i))$
and $T$ the number of steps of the forward process, $T$ large enough. One can see 
\begin{align*}
\log(Y(0))&\geq \int P^\star(Y(1:T)\mid Y(0))\log(P^\star(Y(T)))\prod_{i=1}^{T}\frac{P(Y(i-1)\mid Y(i))}{P^\star(Y(i)\mid Y(i-1))}\\
&=\mathbb{E}_{P^\star}\left[\log P^\star (Y(T))+\sum_{i=1}^T\log\frac{P(Y(i-1)\mid Y(i))}{P^\star(Y(i)\mid Y(i-1))}\right]
\end{align*}

Given training data, we can approximate this empirically 
\begin{align*}
    \approx \frac{1}{N}\sum_{i=1}^N\left(\log(P^\star(Y_j(T)))+\sum \log\frac{P(Y_j(i-1)\mid Y_j(i))}{P^\star(Y_j(i)\mid Y_j(i-1))}\right)
\end{align*}
where $P^*(Y_j(i)\mid Y_j(i-1))$ is known and wen can optain 
\[y_j(i)\]
by sampling from $P^\star(Y_j(i) \mid y_j(i-1))$. Therefore we can evaluate this expression for each 
$y_j=y_j(0)$.

We assume that $T$ is large enough, s.t. $P^\star(Y_j(T))\sim\mathcal{N}(0,I)$. Together, we can evaluate the expression 
and minimize it with a SDG-type algorithm w.r.t. weights and biases of the network for $m_i$.

With the learned $w_i$, we then propagate a random sample $Y(T)\sim\mathcal{N}(0,I)$ by computing the 
$m_i$ and following the backwards process  for $i=T_1,\dots,1$ and get the new data $y(0)$ approximately
from the image distribution.

\chapter{Misc}

\section{What we didn't cover}

For dimensionality reduction: There is further probabilistic views on that! We can also use an embedding, which more strongly reflects our data classes.
We might want to include this information in our dim. reduction.

From the stochastic view (think PCA, we  assume linear map). But we can just think of random matrices, there is the Johnson-Lindenstrauss lemma.

In view of NNs we can also talk about restricted Boltzmann machines. 

For the kernel stuff: We didn't do sampling inequalities! Idea of oversampling. We can also do Wendler-Kernels\marginnote{Compactly supported kernels $\to$ sparse matrices}?

\section{Review of the lecture notes}

\textbf{Kernels}

\begin{itemize}
    \item Kernels, applications 
    \item Feature map / mercer kernel view , kernels as generalizations of scalar products
    \item Collection of properties, how do we do computations with kernels 
    \item Riesz-Rep / Functional analysis view 
    \item Kernels give a way to represent functions / reproducing kernel hilber spaces
    \item Theorem 1.9 semi is missing 
    \item Subspaces and connections to mercer kernels 
    \item Function approximation through projection onto subspaces 
    \item Best approximation / minimizing norm (Hilbert projection theorem)
    \item Power function and error bounds 
    \item Power function in terms of the kernel 
    \item Power function and condition of kernel matrix (eigenvalues, smallest eigenvalue and power functions), relation to fill distance (balancing act)
    \item Polynomial reproduction / cone condition, a bit technical
    \item Seite 36: Beweisidee Taylorexpanssion wichtig
    \item Hermite interpolation 
    \item Connection to PDEs
    \item cpsd $\to$ later kernel PCA (for order 1)
    \item Loss functions (as a relaxation of interpolation)
    \item Some loss functions
    \item Expected and empirical risk 
    \item Regularization (also relates to power functions and fill distances)
    \item representer theorem and regularization operators
    \item Greens kernel (vllt. nicht so wichtig?)
    \item eigendecomposition and kernels $\to$ mercer kernel
    \item Gaussian process 
    \item SVMs (key algorithm) (connection to hinge loss?)
    \item 1.55 distances and cpsd of order 1   
\end{itemize}


\textbf{dimensionality reduction}

\begin{itemize}
    \item Distances, kernels connections
    \item Work with data PCA / MDS / SVD
    \item Several ways to view / derive PDA 
    \item Key thing: MDS is the same as PCA just slightly different. Similarities ... 
    \item Theorem 2.6 important to connect scalar products to distances 
    \item 3 different ways to compute the embedding (page 90)
    \item Problems in higher dimensions
    \item Graph distances, Iso map \dots 
    \item Under certain assumptions Iso map ... is reasonable 
    \item Parallel transport unfolding (not as important? Differential geometry)
    \item Perturbation analysis 
    \item Nonlinear PCA and Kernel MDS 
    \item Maximum variance unfolding, find a kernel ..., similar to iso map, also works with graph distances
    \item Spectral clustering, diffusion processes and random walks on graphs, cuts ...
    \item K-means 
    \item diffusion maps, non symmetric matric $\to$ construct symmetric matrix
    \item diffusion distance 
    \item density aware diffusion (only sketched)
    \item Two steps in the diffusion map algorithm 
    \item t-SNE(why can we compute this? quadtrees!)
    \item Autoencoder, VAE (ElBo)
    \item Diffusion process and SDEs 
\end{itemize}


