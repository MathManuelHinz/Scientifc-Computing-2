\begin{aremark}
    Idea: High dimensional data $Y\to$ find low dimensional data $X$ which represents $Y$ to a good degree.

    For example: Represent vectors w.r.t. to a subset of the basis!
\end{aremark}

Set $Y$ of $y_i\in\R^d$, $i=1,\dots,N$\marginnote{We are now in the unsupervised setting}
goal is to find 
\[x_i\in\R^p, p\ll d\]
$d$ is called the \dhighlight{extrinsic} dimension, while $p$ is the \dhighlight{intrinsic} dimension.

\section{Linear dimensionality reduction}

$\{y_i\}_{i=1}^N$ are samples of a random variable $Y(\omega)\in\R^d$\marginnote{He writes $Y\in\R^d$, which seems to be the wrong space for a random variable \dots}.
We assume $Y$ stems from $p$ unknown latent variables $X(\omega)\in\R^p$ by a linear transformation $W$.
\[Y=WX\]
which can be read as a statement about vectors (a single realization of the random variable) or in terms of matrices (the sampled realizations as a whole)-

We assume that $Y,X$ are mean centered, i.e. $\bE(Y)=0$. We further assume 
$W$ to be an axis change, i.e. the columns $w_i$ of $W$ are orthogonal to each other and unit norm:
\[W^\intercal W = I_p\]
We write the data in matrix form\marginnote{with slightly abusive notation, as the matrix $Y$ consists of $N$ samples of the random variable $Y$} 
\[Y=\begin{bmatrix}
    \vert &&\vert\\
    y_1 & \dots & y_N\\
    \vert &&\vert
\end{bmatrix}\]

For any $W$, consider the pseudo inverse 
\[W^\dagger = (W^\intercal W)^{-1}W^\intercal=W^\intercal\]
and therefore 
\[X_i=W^T y_i.\]
We aim for a good reconstruction of $Y$ by $X$:
\begin{align*}
    &\bE(\Vert y-W(W^T y)\Vert_2^2)\\
    &=\bE(y^\intercal y-2y^\intercal W W^\intercal y+y^\intercal W W^\intercal W W^\intercal y)\\
    &=\bE(y^\intercal y-y^\intercal WW^\intercal y)
\end{align*}
where 
\begin{align*}
    \bE(Y^\intercal WW^\intercal \underbrace{Y}_{\in\Omega\times\R^d})&\approx\frac{1}{N}\sum_{i=1}^N y_i^\intercal W W^\intercal y_i\\
    &=\frac{1}{N}\text{tr}(Y^\intercal W W^\intercal \underbrace{Y}_{\in \R^{d\times N}})\\
    &=\frac{1}{N}\text{tr}(W^\intercal YY^\intercal W)
\end{align*}

Adding the constraint, we set the lagrangian 
\[\mathcal{L}=\text{tr}(W^\intercal YY^\intercal W)+\tr((I_p-W^\intercal W)\Lambda)\]
where $\Lambda=\Lambda^T\in\R^{p\times p}$.

Conditions for an extrema:
\begin{align*}
    (\star):YY^\intercal W = W\Lambda\implies \Lambda=W^\intercal YY^\intercal W
\end{align*}
The objective reduces to $\tr(\Lambda)$. We can rotate $W$ and have the same \dhighlight{reconstruction error},
i.e. we can use $W'=WR$ giving $\Lambda'=R\Lambda R^T$ for some rotation matrix $R$.
$\implies \Lambda=\Lambda^T$ is diagonalizable with orthogonal matrices, so we choose $R$ s.t. 
$\Lambda'$ is diagonal, w.l.o.g. $\Lambda$ is diagonal.

From $(\star)$ follows that the columns of $W$ must be eigenvectors of $YY^\intercal$ with 
corresponding eigenvalues as the diagonal of $\Lambda$. Since we maximize $\tr(\Lambda)$, we 
take the $p$ largest eigenvalues of $YY^\intercal$ and the corresponding eigenvectors.

SVD of $Y$: $U\Sigma V^\intercal$, we take the first $p$ columns of $U$ for $W$, i.e. $W=UI_{d\times p}$.
Furthermore, as $U$ is orthogonal,
\[X=W^\intercal Y=W^\intercal U \Sigma V^\intercal=I_{p\times d}\Sigma V^\intercal.\]
Using $\Lambda=W^\intercal U\Sigma^2 U W$, we get 
\[\tr(\Sigma^2)-\tr(I_{p\times d}\Sigma^2)=\sum_{i=p+1}^d \sigma_i^2=\sum_{i=p+1}^d \lambda_i\]

\begin{theorem}\label{thm:2.1}
    Let $Y=[y_1,\dots,y_N]\in\R^{d\times N}$ be a matrix of zero mean data points. Denote the SVD of 
    $Y$ by $Y=U\Sigma V^\intercal$. Then for given $p<d$, the minimizer $W$ for 
    \[\min_{\stackrel{W}{W^\intercal W=I_p}}\sum_{i=1}^N\Vert y_i-WW^\intercal y_i\Vert_2^2\]
    is given using $W=[u_1,\dots,u_p]$. The lower dimensional embedding is given by 
    $X=I_{p\times d}\Sigma V^\intercal=I_{p\times d}U^\intercal Y$ and the \dhighlight{reconstruction error} 
    is \[\sum_{i=p+1}^N\sigma_i^2\]
\end{theorem}

\beginlecture{15}{11.06.24}

To get compactness: Take a\dhighlight{ball of functions: $\Vert f\Vert<=r$} and use ArzelÃ -Ascoli. % TOFIX


\subsection{Alternative derivations of PCA}

\begin{itemize}
    \item Projection: We aim for $y=\sum_{i=1}^p x_i w_i$ with $y_i,w_i\in\R^d, w_i^\intercal,w_j=\delta_{ij}$. Then $x_i=\langle y_i,w_i\rangle$.
    \item Approximation with rank constraints:\[\min_{A}\Vert Y-A\Vert_{F}^2\] s.t. rank $A=p$
    \item From the statistical perspective: 
\end{itemize}

\begin{definition}\label{def:2.2}
    Given a zero mean multivariate random variable $Y\in\Omega\times \R^d$. The $p$ \dhighlight{principle components} of $Y$
    are defined as the $p$ uncorrelated linear components of $Y$:\[x_i=w_i^\intercal y\in\R,\text{ } w_i\in\R^d,\text{ }i=1,\dots,p\] s.t. the variance of $x_i$ maximized subject to $w_i^\intercal w_i=1$  and $\Var(x_1)\geq \Var(x_2)\geq\dots\geq \Var(x_p)$
\end{definition}

\begin{theorem}\label{thm:2.3}\marginnote{In the exercise we show that the greedy approach works in this specific setting?}
    Assume that the rank of the covariance matrix $\bE(YY^\intercal)$ is larger than $p$. Then the first 
    $p$ principle components of a zero mean, multivariate random variable $Y$, denoted by $x_i,i=1,\dots,p$ are given by 
    \[x_i=w_i^\intercal Y\]
    where $\{w_i\}_{i=1}^p$ are the $p$ orthonormal eigenvectors of $\bE(YY^\intercal)$ associated 
    its $p$ largest eigenvalues $\{\lambda_i\}_{i=1}^p$. Moreover $\lambda_i=\Var(X_i)$.
\end{theorem}

\begin{aremark}
    $Y$, which has $N$ columns and $d$ rows ... % BILD 
\end{aremark}

\begin{aremark}
    In general we might want to work with tensors (for example if we have a time dependent structure). This is possible, but can be generalized 
    in multiple, non-equivalent, ways. The sum of vectors representation is not so nice, there are counter examples ... 
\end{aremark}

We aim for embeddings that approximately preserve distances. \marginnote{Here, we always talk about euclidean distances!}
\[d^d(y_1,y_2)\approx d^p(x_1,x_2)\]

\begin{definition}\label{def:2.4}
    A $N\times N$ symmetric matrix $D$ is called \dhighlight{Euclidean distance matrix (EDM)}, if there exists 
    an integer $d>0$ and a vector set $Y=\{y_1,\dots,y_N\},y_i\in\R^d$, s.t. $D_{i,j}=d_E^2(y_i,y_j)$,\marginnote{Careful! Some authors use the euclidean distance matrix with a square, and some without the square!}
    where $d_E$ is the euclidean distance. The vector set $Y$ is called the \dhighlight{configuration} of $D$. We write $D\in\text{EDM}$.
\end{definition}

\begin{definition}\label{def:2.5}
    A $N\times N$ symmetric matrix $D$ with non-negative entries $d_{i,j}$ is called \dhighlight{distance matrix}, if $d_{ii}=0$ for all $i$ and 
    \[\sqrt{d_{ij}}\leq \sqrt{d_{ik}}+\sqrt{d_{kj}}\]
    for all $i,j,k$. We write $D\in \DM$. 
\end{definition}

Obviously $\EDM\subset\DM$.

\[(\star)\text{ } d_E^2(y_i,y_j)=\underbrace{\langle y_i,y_i\rangle}_{G_{ii}}-2\underbrace{\langle y_i,y_j\rangle}_{G_{ij}}+\underbrace{\langle y_j,y_j\rangle}_{G_{jj}}\]
where $G_{ij}=\langle y_i,y_j\rangle = (Y^\intercal Y)_{ij}$.

As for PCA, we aim for mean centered data, where we use the centering matrix
\[H=I-\frac{1}{N}1_N,\]
where $1_W=1_N\cdot 1_N^\intercal$ is the matrix of all ones, where $1_N$ is the vector of all ones. %TODO: Fix 
with \[Y^c=Y-(\frac{1}{N}Y1_N)1_N^{\intercal}=YH.\]
We set the centered data. The centered Gram matrix
\[G^c=(Y^c)^\intercal Y^c=H^\intercal Y^\intercal Y H =H^\intercal G H\]

\begin{theorem}\label{thm:2.6}
    For the Euclidean distance matrix $D$ and the centered Gram matrix $G1 c$ of a data set $Y$, it holds 
    \[G^c=-\frac{1}{2}H D H\]
\end{theorem}

\begin{proof}
    Straight forward calculation from $(\star)$.
\end{proof}


\begin{lemma}\label{lem:2.7}
    Assume that the matrix $D\in\EDM$ and let $G^c=-\frac{1}{2} HDH$. If the rank of $G^c$ is $r$, then 
    there is a centered $r$ dimensional configuration $Y=\{y_1,\dots,y_N\}\in\R^r$, s.t. $d_E(y_i,y_j)=d_{ij}$.
\end{lemma}

\begin{proof}
    Since $D\in\EDM$, there exists a set $Z=\{z_1,\dots,z_n\}\in\R^d$, s.t. $d_{ij}=d_E(z_i,z_j)$,
    $G^c$ is the Gram matrix of $Z$ and therefore psd. The rank is $r$, so we have $G^c=Y^\intercal Y$ with a centered 
    data matrix (via EDM). The centered data satisfies $d_{i,j}=d_E(y_i,y_j)$. We call $r$ the \dhighlight{intrinsic configuration dimension} and 
    $Y$ is the \dhighlight{exact configuration} of $D$.
\end{proof}

\subsection{(classical) multidimensional scaling (MDS)}

Instead of exact configuration of dimension $r$, we seek a lower dimensional configuration $Y\subset\R^p$, $p<r$;
\[X=\argmin_{X\subset\R^{p\times N}}\sum_{i,j}^N |d_{ij}^2-d_E^2(x_i,x_j)|\text{ } (\star\star)\]
s.t. $X=T(Y)$, with $T$ an orthogonal projection from $\R^r$ to a $p$-dimensional space $S_p\subset\R^r$ and $Y$ is an exact configuration. 

\begin{lemma}\label{lem:2.8}
    Let $Z\subset\R^r$ be a given data set with $D_Z=[d_E^2(z_i,z_j)]_{i,j=1}^N$ and let $G_Z^c=-\frac{1}{2}HD_ZH$.
    Then \[\tr(G_Z^2)=\frac{1}{2N}\sum_{i,j=1}^N d_E^2(z_i,z_j)\]
\end{lemma}

\begin{proof}
    Write out $G_Z^2$, look at diagonal, straight forward calculation.
\end{proof}

\begin{lemma}\label{lem:2.9}
    Let $D_Z$ as before. $ZH\eqqcolon\tilde{Z}=[\tilde{z}_1,\dots,\tilde{z}_n]$. Then $\Vert \hat{Z}\Vert_F=\frac{1}{\sqrt{2N}}\Vert D_Z\Vert_F$.
\end{lemma}

\begin{proof}
    With $\Vert D_Z \Vert_F^2=\sum d_E^2(z_i,z_j)$ and $\Vert\hat{Z}\Vert_F^2=\tr(\hat{Z}^\intercal \hat{Z})=\tr(\hat{G_z^2})$. The result follows from lemma \ref{lem:2.8}.
\end{proof}

\begin{theorem}\label{thm:2.10}
    Let $Y\subset\R^r$ be an exact configuration of $D\in\EDM$. The SVD of $Y$ is given by $U\Sigma V^\intercal$. For a given $p\leq r$, let 
    $U_p=[u_1,\dots,u_p]$. Then 
    \[X=U_p^\intercal Y\] 
    is a solution of the MDS minimization problem $(\star\star)$ with an error of \[2N\sum_{i=p+1}^r\sigma_i^2.\]
\end{theorem}


\begin{proof}
    Let $S_p$ be a $p$-dim subspace of $\R^r$. Let $B$ be a $r\times p$ orthogonal matrix, whose 
    columns form an orthonormal basis of $S_p$. We have $T(y)=BB^\intercal y$ and observe 
    \[d_E(B^\intercal y_i B^\intercal y_j)=\Vert B^{\intercal}(y_i-y_j)\Vert T(y_i-y_j)=d_E(Ty_i,Ty_j).\] 
    Wih $\Vert y_i-y_j\Vert\geq \Vert T (y_i-y_j)\Vert$. We get for the objective function 
    \begin{align*}
        \sum_{i,j=1}^N d_E^2(y_i,y_j)-d_E^2(B^\intercal y_i,B^\intercal y_j)&=\sum_{i,j=1}^N \langle y_i-y_j,y_i-y_j\rangle-\langle B^\intercal (y_i-y_j),B^\intercal(y_i-y_j)\rangle\\
        \stackrel{\langle B^\intercal y,B^\intercal y\rangle=\langle B^\intercal By, B^\intercal y\rangle}{=}&\dots - 2 \langle BB^\intercal (y_i-y_j),(y_i-y_j)\rangle + \langle BB^\intercal (y_i-y_j),(y_i-y_j)\rangle\\
        &=\sum_{i,j=1}^N \vert (I-BB^\intercal)(y_i-y_j)\vert^2=\Vert D_Z\Vert_F^2
    \end{align*}
    wih $Z=(I-BB^\intercal)Y$. Using Lemma \ref{lem:2.9}, we get $\Vert D_Z\Vert_F^2=2N\Vert ZH\Vert_F^2=2N\Vert Z\Vert_F^2$, 
    since $Y$ is centered $Z$ is also centered. Therefore we have to solve 
    \[\argmin_{\stackrel{B\in\R^{r\times p}}{B^\intercal B=I_p}}\Vert Y-BB^\intercal Y\Vert_F.\]
    Again we use Schmidt-Eckart-Yanns for the SVD. THe matrix $U_p \Sigma_pV_p^\intercal$ ??? So getting $B=U_p$ gives 
    \[U_pU_p^\intercal U\Sigma V^\intercal  = U_p\Sigma_pV_p^\intercal\] and 
    $X=U_p^\intercal Y$. The error estimate follows from ??? SVD and Lemma \ref{lem:2.9}.
\end{proof}

It is important to understand the flip: You can choose the minimum of the number of dimensions and the number of data points.

\beginlecture{16}{13.06.24}

Recap:

$G^c=Y^\intercal Y=(W^\intercal X| W X)=X^\intercal W^\intercal W X = X^\intercal X$

EVD of $G^c$:\marginnote{EVD= Eigen value decomposition}
\begin{align*}
    G^c=V\Lambda V^\intercal &= (V \Lambda^{\frac{1}{2}})(\Lambda^{\frac{1}{2}} V^\intercal)\\
    &=(\Lambda^{\frac{1}{2}} V^\intercal)^\intercal (\Lambda^{\frac{1}{2}} V^\intercal)
\end{align*}

Taking the top $p$ eigenvalues gives 
\[X_{\text{MDS}}=I_{p\times N}\Lambda^{\frac{1}{2}}V^\intercal\]
Also $Y=U\Sigma V^\intercal$ for PCA:
\begin{align*}
    X_{\text{PCA}}&=I_{p\times d} U^\intercal Y = I_{p\times d} \Sigma V^\intercal\\
    &=I_{p\times d} (\Sigma^\intercal \Sigma)^{\frac{1}{2}}V^\intercal=I_{p\times d}\Lambda^{\frac{1}{2}}V^\intercal
\end{align*}



We can use the following approaches:\marginnote{The SVD is most commenly used and ist most stable. If you have extreme differences in $d$ and $N$, it might be worth to use on of the other two approaches}

\begin{itemize}
    \item SVD of $Y$: $(d\times N)$: Reconstruct $Y$
    \item EVD of $YY^\intercal$ $(d\times d)$: maximal variance 
    \item EVD of $Y^\intercal Y$ $(N\times N)$: preserving similarity 
\end{itemize}


\begin{algorithm}[H] % TODO: FIx numbering
    \caption{MDS}\label{alg:mds}
 \textbf{Input:} EDM $D,P$\\
 \textbf{Output:} embedding $X$ in $p$ dimensions
 \begin{algorithmic}
   \State $G=-\frac{1}{2} H D H$
   \State $[V_p,\Lambda_p]=\text{EVD}(G,p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V^\intercal$
 \end{algorithmic}
\end{algorithm}

\begin{aremark}
    There are multiple generalizations!
\end{aremark}

\subsection{Strange effects in high dimensions}

\subsubsection*{Angles}

First we look at angles between vectors. In 2d the angle between the diagonal and an axis is $\frac{\pi}{4}$.

In general: 
\[\varphi=\arccos\frac{\langle x,y\rangle}{\Vert x\Vert\Vert y\Vert}\]
Then $\alpha=\frac{1}{\sqrt{d}}$.

\subsubsection*{Volume}

The volume of the hypersphere in $d$ dimensions:
\[\frac{r^d\pi^{\frac{d}{2}}}{\Gamma(1+\frac{d}{2})}\]
The Volume of the hypercube is $(2r)^d$.

Therefore in higher dimensions the sizes of a unit hypersphere and its bounding hypercube are not close, their ratio goes to $0$!
\subsubsection*{Concentration of measure}
If we increase the radius of the sphere by just some small $\epsilon$, we can see that most of the mass 
lies in the outer shell of hyperspheres!
\[\frac{V_{r} }{V_{r(1-\epsilon)}}=\frac{1}{(1-\epsilon)^d}\]
\subsubsection*{Curse of dimensionality}\marginnote{If we do datascience in higher dimensions, we always implicitly assume that there is more structure, otherwise most problems would not be feasable}
The higher the dimension, the more points I need to approximate functions of the same complexity as the dimension increases. This scaling is exponential!
Therefore we need lots of data to recover functions in higher dimensions.
\subsubsection*{ANOVA decomposition}
However the dimensions might be highly correlated! We can than use the ANOVA decomposition:
\[f(x)=\sum_{\tilde{d}_i={1,\dots,D}} f_{\tilde{d}_i}(x_{\tilde{1}},\dots,x_{\tilde{p}})\]

\subsection{Properties in dimensionality reduction approaches}

\begin{enumerate}
    \item Estimate the intrinsic dimensionality
    \item What kind of properties of the original data does one (implicitly or explicitly) assume to hold and to approximately preserve
    \item One often aims for latent variable separation (statistical independence, orthogonality)\footnote{this is also useful for interpretability!}
\end{enumerate}

For PCA/MDS, we can consider the rank of the matrix as the intrinsic dimensionality.\marginnote{This assumes no noise, otherwise we can be reasonably sure that the intrinsic dimensionality is smaller than the rank}

We can use criteria such as 
\[\frac{\sum_{i=1}^p\lambda_i}{\sum_{i=1}^d\lambda_i}\geq 0.95\]
or 
\[\lambda_{p+1} \leq 0.01\sum_{i=1}^d \lambda_i\]

Another procedure: The $L$-curve: Linearly approximate the eigenvalues as a function of the index from both the left and the right and take the intersection as the cutoff point.

In the latter approaches we assume a fast decline in eigenvalues.

\section{nonlinear dimensionality reduction}

We use manifolds instead of linear subspaces. Ideally we use geodesic distances $d_M(x,y)$ and 
preserve $d_M$ in the embedding. But both $M$ and $d_M$ are unknown, only 
$D\subset M$ is given. 

\dhighlight{Approach:} Build an undirected neighborhood graph $[Y,E]$ and use the graph distances $d_G$
instead of $d_M$ and aim to preserve it.

\begin{definition}\label{def:11} % TODO: Footnote not in colorbox
    Given a graph $[Y,E]$ for a data set $Y\subset\R^d$, s.t. $(y_i,y_j)\in E$ if and only if $y_i,y_j$ are adjacent\footnote{in some geometric senes}. 
    We define the \dhighlight{graph distance $d_G$} between two points $y_i,y_j\in Y$ by 
    \begin{enumerate}
        \item If $(y_i,y_j)\in E$ then $d_G(y_i,y_j)=d_E(y_i,y_j)$
        \item If $(y_i,y_j)\not\in E$ then let $\Gamma\{\gamma\mid \gamma=(\gamma_0,\dots,\gamma_s),\gamma_i\in Y, \gamma_0=y_i,\gamma_s=y_j\}$ and define 
        \[d_G(y_i,y_j)=\min_{\gamma\in\Gamma} \sum_{i=0,\dots,s-1}d_E(\gamma_i,\gamma_{i+1})\]
    \end{enumerate}
\end{definition}

We assume $Y\subset M\subset \R^d$ and that an isometric\marginnote{isometric means distance preserving} mapping 
\[f:M\to\R^p\]
exists $f(y)=x$ for $y\in M$ and \[d_E(f(y_i),f(y_j))=d_M(y_i,y_j)\]
for all $y_i,y_j\in M$.

Assume $Y$ is sampled densely enough from $M$, we expect that \[d_G(y_i,y_j)\approx d_M(y_i,y_j).\]

$D_G=[d_G^2(y_i,y_j)]_{i,j=1}^N$, we aim for $D\in\EDM$, with a configuration $X\in\R^p$ s.t. 
$D_G\approx D=[d_E^2(x_i,x_j)]_{i,j=1}^N$.

\begin{algorithm}[H]
    \caption{Isomap}\label{alg:isomap}
 \textbf{Input:} Dataset  $Y$, $p$\\
 \textbf{Output:} data set embedding $X$ in $p$ dimensions
 \begin{algorithmic}
    \State Build a neighborhood graph $[Y,E]$
    \State $D_{ij}=[d_E^2(x_i,x_j)]$ using Dijkstra's algorithm 
    \State $G=-\frac{1}{2} H D H$
    \State $[V_p,\Lambda_p]=\text{EVD}(G,p)$
    \State Return $\Lambda_p^{\frac{1}{2}} V^\intercal$
 \end{algorithmic}
\end{algorithm}

Assume $D_G\in\EDM$ we can invoke the MDS theorem \ref{thm:2.10}. We get 
\[\sum_{,j=1}^N \vert d_G^2(y_i,y_j)-d_E^2(x_i,x_j)\leq 2 N \sum_{l=p+1}^N \lambda_l.\] 

For the analysis we will use $r$-ball graphs: 

$(y_i,y_j)\in E\iff \Vert y_i-y_j\Vert_E\leq r$.

Furthermore we use the Hausdorff distance between $Y$ and $M$

\[\epsilon=H(M\mid Y)=\sup_{y\in M}\min_{y_i\in Y}\Vert y-y_i\Vert\]

\begin{theorem}\label{thm:2.12}
    Consider $M\subset\R^d$ compact and a sample $Y=\{y_1,\dots,y_N\}\subset M$ and let 
    $\epsilon=H(M\mid Y)$. For $r>0$ form the corresponding $r$-ball graph. When $\epsilon\leq \frac{r}{4}$, we 
    have for any $x,z\in Y$
    \[d_G(x,z)\leq \left(1+\frac{4\epsilon}{r}\right)d_M(x,z)\] 
\end{theorem}


\beginlecture{17}{18.06.24}

In practice we often choose $k$-nearest neighbors instead of $r$-ball neighborhoods!

\begin{aremark}
    CMDS is the same as MDS in our lecture. This is relevant for the sheets.
\end{aremark}

\begin{proof}
    For $d_E(x,z)\leq r$ we have that $(x,z)\in E$ in the $r$-ball graph and so 
    \[d_G(x,z)=d_E(x,z)\leq d_M(x,z).\]
    Now consider $d_E(x,z)>r$ and let $a=d_M(x,z)$ and let $\gamma:[0,a]\to M$ be parametrized by 
    arc length s.t. $\gamma(0)=x=y_{i_0}$ and $\gamma(a)=z=y_{i_s}$. Let $\hat{y}_{i_j}=\gamma\left(\frac{j a}{s}\right)$
    for $j=0,\dots,s$ where $s=\lceil 2a/r\rceil\geq 2$, where $\hat{y}_0=a,\hat{y}_s=z$.
    Let $y_j=\argmin_{y\in Y} d_E(y,\hat{y}_j)$. Clearly $\max_s d_E(y_{i,j}\hat{y}_j)\leq \epsilon$ for any $j=0,\dots,s-1$.
    \begin{align*}
        d_E(y_{i_j}, y_{i_{j+1}})&\leq d_E(y_{i_j},\hat{y}_j)+d_E(\hat{y}_j,\hat{y}_{j+1})+d_E(\hat{y}_{j+1},y_{i_{j+1}})\\
        &\leq \epsilon +d_M(\hat{y_j},\hat{y}_{j+1})+\epsilon\\
        &=a/s + 2\epsilon\leq r/2 + 2\epsilon< r
    \end{align*}
    Therefore $(y_{i_0},\dots,y_{i_s})$ forms a path in the $r$-ball graph.\marginnote{This might not be the shortest path in the graph}
    \begin{align*}
        d_G(x,z)&\leq \sum_{i=j}^{s-1} d_E(y_j,y_{j+1})\\
        &\leq d_M(\underbrace{\hat{y}_0}_{=x},\hat{y}_1)+\epsilon + \sum_{j=1}^{s-2} d_M(\hat{y}_j,\hat{y}_{j+1})+2\epsilon+d_M(\hat{y}_{s-1},\underbrace{\hat{y}_s}_{=z})+\epsilon\\
        &=d_M(x,z)+2(s-1)\epsilon\\
        &\leq \left(1+\frac{4\epsilon}{r}\right)d_M(x,z)
    \end{align*}
    we use that $s-1\leq \frac{2a}{r}$ with $a=d_M(x,z)$.
\end{proof}

\begin{aremark}
    Here we use that the subset of a shortest path is still a shortest path for the specific start and end points.
\end{aremark}

\begin{lemma}\label{lem:2.13}
    Let $\gamma:[0,a]\to\R^d$ be a unit speed curve with curvature bounded by $\kappa$. Then $d_E(\gamma(s),\gamma(t))\geq \frac{2}{\kappa}\sin\left(\frac{\kappa|t-s|}{2}\right)$
\end{lemma}

\begin{proof}[Proof of a weaker statement]
    Let $c$ denote the unit-speed parametrization of a circle of radius $\frac{1}{\kappa}$. From Darbins (1957) we obtain 
    $|t-s|\leq \frac{\pi}{\kappa}$:
    \[\langle \gamma(s), \gamma(t)\rangle \geq \langle c(s),c(a)\rangle\]
    This leads to \begin{align*}
        \Vert \gamma(t)-\gamma(s)\Vert \Vert \dot{\gamma}(s)&\stackrel{\text{C.S.}}{\geq}\langle \dot{\gamma(s)},\gamma(t)-\gamma(s)\rangle\\
        & = \int_s^t\langle \dot{\gamma}(s),\gamma(u)\rangle du\\
        &\geq \int_s^t\langle c(s),c(u)\rangle du\\
        &=\langle c(s),c(t)-c(s)\rangle\\
        &=\frac{1}{\kappa}\sin(\kappa\cdot(|t-s|))
    \end{align*}
    when $0\leq t-\leq \frac{\pi}{\kappa}$.

    We now assume $M\subset \R^d$ is compact and connected $C^2$-manifold with empty or $C^2$-boundary. In particular, shortest path on $M$ have curvature bounded by some $\kappa$.
\end{proof}

\begin{lemma}\label{lem:2.14}
    Suppose $M\subset \R^d$ has the above properties. Then for any $x,z\in M$ s.t. $d_M(x,z)\leq \frac{\pi}{\kappa}$
    \begin{enumerate}
        \item $d_M(x,z)\max\left(\frac{2}{\pi},1-\frac{\kappa^2}{24}d_M(x,z)^2\right)\leq d_E(x,z)\leq d_M(x,z)$. Moreover there is $\tau>0$ depending on $M$ s.t. for all $x,z\in M$ with 
        $d_E(x,z)\leq \tau$ it holds
        \item $d_E(x,z)\leq d_M(x,z)\leq d_E(x,z)\min\left(\frac{\pi}{2}+C_0\kappa^2d_E(x,z)^2\right)$ where $c_0$ is a constant that can be taken to be $c_0=\frac{\pi^2}{50}$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    For 1.:
    
    Take $x,z\in M$ and let $a=d_M(x,z)\leq \frac{\pi}{\kappa}$. Let $\gamma:[0,a]\to M$ be a unit-speed 
    shortest path on $M$ s.t. $\gamma(0)=x,\gamma(a)=z$. By assumption $\gamma$ has curvature bounded by $\kappa$. Applying lemma \ref{lem:2.13} gives 
    \[d_E(x,z)=d_E(\gamma(0),\gamma(a))\geq \frac{2}{\kappa}\sin\left(\kappa \frac{a}{2}\right)\geq \max\left(a-\frac{\kappa^2}{24}a^3,\frac{2}{\pi}a\right)\]
    and $\sin(t)\geq t-\frac{t^3}{6},\sin(t)\geq \frac{2t}{\pi}\forall t\leq \frac{\pi}{2}$.

    For 2.: by 1. $d_M(x,z)\leq \frac{\pi}{\kappa}$, then $d_M(x,z)\leq \frac{\pi}{2}d_E(x,z)$ and 
    \begin{align*}
        d_M(x,z)&\leq \frac{d_E(x,z)}{\left(1-\frac{\kappa^2}{24}d_M(x,z)^2\right)}\\
        &\leq \frac{d_E(x,z)}{\left(1-\frac{\kappa^2}{24}\left(\frac{\pi}{2}d_E(x,z)\right)^2\right)}\\
        &\leq d_E(x,z)(1+C_0\kappa^2 d_E(x,z)^2)
    \end{align*}
    where the last inequality holds if $\kappa d_E(x,z)$ is small enough. Therefore, it is enough to show that 
    there is $\tau>0$ s.t. \[d_M(x,z)\leq \frac{\pi}{\kappa}\]
    when $d_E(x,z)\leq \tau$. This is true, because the smoothness of $M$ guarantees that $d_M$  
    be continuous as a function  of the compact set $M\times M$.
\end{proof}
\begin{theorem}\label{thm:2.15}
    Suppose $M\subset\R^d$ has the above properties. Consider a sample $Y=\{y_1,\dots,y_m\}\subset M$. For $r>0$ from 
    the corresponding $r$-ball graph. Let $c_0$ and $\tau$ be defined per lemma \ref{lem:2.14} and $\kappa r\leq \frac{1}{3}$. We have 
    \[d_M(x,z)\leq (1+C_0\kappa^2r^2)d_G(x,z)\ \forall x,y\in Y\]
\end{theorem}

\begin{proof}
    Fix $x,z\in Y$. Let $x=y_{i_0},\dots,z=y_{i_s}$ define a shortest path in the graph joining $x,z$, so that $d_G(x,z)=\sum_{j=0}^{s-1}\Delta_j$, where 
    $\Delta_j=d_E(y_{i_j},y_{i_{j+1}})$. Define $a=d_M(x,z)$ and $a_j=d_M(y_{i_j},y_{i_{j+1}})$. 
    Since $\Delta_j\leq r \leq \tau$ by lemma \ref{lem:2.14} we see 
    \[\Delta_j\min\left(\frac{\pi}{2}\,c_0\kappa^2 \Delta_j^2 \right)\geq a_j.\]
    By assumption, $\kappa r\leq \frac{1}{3}$, and this can be seen to imply $1+C_0\kappa^2 r^2\leq \frac{\pi}{2}$, which then implies that $a_j\leq \Delta_j+C_0\kappa^2\Delta_j^3$, we thus have 
    \[a\leq \sum_{j=1}^{s-1} a_j\leq \sum_{j=0}^{s-1}(\Delta_j + C_0\kappa^2\Delta_j^3)\leq \sum_{j=0}^{s-1}\Delta_j (1+C_0\kappa^2r^2)=(1+C_0 \kappa^2 r^2)d_G(x,z)\qedhere\]
\end{proof}

\begin{aremark}
    This is the justification for using Isomap. One should consider, that the assumptions are NOT always met in practice, which might result our algorithm to fail. In particular we don't know 
    $M$ in practice, which makes our assumptions hard to check \dots

    Luckily the assumptions are often met ``naturally''.
\end{aremark}

\subsection{Parallel transport unfolding}

Underlying idea: % Bild 
Systematically overestimating the distance (by doing unnecessary turns on the graph). We then think about the tangent spaces and try to 
connect the local views of the tangent spaces. Main idea we find a better path (closer to the geodesic).

\beginlecture{18}{20.06.24}

We will now consider the computational ingredients using discrete parallel transport.
We are using a $l$-nearest neighbor graph.
\begin{enumerate}
    \item Approximate tangent spaces: Take $q$-NN in the graph, $q\geq k$\marginnote{separate $k,q$ to avoid unwanted connection while still getting good approximations}, denoted \[\{\tilde{y}_{l}^i\}_{l=1}^q,\text{ }\hat{y}_l^i = \tilde{y}_l^i-y_i\ \hat{Y}^i=[\hat{y}_1^i,\dots,\hat{y}_q^i]\]\marginnote{The space spanned by the columns of $T_i$}The $p$ left singular vectors of $\hat{Y}^i$ to the $p$ largest singular values give an orthonormal basis for $T_i$, spanning tje approximate tangent space\[T_i=[t_1^i,\dots,t_p^i]\in\R^{d\times p}\]
    \item Now given $y_o$\marginnote{$O(p)$ are the $p\times p$ orthogonal matrices} % TODO: FIX
    and $y_j$ connected in $G$ \[R_{j,i}=\argmin_{R\in O(p)}\Vert T_i-T_jR\Vert_F \]
    $R_{j,i}$ is the discrete metric connection between $y_i,y_j$, it best aligns $T_i,T_j$. By definition $R_{i,j}=R_{j,i}^{-1}=R_{j,i}^T$. With $T_i^TT_j=U\Sigma V^\intercal$ we can show $R_{j,i}=VU^T$.
    \item  Consider $(y_i,y_j,y_k)$ and start w.l.o.g. $y_i$. The first edge \[l_i=y_j-y_i\]
    is projected onto $T_i$: $v_i=T_i^\intercal e_i$.\marginnote{Projected in the $L^2$ sense}
    We set $z_i=0$ and $z_j=z_i+v_i$ defines the first modified edge. Now, $e_j=y_k-y_i$ and $T_j^\intercal e_j$ is parallel transported 
    into the tangent space at $y_i$\[v_j=R_{i,j}[T_j^\intercal e_j]\] and $z_k=z_j+v_j$%TODO: FIX
    Under some assumptions 
    \[\Vert v_i+v_j\Vert_2=d_E(z_i,z_k)\approx d_M(y_i,y_k)\]
    \item For longer paths $y_{i_1},\dots,y_{i_m}$ we iteratively project the edges $l_{i_s}=y_{i_{s+1}}-y_{i_s}$ onto $T_{i_j}$ before parallel transporting back to $T_{i_1}$\[V_{i_s}=\left(\prod_{j=1}^{s-1} R_{i_j,i_{j+1}}\right)[T_{i_s}^\intercal l_{i_s}]\]
    with $V=\sum_{j=1}^M v_{I_S}$ WE GET $\Vert v \Vert_2\approx d_M(y_1,y_{i_m})$
\end{enumerate}
\[(\star)\ 1-\xi \frac{d_G(y_i,y_j)}{d_M(y_i,y_j)}\leq 1+\xi\]
We saw in PTU 
\[\min_{R\in O(p)}\Vert Y^\intercal - X^\intercal R\Vert_F.\]
Generally aligning two point sets using an orthogonal transformation is called \dhighlight{procrustes problem}.

Pointwise:

\[\min_{R\in O(p)}\sum_{i=1}^N\Vert y_i - Rx_i\Vert_2^2.\]

Consider two configurations $X,Y$\marginnote{which stem from two different EDMs, which are somehow alligned / comparable}, given
\[\Vert Y^\intercal Y-X^\intercal X\Vert_F=\epsilon^2\]
and 
\[\Vert X^\dagger\Vert_2\epsilon\leq \frac{1}{\sqrt{2}}\]
it holds 
\[\min_{R\in O(d)}\Vert Y^\intercal X^\intercal R\Vert_F\leq (1+\sqrt{2})\Vert X^\dagger\Vert_2 \epsilon^2\]

For $D,\tilde{D}\in \EDM$ it holds using $\epsilon^2=\frac{1}{2}\Vert H(\tilde{D}-D)H\Vert_F$ a corresponding result.

Using $(\star)$ for Isomap amd $X$ the exact embedding using $d_M$ into $\R^p$ one has 
\[\min_{R\in O(p)}\left(\frac{1}{N}\sum_{i=1}^N\Vert z_i-R x_i\Vert^2\right)^{\frac{1}{2}}\leq \frac{36\sqrt{p}\rho^3}{w^2}\xi\]
for $Z$ the approximate embedding and $\rho,w$ are singular values of our datamatrix:

\[\rho=\rho(X)=\frac{\sigma_1(X)}{\sqrt{N}}\]
radius of $X$ largest deviations along any direction in space.
\[w=w(X)=\frac{\sigma_p(w)}{\sqrt{N}}\]
half width of $X$, smallest deviations along any direction in space.

\subsection*{Using kernels again ...}

Nonlinear PCA

Reminder: $C=\frac{1}{N}Y\cdot Y^\intercal=\frac{1}{N}\sum_{i=1}^N y_i\cdot _i^T$
we consider some \dhighlight{feature map} $\phi:R^d\to\R^r\eqcolon \cF$
For simplicity, we assume to be mean-centered on the feature space:
\[\sum_{i=1}^N\phi(y_i)=0.\]
Now, do PCA in $\cF$, $C_\phi=\frac{1}{N}\sum_{i=1}^N\phi(y_i)\phi(y_i)^\intercal = \frac{1}{N}\phi(Y)\phi(Y)^\intercal=\frac{1}{N}\Phi\cdot\Phi^\intercal$.

EVD gives $C_\phi u_i=\lambda_i u_i$ and by theorem \ref{thm:2.3}  the nonlinear principal components are 
\[X_i=u_i^\intercal \Phi.\]
IGNORE:
\[\Phi\Phi^\intercal\to \Phi^\intercal \Phi\]
\[U^\intercal=V^\intercal \Phi^\intercal\]
\[X=V^\intercal\Phi^\intercal \Phi=\Lambda^{-\frac{1}{2}}\Phi^\intercal \Phi=\Lambda^{-\frac{1}{2}}V^\intercal V \Lambda V^\intercal=\Lambda^{\frac{1}{2}}V^\intercal\]
STOP IGNORING
\[x_I=V^\intercal \Phi^\intercal\Phi(y_i)\]
\[K_{i,j}=\langle \Phi(y_i),\Phi(y_j) \rangle_{\cF}\eqqcolon K(y_i,y_j)\]
A projection of a point $y$ with image $\phi(y)$ gives 
\begin{align*}
    x&=V^\intercal \Phi^\intercal \phi(y)\\
    &=\sum_{i=1}^N v_i\langle \phi(y_i),\phi(y)\rangle_{\cF}\\
    &=\sum_{i=1}^N v_i K(y_i,y)
\end{align*}

This kernelized form we call \dhighlight{Kernel MDS}, but it is usually called 
\dhighlight{Kernel PCA} in the literature.\marginnote{The interpretation step of PCA gets a lot harder. Especiially, since the feature maps are not unique \dots} 

To consider non-centered data, we need to center it: 
\[\tilde{\phi}(y_i)\coloneqq \phi(y_i)-\frac{1}{N}\sum_{j=1}^N\phi(y_j)\]
with associated kernel $\tilde{K}$.
\[\tilde{\Phi}(Y)=(\Phi^c(Y)-\frac{1}{N}\Phi(Y)1)1^\intercal = \Phi(Y)H\]
Action of $H$ goes through the SP 
\[\tilde{K}=K^c=HKH\]
\begin{algorithm}[H] 
    \caption{Nonlinear PCA}\label{alg:nl_pca}
 \textbf{Input:}  nonlinear data set $Y$ $\phi:\R^d\to\R^r,p$\\
 \textbf{Output:} configuration $X$ in $p\leq \min(r,N)$ dimensions
 \begin{algorithmic}
   \State $K=-H\Phi^\intercal(Y)\Phi H$
   \State $[V_p,\Lambda_p]=\text{EVD}(K,p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V_p^\intercal$
 \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
    \caption{Nonlinear MDS}\label{alg:nl_mds}
 \textbf{Input:}  nonlinear data set $Y$, Kernel $K:\R^d\times\R^d\to\R$\\
 \textbf{Output:} configuration $X$ in $p\leq N$ dimensions
 \begin{algorithmic}
   \State $K_{i,j}=K(y_i,y_j)$
   \State $\hat{K}=HKH$
   \State $[V_p,\Lambda_p]=\text{EVD}(\hat{K},p)$
   \State Return $\Lambda_p^{\frac{1}{2}} V_p^\intercal$
 \end{algorithmic}
\end{algorithm}

